<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python_update_wrapper]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-update-wrapper%2F</url>
    <content type="text"><![CDATA[来自 functoolsfunctools 库中装饰器相关的函数是 update_wrapper 、wraps，还搭配 WRAPPER_ASSIGNMENTS 和WRAPPER_UPDATES 两个常量使用，作用就是消除 Python 装饰器的一些负面作用。 wraps例： 12345678910def decorator(func): def wrapper(*args, **kwargs): return func(*args, **kwargs) return wrapper@decoratordef add(x, y): return x + yadd # &lt;function __main__.wrapper&gt; 可以看到被装饰的函数的名称，也就是函数的 __name__ 属性变成了 wrapper， 这就是装饰器带来的副作用，实际上add 函数整个变成了 decorator(add)，而 wraps 装饰器能消除这些副作用： 1234567891011def decorator(func): @wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) return wrapper@decoratordef add(x, y): return x + yadd # &lt;function __main__.add&gt; 更正的属性定义在 WRAPPER_ASSIGNMENTS 中： 1234&gt;&gt;&gt; functools.WRAPPER_ASSIGNMENTS('__module__', '__name__', '__doc__')&gt;&gt;&gt; functools.WRAPPER_UPDATES('__dict__',) update_wrapperupdate_wrapper 的作用与 wraps 类似，不过功能更加强大，换句话说，wraps 其实是 update_wrapper 的特殊化，实际上 wraps(wrapped) 相当于 partial(update_wrapper, wrapped=wrapped, kwargs)**。 因此，上面的代码可以用 update_wrapper 重写如下： 1234def decorator(func): def wrapper(*args, **kwargs): return func(*args, **kwargs) return update_wrapper(wrapper, func)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>warpper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C14_Testing_Debugging_and_Exceptions]]></title>
    <url>%2F2018%2F12%2F15%2FC14-Testing-Debugging-and-Exceptions%2F</url>
    <content type="text"><![CDATA[Testing Output Sent to stdoutYou have a program that has a method whose output goes to standard Output (sys.stdout). This almost always means that it emits text to the screen. You’d like to write a test for your code to prove that, given the proper input, the proper output is displayed. Using the unittest.mock module’s patch() function, it’s pretty simple to mock out sys.stdout for just a single test, and put it back again, without messy temporary variables or leaking mocked-out state between test cases.123456789101112131415161718def urlprint(protocol, host, domain): url= '&#123;&#125;://&#123;&#125;.&#123;&#125;'.format(protocol, host, domain) print(url)from io import StringIOfrom unittest import TestCasefrom unittest.mock import patchclass TestURLPrint(TestCase): def test_url_gets_to_stdout(self): protocol= 'http' host= 'www' domain= 'example.com' expected_url='&#123;&#125;://&#123;&#125;.&#123;&#125;\n'.format(protocol, host, domain) with patch('sys.stdout', new= StringIO()) as fake_out: urlprint(protocol, host, domain) self.assertEqual(fake_out.getvalue(), expected_url) To run the test, the unittest.mock.patch() function is used as a context manager to replace the value of sys.stdout with a StringIO object as a substitute. The fake_out variable is the mock object that’s created in this process. This can be used inside the body of the with statement to perform various checks. When the with statement completes, patch conveniently puts everything back the way it was before the test ever ran.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C13_Utility_Scripting_and_System_Administration]]></title>
    <url>%2F2018%2F12%2F15%2FC13-Utility-Scripting-and-System-Administration%2F</url>
    <content type="text"><![CDATA[Accepting Script Input via Redirection, Pipes, or Input FilesYou want a script you’ve written to be able to accept input using whatever mechanism is easiest for the user. This should include piping output from a command to the script, redirecting a file into the script, or just passing a filename, or list of filenames, to the script on the command line. Python’s built-in fileinput module makes this very simple and concise.12345import fileinputwith fileinput.input() as f_input: for line in f_input: print(line, end= '') Then you can already accept input to the script in all of the previously mentioned ways. If you save this script as filein.py and make it executable, you can do all of the following and get the expected output: 12345678910111213141516$ ls | ./filein.py # Prints a directory listing to stdout.$ ./filein.py /etc/passwd # Reads /etc/passwd to stdout.$ ./filein.py &lt; /etc/passwd # Reads /etc/passwd to stdout. ``` The **fileinput.input()** function creates and returns an instance of the **FileInput** class. In addition to containing a few handy helper methods, the instance can also be used as a context manager. So, to put all of this together, if we wrote a script that expected to be printing output from several files at once, we might have it include the filename and line number in the output, like this:```python&gt;&gt;&gt; import fileinput&gt;&gt;&gt; with fileinput.input('/etc/passwd') as f:&gt;&gt;&gt; for line in f:... print(f.filename(), f.lineno(), line, end='').../etc/passwd 1 ##/etc/passwd 2 # User Database/etc/passwd 3 # Terminating a Program with an Error MessageYou want your program to terminate by printing a message to standard error and returning a nonzero status code. To have a program terminate in this manner, raise a SystemExit exception, but supply the error message as an argument. raise SystemExit(&#39;It failed!&#39;) This will cause the supplied message to be printed to sys.stderr and the program to exit with a status code of 1. Parsing Command-Line OptionsYou want to write a program that parses options supplied on the command line (found in sys.argv). The argparse module can be used to parse command-line options. 123456789101112131415161718192021222324import argparseparser= argparse.ArgumentParser(description= 'Search some files')parser.add_argument(dest= 'filenames', metavar= 'filename', nargs= '*')parser.add_argument('-p', '--pat', metavar= 'pattern', required= True, dest='patterns', action= 'append', help='text pattern to search for')parser.add_argument('-v', dest= 'verbose', action= 'store_true', help= 'verbose mode')parser.add_argument('-o', dest= 'outfile', action= 'store', help= 'output file')parser.add_argument('--speed', dest= 'speed', action= 'store', choices= &#123;'slow', 'fast'&#125;, default= 'slow', help= 'search speed')args= parser.parse_args()print(args.filenames)print(args.patterns)print(args.verbose)print(args.outfile)print(args.speed) This program defines a command-line parser with the following usage:1234567891011121314151617PS D:\python\computebaseline&gt; python .\temporary.py -husage: temporary.py [-h] [-p pattern] [-v] [-o OUTFILE] [--speed &#123;slow,fast&#125;] [filename [filename ...]]Search some filespositional arguments: filenameoptional arguments: -h, --help show this help message and exit -p pattern, --pat pattern text pattern to search for -v verbose mode -o OUTFILE output file --speed &#123;slow,fast&#125; search speed The following session shows how data shows up in the program. Carefully observe the output of the print() statements. 123456789101112131415161718192021222324PS D:\python\computebaseline&gt; python .\temporary.py -v -p spam --pat= eggs foo.txt bar.txt['eggs', 'foo.txt', 'bar.txt']['spam', '']TrueNoneslowPS D:\python\computebaseline&gt; python .\temporary.py -v -p spam --pat=eggs foo.txt bar.txt['foo.txt', 'bar.txt']['spam', 'eggs']TrueNoneslowPS D:\python\computebaseline&gt; python .\temporary.py -v -p spam --pat=eggs foo.txt bar.txt -o results['foo.txt', 'bar.txt']['spam', 'eggs']TrueresultsslowPS D:\python\computebaseline&gt; python .\temporary.py -v -p spam --pat=eggs foo.txt bar.txt -o results --speed=fast['foo.txt', 'bar.txt']['spam', 'eggs']Trueresultsfast To parse options, you first create an ArgumentParser instance and add declarations for the options you want to support it using the add_argument() method. In each add_argument() call, the dest argument specifies the name of an attribute where the result of parsing will be placed. The metavar argument is used when generating help messages. The action argument specifies the processing associated with the argument and is often store for storing a value or append for collecting multiple argument values into a list. The following argument collects all of the extra command-line arguments into a list. It’s being used to make a list of filenames in the example: parser.add_argument(dest=&#39;filenames&#39;,metavar=&#39;filename&#39;, nargs=&#39;*&#39;) The following argument sets a Boolean flag depending on whether or not the argument was provided: 12parser.add_argument('-v', dest='verbose', action='store_true', help='verbose mode') The following argument takes a single value and stores it as a string: 12parser.add_argument('-o', dest='outfile', action='store', help='output file') The following argument specification allows an argument to be repeated multiple times and all of the values append into a list. The required flag means that the argument must be supplied at least once. The use of -p and –pat mean that either argument name is acceptable. 123parser.add_argument('-p', '--pat',metavar='pattern', required=True, dest='patterns', action='append', help='text pattern to search for') Finally, the following argument specification takes a value, but checks it against a set of possible choices. parser.add_argument(&#39;--speed&#39;, dest=&#39;speed&#39;, action=&#39;store&#39;, choices={&#39;slow&#39;,&#39;fast&#39;}, default=&#39;slow&#39;, help=&#39;search speed&#39;) Once the options have been given, you simply execute the parser.parse() method. This will process the sys.argv value and return an instance with the results. The results for each argument are placed into an attribute with the name given in the dest parameter to add_argument(). Copying or Moving Files and DirectoriesThe shutil module has portable implementations of functions for copying files and directories. 123456789import shutil# Copy src to dst. (cp src dst)shutil.copy(src, dst)# Copy files, but preserve metadata (cp -p src dst)shutil.copy2(src, dst)# Copy directory tree (cp -R src dst)shutil.copytree(src, dst)# Move src to dst (mv src dst)shutil.move(src, dst) By default, symbolic links are followed by these commands. For example, if the source file is a symbolic link, then the destination file will be a copy of the file the link points to. If you want to copy the symbolic link instead, supply the follow_symlinks keyword argument like this: shutil.copy2(src, dst, follow_symlinks=False) If you want to preserve symbolic links in copied directories, do this: shutil.copytree(src, dst, symlinks=True) The copytree() optionally allows you to ignore certain files and directories during the copy process. To do this, you supply an ignore function that takes a directory name and filename listing as input, and returns a list of names to ignore as a result. For example: 123def ignore_pyc_files(dirname, filenames): return [name in filenames if name.endswith('.pyc')]shutil.copytree(src, dst, ignore=ignore_pyc_files) Since ignoring filename patterns is common, a utility function ignore_patterns() has already been provided to do it. shutil.copytree(src, dst, ignore=shutil.ignore_patterns(&#39;*~&#39;,&#39;*.pyc&#39;)) One tricky bit about copying directories with copytree() is the handling of errors. For example, in the process of copying, the function might encounter broken symbolic links, files that can’t be accessed due to permission problems, and so on. To deal with this, all exceptions encountered are collected into a list and grouped into a single exception that gets raised at the end of the operation. Here is how you would handle it: 12345678try: shutil.copytree(src, dst)except shutil.Error as e: for src, dst, msg in e.args[0]: # src is source name # dst is destination name # msg is error message from exception print(dst, src, msg) If you supply the ignore_dangling_symlinks=True keyword argument, then copytree() will ignore dangling symlinks. Adding Logging to LibrariesYou would like to add a logging capability to a library, but don’t want it to interfere with programs that don’t use logging. For libraries that want to perform logging, you should create a dedicated logger object, and initially configure it as follows: 12345678# somelib.pyimport logginglog= logging.getLogger(__name__)log.addHandler(logging.NullHandler())def func(): log.critical('A critical error!') log.debug('A debug message') With this configuration, no logging will occur by default. 123&gt;&gt;&gt; import somelib&gt;&gt;&gt; somelib.func()&gt;&gt;&gt; However, if the logging system gets configured, log messages will start to appear. 12345&gt;&gt;&gt; import logging&gt;&gt;&gt; logging.basicConfig()&gt;&gt;&gt; somelib.func()CRITICAL:somelib:A Critical Error!&gt;&gt;&gt; Libraries present a special problem for logging, since information about the environment in which they are used isn’t known. As a general rule, you should never write library code that tries to configure the logging system on its own or which makes assumptions about an already existing logging configuration. Thus, you need to take great care to provide isolation. The call to getLogger(__name__) creates a logger module that has the same name as the calling module. Since all modules are unique, this creates a dedicated logger that is likely to be separate from other loggers. The log.addHandler(logging.NullHandler()) operation attaches a null handler to the just created logger object. A null handler ignores all logging messages by default. Thus, if the library is used and logging is never configured, no messages or warnings will appear. One subtle feature of this recipe is that the logging of individual libraries can be independently configured, regardless of other logging settings.12345678910111213&gt;&gt;&gt; import logging&gt;&gt;&gt; logging.basicConfig(level=logging.ERROR)&gt;&gt;&gt; import somelib&gt;&gt;&gt; somelib.func()CRITICAL:somelib:A Critical Error!&gt;&gt;&gt; # Change the logging level for 'somelib' only&gt;&gt;&gt; logging.getLogger('somelib').level=logging.DEBUG&gt;&gt;&gt; somelib.func()CRITICAL:somelib:A Critical Error!DEBUG:somelib:A debug message&gt;&gt;&gt; Here, the root logger has been configured to only output messages at the ERROR level or higher. However, the level of the logger for somelib has been separately configured to output debugging messages. That setting takes precedence over the global setting. Making a Stopwatch TimerYou want to be able to record the time it takes to perform various tasks. The time module contains various functions for performing timing-related functions. However, it’s often useful to put a higher-level interface on them that mimics a stopwatch. 123456789101112131415161718192021222324252627282930313233import timeclass Timer: def __init__(self, func= time.perf_counter): self.elapsed= 0.0 self._func= func self._start= None def start(self): if self._start is not None: raise RuntimeError('Already started') self._start= self._func() def stop(self): if self._start is None: raise RuntimeError('Not started') end= self._func() self.elapsed+= end- self._start self._start= None def reset(self): self.elapsed= 0 @property def running(self): return self._start is not None def __enter__(self): self.start() return self def __exit__(self, *args): self.stop() This class defines a timer that can be started, stopped, and reset as needed by the user. It keeps track of the total elapsed time in the elapsed attribute. 1234567891011121314def countdown(n): while n &gt; 0: n -= 1# Use 1: Explicit start/stopt = Timer()t.start()countdown(1000000)t.stop()print(t.elapsed)# Use 2: As a context managerwith t: countdown(1000000) print(t.elapsed) One issue in making timing measurements concerns the underlying time function used to do it. As a general rule, the accuracy of timing measurements made with functions such as time.time() or time.clock() varies according to the operating system. In contrast, the time.perf_counter() function always uses the highest-resolution timer available on the system. As shown, the time recorded by the Timer class is made according to wall-clock time, and includes all time spent sleeping. If you only want the amount of CPU time used by the process, use time.process_time() instead. For example:12345t = Timer(time.process_time)with t: countdown(1000000)print(t.elapsed) Launching a Web BrowserThe webbrowser module can be used to launch a browser in a platform-independent manner. For example:1234&gt;&gt;&gt; import webbrowser&gt;&gt;&gt; webbrowser.open('http://www.python.org')True&gt;&gt;&gt; This opens the requested page using the default browser. If you want a bit more control over how the page gets opened, you can use one of the following functions:12345678&gt;&gt;&gt; # Open the page in a new browser window&gt;&gt;&gt; webbrowser.open_new('http://www.python.org')True&gt;&gt;&gt;&gt;&gt;&gt; # Open the page in a new browser tab&gt;&gt;&gt; webbrowser.open_new_tab('http://www.python.org')True If you want to open a page in a specific browser, you can use the webbrowser.get() function to specify a particular browser. 12345&gt;&gt;&gt; c = webbrowser.get('firefox')&gt;&gt;&gt; c.open('http://www.python.org')True&gt;&gt;&gt; c.open_new_tab('http://docs.python.org')True]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C12_Concurrency]]></title>
    <url>%2F2018%2F12%2F15%2FC12-Concurrency%2F</url>
    <content type="text"><![CDATA[Starting and Stopping ThreadsThe threading library can be used to execute any Python callable in its own thread. To do this, you create a Thread instance and supply the callable that you wish to execute as a target. 12345678910&gt;&gt;&gt; import time&gt;&gt;&gt; def countdown(n):... while n &gt; 0:... print('T-minus', n)... n-= 1... time.sleep(3)... &gt;&gt;&gt; from threading import Thread&gt;&gt;&gt; t= Thread(target= countdown, args=(10,))&gt;&gt;&gt; t.start() When you create a thread instance, it doesn’t start executing until you invoke its start() method (which invokes the target function with the arguments you supplied). Threads are executed in their own system-level thread (e.g., a POSIX thread or Windows threads) that is fully managed by the host operating system. Once started, threads run independently until the target function returns. You can query a thread instance to see if it’s still running: 1234if t.is_alive(): print('Still running')else: print('Completed') You can also request to join with a thread, which waits for it to terminate: t.join() The interpreter remains running until all threads terminate. For long-running threads or background tasks that run forever, you should consider making the thread daemonic. 12t = Thread(target=countdown, args=(10,), daemon=True)t.start() Daemonic threads can’t be joined. However, they are destroyed automatically when the main thread terminates. Beyond the two operations shown, there aren’t many other things you can do with threads. For example, there are no operations to terminate a thread, signal a thread, adjust its scheduling, or perform any other high-level operations. If you want these features, you need to build them yourself. 1234567891011121314151617181920&gt;&gt;&gt; class CountdowmTask:... def __init__(self):... self._running= True... def terminate(self):... self._running= False... def run(self, n):... while self._running and n &gt; 0:... print('T-minus', n)... n-= 1... time.sleep(2)... &gt;&gt;&gt; c= CountdowmTask()&gt;&gt;&gt; t= Thread(target= c.run, args=(10, ))&gt;&gt;&gt; t.start()T-minus 10T-minus 9T-minus 8T-minus 7&gt;&gt;&gt; c.terminate() # Signal termination&gt;&gt;&gt; t.join() # Wait for actual termination (if needed) Due to a global interpreter lock (GIL), Python threads are restricted to an execution model that only allows one thread to execute in the interpreter at any given time. For this reason, Python threads should generally not be used for computationally intensive tasks where you are trying to achieve parallelism on multiple CPUs. They are much better suited for I/O handling and handling concurrent execution in code that performs blocking operations (e.g., waiting for I/O, waiting for results from a database, etc.). Sometimes you will see threads defined via inheritance from the Thread class. 12345678910111213&gt;&gt;&gt; from threading import Thread&gt;&gt;&gt; class CountdownThread(Thread):... def __init__(self, n):... super().__init__()... self.n= n... def run(self):... while self.n &gt; 0:... print('T-minus', self.n)... self.n-= 1... time.sleep(3)... &gt;&gt;&gt; c=CountdownThread(5)&gt;&gt;&gt; c.start() Determining If a Thread Has StartedYou’ve launched a thread, but want to know when it actually starts running. A key feature of threads is that they execute independently and nondeterministically. This can present a tricky synchronization problem if other threads in the program need to know if a thread has reached a certain point in its execution before carrying out further operations. To solve such problems, use the Event object from the threading library. Event instances are similar to a “sticky” flag that allows threads to wait for something to happen. Initially, an event is set to 0. If the event is unset and a thread waits on the event, it will block (i.e., go to sleep) until the event gets set. A thread that sets the event will wake up all of the threads that happen to be waiting (if any). If a thread waits on an event that has already been set, it merely moves on, continuing to execute. 1234567891011121314151617181920212223from threading import Thread, Eventimport time# Code to execute in an independent threaddef countdown(n, started_evt): print('countdown starting') started_evt.set() while n &gt; 0: print('T-minus', n) n -= 1 time.sleep(5)# Create the event object that will be used to signal startupstarted_evt = Event()# Launch the thread and pass the startup eventprint('Launching countdown')t = Thread(target=countdown, args=(10,started_evt))t.start()# Wait for the thread to startstarted_evt.wait()print('countdown is running') Event objects are best used for one-time events. That is, you create an event, threads wait for the event to be set, and once set, the Event is discarded. Although it is possible to clear an event using its clear() method, safely clearing an event and waiting for it to be set again is tricky to coordinate, and can lead to missed events, deadlock, or other problems . If a thread is going to repeatedly signal an event over and over, you’re probably better off using a Condition object instead. For example, this code implements a periodic timer that other threads can monitor to see whenever the timer expires: 1234567891011121314151617181920212223242526272829303132333435363738394041424344import threadingimport timeclass PeriodicTimer: def __init__(self, interval): self._interval= interval self._flag= 0 self._cv= threading.Condition() def start(self): t= threading.Thread(target= self.run) t.daemon= True t.start() def run(self): while True: time.sleep(self._interval) with self._cv: self._flag^= 1 self._cv.notify_all() def wait_for_tick(self): with self._cv: last_flag= self._flag while last_flag == self._flag: self._cv.wait()# Example use of the timerptimer= PeriodicTimer(5) ptimer.start()# Two threads that synchronize on the timerdef countdown(nticks): while nticks &gt; 0: ptimer.wait_for_tick() print('T-minus', nticks) nticks-=1def countup(last): n= 0 while n &lt; last: ptimer.wait_for_tick() print('Counting', n) n+= 1threading.Thread(target= countdown, args=(10,)).start()threading.Thread(target= countup, args=(5,)).start() A critical feature of Event objects is that they wake all waiting threads. If you are writing a program where you only want to wake up a single waiting thread, it is probably better to use a Semaphore or Condition object instead. For example, consider this code involving semaphores: 12345678910&gt;&gt;&gt; def worker(n, sema):... sema.acquire()... print('Working', n)... &gt;&gt;&gt; sema= threading.Semaphore(0)&gt;&gt;&gt; nworkers= 10&gt;&gt;&gt; for n in range(nworkers):... t= threading.Thread(target= worker, args=(n, sema,))... t.start()... If you run this, a pool of threads will start, but nothing happens because they’re all blocked waiting to acquire the semaphore. Each time the semaphore is released, only one worker will wake up and run. 1234567&gt;&gt;&gt; sema.release()Working 0&gt;&gt;&gt; sema.release()Working 1&gt;&gt;&gt; sema.release()Working 2&gt;&gt;&gt; Communicating Between ThreadsYou have multiple threads in your program and you want to safely communicate or exchange data between them. Perhaps the safest way to send data from one thread to another is to use a Queue from the queue library. To do this, you create a Queue instance that is shared by the threads. Threads then use put() or get() operations to add or remove items from the queue. 123456789101112131415161718192021from queue import Queuefrom threading import Thread# A thread that produces datadef producer(out_q): while True: # Produce some data ... out_q.put(data)# A thread that consumes datadef consumer(in_q): while True: # Get some data data = in_q.get() # Process the data ...# Create the shared queue and launch both threadsq = Queue()t1 = Thread(target=consumer, args=(q,))t2 = Thread(target=producer, args=(q,))t1.start()t2.start() Queue instances already have all of the required locking, so they can be safely shared by as many threads as you wish. Although queues are the most common thread communication mechanism, you can build your own data structures as long as you add the required locking and synchronization. The most common way to do this is to wrap your data structures with a condition variable. For example, here is how you might build a thread-safe priority queue. 123456789101112131415161718import heapq import threadingclass PriorityQueue: def __init__(self): self._queue= [] self._count= 0 self._cv= threading.Condition() def put(self,item, priority): with self._cv: heapq.heappush(self._queue, (-priority, self._count, item)) self._count+= 1 self._cv.notify() def get(self): with self._cv: while len(self._queue) == 0: self._cv.wait() return heapq.heappop(self._queue)[-1] Thread communication with a queue is a one-way and nondeterministic process. In general, there is no way to know when the receiving thread has actually received a message and worked on it. However, Queue objects do provide some basic completion features, as illustrated by the task_done() and join() methods. Locking Critical SectionsTo make mutable objects safe to use by multiple threads, use Lock objects in the threading library. 123456789101112import threadingclass SharedCounter: def __init__(self, initial_value= 0): self._value= initial_value self._value_lock = threading.Lock() def incr(self, delta= 1): with self._value_lock: self._value+= delta def decr(self, delta= 1): with self._value_lock: self._value-= delta A Lock guarantees mutual exclusion when used with the with statement—that is, only one thread is allowed to execute the block of statements under the with statement at a time. The with statement acquires the lock for the duration of the indented statements and releases the lock when control flow exits the indented block. In the threading library, you’ll find other synchronization primitives, such as RLock and Semaphore objects. As a general rule of thumb, these are more special purpose and should not be used for simple locking of mutable state. An RLock or re-entrant lock object is a lock that can be acquired multiple times by the same thread. It is primarily used to implement code based locking or synchronization based on a construct known as a “monitor.” With this kind of locking, only one thread is allowed to use an entire function or the methods of a class while the lock is held. For example, you could implement the SharedCounter class like this:1234567891011class SharedCounter: _lock= threading.RLock() def __init__(self, initial_value= 0): self._value= initial_value def incr(self, delta= 1): with SharedCounter._lock: self._value+= delta def decr(self, delta= 1): with SharedCounter._lock: self.incr(-delta) Storing Thread-Specific StateSometimes in multithreaded programs, you need to store data that is only specific to the currently executing thread. To do this, create a thread-local storage object using threading.local(). Attributes stored and read on this object are only visible to the executing thread and no others. 1234567891011121314151617181920212223242526272829303132333435363738from socket import socket, AF_INET, SOCK_STREAMimport threadingclass LazyConnecion: def __init__(self, address, family= AF_INET, type= SOCK_STREAM): self.address= address self.family= AF_INET self.type= SOCK_STREAM self.local= threading.local() def __enter__(self): if hasattr(self.local, 'sock'): raise RuntimeError('Already connected') self.local.sock= socket(self.family, self.type) self.local.sock.connect(self.address) return self.local.sock def __exit__(self, exc_ty, exc_val, tb): self.local.sock.close() del self.local.sockfrom functools import partialdef test(conn) : with conn as s: s.send(b'GET /index.html HTTP/1.0\r\n') s.send(b'Host: www.python.org\r\n') s.send(b'\r\n') resp= b''.join(iter(partial(s.recv, 8192), b'')) print('Got &#123;&#125; bytes'.format(len(resp)))if __name__=='__main__': conn= LazyConnecion(('www.python.org', 80)) t1= threading.Thread(target= test, args=(conn,)) t2= threading.Thread(target= test, args=(conn,)) t1.start() t2.start() t1.join() t2.join() The reason it works is that each thread actually creates its own dedicated socket connection (stored as self.local.sock). Thus, when the different threads perform socket operations, they don’t interfere with one another as they are being performed on dif‐ ferent sockets. Under the covers, an instance of threading.local() maintains a separate instance dictionary for each thread. All of the usual instance operations of getting, setting, and deleting values just manipulate the per-thread dictionary. The fact that each thread uses a separate dictionary is what provides the isolation of data. Creating a Thread PooYou want to create a pool of worker threads for serving clients or performing other kinds of work. The concurrent.futures library has a ThreadPoolExecutor class that can be used for this purpose.123456789101112131415161718192021222324from socket import socket, AF_INET, SOCK_STREAMfrom concurrent.futures import ThreadPoolExecutordef echo_client(sock, client_addr): print('Got connectin from', client_addr) while True: msg= sock.recv(65536) if not msg: break sock.sendall(msg) print('Client closed connection') sock.close()def echo_server(addr) : pool= ThreadPoolExecutor(128) sock= socket(AF_INET, SOCK_STREAM) sock.bind(addr) sock.listen(5) while True: client_sock, client_addr= sock.accept() pool.submit(echo_client, client_sock, client_addr)echo_server(('', 16000)) If you want to manually create your own thread pool, it’s usually easy enough to do it using a Queue. 12345678910111213141516171819202122232425262728from socket import socket, AF_INET, SOCK_STREAMfrom threading import Threadfrom queue import Queuedef echo_client(q): sock, client_addr= q.get() print('Got connection from', client_addr) while True: msg= sock.recv(65536) if not msg: break sock.sendall(msg) print('Client closed connection') sock.close()def echo_server(addr, nworkers): q= Queue() for n in range(nworkers): t= Thread(target= echo_client, args= (q,)) t.daemon= True t.start() sock= socket(AF_INET, SOCK_STREAM) sock.bind(addr) sock.listen(5) while True: client_sock, client_addr= sock.accept() q.put((client_sock, client_addr))echo_server(('', 15000), 128) One advantage of using ThreadPoolExecutor over a manual implementation is that it makes it easier for the submitter to receive results from the called function. For example, you could write code like this: 12345678910111213from concurrent.futures import ThreadPoolExecutorimport urllib.requestdef fetch_url(url): u= urllib.request.urlopen(url) data= u.read() return datapool= ThreadPoolExecutor(10)a= pool.submit(fetch_url, 'http://www.python.org')b= pool.submit(fetch_url, 'http://www.pypy.org')x= a.result()y= b.result() The result objects in the example handle all of the blocking and coordination needed to get data back from the worker thread. Specifically, the operation a.result() blocks until the corresponding function has been executed by the pool and returned a value. Performing Simple Parallel ProgrammingYou have a program that performs a lot of CPU-intensive work, and you want to make it run faster by having it take advantage of multiple CPUs. The concurrent.futures library provides a ProcessPoolExecutor class that can be used to execute computationally intensive functions in a separately running instance of the Python interpreter. 12345678910111213141516171819202122232425import gzipimport ioimport globfrom concurrent import futuresdef find_robots(filename): robots= set() with gzip.open(filename) as f: for line in io.TextIOWrapper(f, encoding= 'ascii'): fields= line.split() if fields[6] == '/robots.txt': robots.add(fields[0]) return robotsdef find_all_robots(logdir): files= glob.glob(logdir+'/*.log.gz') all_robots= set() with futures.ProcessPoolExecutor() as pool: for robots in pool.map(find_robots, files): all_robots.update(robots) return all_robotsif __name__=='__main__': robots= find_all_robots('logs') for ipaddr in robots: print(ipaddr) The script produces the same result but runs about 3.5 times faster on our quad-core machine. Under the covers, a ProcessPoolExecutor creates N independent running Python interpreters where N is the number of available CPUs detected on the system. You can change the number of processes created by supplying an optional argument to ProcessPoolExecutor(N). The pool runs until the last statement in the with block is executed, at which point the process pool is shut down. However, the program will wait until allsubmitted work has been processed. Work to be submitted to a pool must be defined in a function. There are two methods for submission. If you are are trying to parallelize a list comprehension or a map() operation, you use pool.map(): 123456789# A function that performs a lot of workdef work(x): ... return result# Nonparallel coderesults = map(work, data)# Parallel implementationwith ProcessPoolExecutor() as pool: results = pool.map(work, data) Alternatively, you can manually submit single tasks using the pool.submit() method: 1234567891011# Some functiondef work(x): ... return resultwith ProcessPoolExecutor() as pool: ... # Example of submitting work to the pool future_result = pool.submit(work, arg) # Obtaining the result (blocks until done) r = future_result.result() ... If you manually submit a job, the result is an instance of Future. To obtain the actual result, you call its result() method. This blocks until the result is computed and returned by the pool. Instead of blocking, you can also arrange to have a callback function triggered upon completion instead.123456def when_done(r): print('Got:', r.result())with ProcessPoolExecutor() as pool: future_result = pool.submit(work, arg) future_result.add_done_callback(when_done) The user-supplied callback function receives an instance of Future that must be used to obtain the actual result (i.e., by calling its result() method). Using Generators As an Alternative to ThreadsYou want to implement concurrency using generators (coroutines) as an alternative to system threads. This is sometimes known as user-level threading or green threading. To implement your own concurrency using generators, you first need a fundamental insight concerning generator functions and the yield statement. Specifically, the fundamental behavior of yield is that it causes a generator to suspend its execution. By suspending execution, it is possible to write a scheduler that treats generators as a kind of “task” and alternates their execution using a kind of cooperative task switching. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&gt;&gt;&gt; def countdown(n):... while n&gt;0:... print('T-minus', n)... yield ... n-= 1... print('Blastoff!')... &gt;&gt;&gt; def countup(n):... x=0... while x &lt; n:... print('Counting up', x)... yield ... x+= 1...&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; class TaskSheduler:... def __init__(self):... self._task_queue= deque()... def new_task(self, task):... self._task_queue.append(task)... def run(self):... while self._task_queue:... task= self._task_queue.popleft()... try:... next(task)... self._task_queue.append(task)... except StopIteration:... pass... &gt;&gt;&gt; sched= TaskSheduler()&gt;&gt;&gt; sched.new_task(countdown(10))&gt;&gt;&gt; sched.new_task(countdown(5))&gt;&gt;&gt; sched.new_task(countup(15))&gt;&gt;&gt; sched.run()T-minus 10T-minus 5Counting up 0T-minus 9T-minus 4Counting up 1T-minus 8T-minus 3Counting up 2T-minus 7T-minus 2Counting up 3T-minus 6T-minus 1Counting up 4T-minus 5Blastoff!Counting up 5T-minus 4Counting up 6T-minus 3Counting up 7T-minus 2Counting up 8T-minus 1Counting up 9Blastoff!Counting up 10Counting up 11Counting up 12Counting up 13Counting up 14&gt;&gt;&gt; In practice, you probably wouldn’t use generators to implement concurrency for something as simple as shown. Instead, you might use generators to replace the use of threads when implementing actors or network servers. The following code illustrates the use of generators to implement a thread-free version of actors: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from collections import dequeclass ActorScheduler: def __init__(self): self._actors= &#123;&#125; self._msg_queue= deque() def new_actor(self, name, actor): self._msg_queue.append((actor, None)) self._actors[name]= actor def send(self, name, msg): actor= self._actors.get(name) if actor: self._msg_queue.append((actor, msg)) def run(self): while self._msg_queue: actor, msg= self._msg_queue.popleft() try: actor.send(msg) except StopIteration: passif __name__== '__main__': def printer(): while True: msg= yield print('Got:', msg) def counter(sched): while True: n= yield if n==0: break sched.send('printer', n) sched.send('counter', n-1) sched= ActorScheduler() sched.new_actor('printer', printer()) sched.new_actor('counter', counter(sched)) sched.send('counter', 10) sched.run()#resultsGot: 10Got: 9Got: 8Got: 7Got: 6Got: 5Got: 4Got: 3Got: 2Got: 1]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C11_Network_and_Web_Programming]]></title>
    <url>%2F2018%2F12%2F15%2FC11-Network-and-Web-Programming%2F</url>
    <content type="text"><![CDATA[Creating a TCP ServerYou want to implement a server that communicates with clients using the TCP Internet protocol. An easy way to create a TCP server is to use the socketserver library. 123456789101112from socketserver import BaseRequestHandler, TCPServerclass EchoHandler(BaseRequestHandler): def handle(self): print('Got connection from', self.client_address) while True: msg = self.request.recv(8192) if not msg: break self.request.send(msg)if __name__ == '__main__': serv = TCPServer(('', 20000), EchoHandler) serv.serve_forever() In this code, you define a special handler class that implements a handle() method for servicing client connections. The request attribute is the underlying client socket and client_address has client address. To test the server, run it and then open a separate Python process that connects to it: 12345678&gt;&gt;&gt; from socket import socket, AF_INET, SOCK_STREAM&gt;&gt;&gt; s = socket(AF_INET, SOCK_STREAM)&gt;&gt;&gt; s.connect(('localhost', 20000))&gt;&gt;&gt; s.send(b'Hello')5&gt;&gt;&gt; s.recv(8192)b'Hello'&gt;&gt;&gt; Here is an example that uses the StreamRequestHandler base class to put a file-like interface on the underlying socket: 1234567891011from socketserver import StreamRequestHandler, TCPServerclass EchoHandler(StreamRequestHandler): def handle(self): print('Got connection from', self.client_address) # self.rfile is a file-like object for reading for line in self.rfile: # self.wfile is a file-like object for writing self.wfile.write(line)if __name__ == '__main__': serv = TCPServer(('', 20000), EchoHandler) serv.serve_forever() socketserver makes it relatively easy to create simple TCP servers. However, you should be aware that, by default, the servers are single threaded and can only serve one client at a time. If you want to handle multiple clients, either instantiate a ForkingTCPServer or ThreadingTCPServer object instead. 12345from socketserver import ThreadingTCPServer...if __name__ == '__main__': serv = ThreadingTCPServer(('', 20000), EchoHandler) serv.serve_forever() One issue with forking and threaded servers is that they spawn a new process or thread on each client connection. There is no upper bound on the number of allowed clients, so a malicious hacker could potentially launch a large number of simultaneous connections in an effort to make your server explode. You create an instance of a normal nonthreaded server, but then launch the serve_forever() method in a pool of multiple threads. 123456789if __name__ == '__main__': from threading import Thread NWORKERS = 16 serv = TCPServer(('', 20000), EchoHandler) for n in range(NWORKERS): t = Thread(target=serv.serve_forever) t.daemon = True t.start() serv.serve_forever() Normally, a TCPServer binds and activates the underlying socket upon instantiation. However, sometimes you might want to adjust the underlying socket by setting options. To do this, supply the bind_and_activate=False argument. 12345678if __name__ == '__main__': serv = TCPServer(('', 20000), EchoHandler, bind_and_activate=False) # Set up various socket options serv.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) # Bind and activate serv.server_bind() serv.server_activate() serv.serve_forever() The StreamRequestHandler class is actually a bit more flexible, and supports some features that can be enabled through the specification of additional class variables. 123456789101112131415import socketclass EchoHandler(StreamRequestHandler): # Optional settings (defaults shown) timeout = 5 # Timeout on all socket operations rbufsize = -1 # Read buffer size wbufsize = 0 # Write buffer size disable_nagle_algorithm = False # Sets TCP_NODELAY socket option def handle(self): print('Got connection from', self.client_address) try: for line in self.rfile: # self.wfile is a file-like object for writing self.wfile.write(line) except socket.timeout: print('Timed out!') Finally, it should be noted that most of Python’s higher-level networking modules (e.g., HTTP, XML-RPC, etc.) are built on top of the socketserver functionality. 123456789101112131415161718from socket import socket, AF_INET, SOCK_STREAMdef echo_handler(address, client_sock): print('Got connection from &#123;&#125;'.format(address)) while True: msg = client_sock.recv(8192) if not msg: break client_sock.sendall(msg) client_sock.close()def echo_server(address, backlog=5): sock = socket(AF_INET, SOCK_STREAM) sock.bind(address) sock.listen(backlog) while True: client_sock, client_addr = sock.accept() echo_handler(client_addr, client_sock)if __name__ == '__main__': echo_server(('', 20000)) Creating a Simple REST-Based InterfaceOne of the easiest ways to build REST-based interfaces is to create a tiny library based on the WSGI standard. 1234567891011121314151617181920&gt;&gt;&gt; import cgi&gt;&gt;&gt; def notfound_404(environ, start_response):... start_response('404 Not Found', [('Content-type', 'text/plain')])... return [b'Not Found']... &gt;&gt;&gt; class PathDispatcher:... def __init__(self):... self.pathmap= &#123;&#125;... def __call__(self, environ, start_response):... path= environ('PATH_INFO')... params= cgi.FieldStorage(environ['wsgi.input'], environ= environ)... method= environ['REQUEST_METHOD'].lower()... environ['params']= &#123;key: params.getvalue(key) for key in params&#125;... handler= self.pathmap.get((method, path), notfound_404)... return handler(environ, start_response)... def register(self, method, path, function):... self.pathmap[method.lower(), path] = function... return function... &gt;&gt;&gt; To use this dispatcher, you simply write different handlers, such as the following: 12345678910111213141516171819202122232425262728293031323334353637383940import time_hello_resp = '''\ &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello &#123;name&#125;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello &#123;name&#125;!&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt;'''def hello_world(environ, start_response): start_response('200 OK', [ ('Content-type','text/html')]) params = environ['params'] resp = _hello_resp.format(name=params.get('name')) yield resp.encode('utf-8')_localtime_resp = '''\ &lt;?xml version="1.0"?&gt; &lt;time&gt; &lt;year&gt;&#123;t.tm_year&#125;&lt;/year&gt; &lt;month&gt;&#123;t.tm_mon&#125;&lt;/month&gt; &lt;day&gt;&#123;t.tm_mday&#125;&lt;/day&gt; &lt;hour&gt;&#123;t.tm_hour&#125;&lt;/hour&gt; &lt;minute&gt;&#123;t.tm_min&#125;&lt;/minute&gt; &lt;second&gt;&#123;t.tm_sec&#125;&lt;/second&gt; &lt;/time&gt;'''def localtime(environ, start_response): start_response('200 OK', [ ('Content-type', 'application/xml') ]) resp = _localtime_resp.format(t=time.localtime()) yield resp.encode('utf-8')if __name__ == '__main__': from resty import PathDispatcher from wsgiref.simple_server import make_server # Create the dispatcher and register functions dispatcher = PathDispatcher() dispatcher.register('GET', '/hello', hello_world) dispatcher.register('GET', '/localtime', localtime) # Launch a basic server httpd = make_server('', 8080, dispatcher) print('Serving on port 8080...') httpd.serve_forever() To test your server, you can interact with it using a browser or urllib. For example:12&gt;&gt;&gt; u = urlopen('http://localhost:8080/hello?name=Guido')&gt;&gt;&gt; u = urlopen('http://localhost:8080/localtime') In WSGI, you simply implement applications in the form of a callable that accepts this calling convention: 123import cgidef wsgi_app(environ, start_response): ... The environ argument is a dictionary that contains values inspired by the CGI interface provided by various web servers such as Apache. 123456def wsgi_app(environ, start_response): method = environ['REQUEST_METHOD'] path = environ['PATH_INFO'] # Parse the query parameters params = cgi.FieldStorage(environ['wsgi.input'], environ=environ) ... The start_response argument is a function that must be called to initiate a response. The first argument is the resulting HTTP status. The second argument is a list of (name, value) tuples that make up the HTTP headers of the response. 123def wsgi_app(environ, start_response): ... start_response('200 OK', [('Content-type', 'text/plain')]) Although WSGI applications are commonly defined as a function, as shown, an instance may also be used as long as it implements a suitable __call__() method. For example:123456class WSGIApplication: def __init__(self): ... def __call__(self, environ, start_response) ... This technique has been used to create the PathDispatcher class in the recipe. The dispatcher does nothing more than manage a dictionary mapping (method, path) pairs to handler functions. When a request arrives, the method and path are extracted and used to dispatch to a handler. In addition, any query variables are parsed and put into a dictionary that is stored as environ[‘params’]. Implementing a Simple Remote Procedure Call with XML-RPCYou want an easy way to execute functions or methods in Python programs running on remote machines.Perhaps the easiest way to implement a simple remote procedure call mechanism is to use XML-RPC. 123456789101112131415161718192021222324from xmlrpc.server import SimpleXMLRPCServerclass KeyValueServer: _rpc_methods_ = ['get', 'set', 'delete', 'exists', 'keys'] def __init__(self, address): self._data = &#123;&#125; self._serv = SimpleXMLRPCServer(address, allow_none=True) for name in self._rpc_methods_: self._serv.register_function(getattr(self, name)) def get(self, name): return self._data[name] def set(self, name, value): self._data[name] = value def delete(self, name): del self._data[name] def exists(self, name): return name in self._data def keys(self): return list(self._data) def serve_forever(self): self._serv.serve_forever()# Exampleif __name__ == '__main__': kvserv = KeyValueServer(('', 15000)) kvserv.serve_forever() Here is how you would access the server remotely from a client: 1234567891011121314&gt;&gt;&gt; from xmlrpc.client import ServerProxy&gt;&gt;&gt; s = ServerProxy('http://localhost:15000', allow_none=True)&gt;&gt;&gt; s.set('foo', 'bar')&gt;&gt;&gt; s.set('spam', [1, 2, 3])&gt;&gt;&gt; s.keys()['spam', 'foo']&gt;&gt;&gt; s.get('foo')'bar'&gt;&gt;&gt; s.get('spam')[1, 2, 3]&gt;&gt;&gt; s.delete('spam')&gt;&gt;&gt; s.exists('spam')False&gt;&gt;&gt; XML-RPC can be an extremely easy way to set up a simple remote procedure call service. All you need to do is create a server instance, register functions with it using the register_function() method, and then launch it using the serve_forever() method. 123456from xmlrpc.server import SimpleXMLRPCServerdef add(x,y): return x+yserv = SimpleXMLRPCServer(('', 15000))serv.register_function(add)serv.serve_forever() Communicating Simply Between InterpretersYou are running multiple instances of the Python interpreter, possibly on different machines, and you would like to exchange data between interpreters using messages. It is easy to communicate between interpreters if you use the multiprocessing.connection module. 123456789101112131415161718from multiprocessing.connection import Listenerimport tracebackdef echo_client(conn): try: while True: msg = conn.recv() conn.send(msg) except EOFError: print('Connection closed')def echo_server(address, authkey): serv = Listener(address, authkey=authkey) while True: try: client = serv.accept() echo_client(client) except Exception: traceback.print_exc()echo_server(('', 25000), authkey=b'peekaboo') Here is a simple example of a client connecting to the server and sending various messages: 123456789101112&gt;&gt;&gt; from multiprocessing.connection import Client&gt;&gt;&gt; c = Client(('localhost', 25000), authkey=b'peekaboo')&gt;&gt;&gt; c.send('hello')&gt;&gt;&gt; c.recv()'hello'&gt;&gt;&gt; c.send(42)&gt;&gt;&gt; c.recv()42&gt;&gt;&gt; c.send([1, 2, 3, 4, 5])&gt;&gt;&gt; c.recv()[1, 2, 3, 4, 5]&gt;&gt;&gt; Unlike a low-level socket, messages are kept intact (each object sent using send() is received in its entirety with recv()). In addition, objects are serialized using pickle. So, any object compatible with pickle can be sent or received over the connection. If you know that the interpreters are going to be running on the same machine, you can use alternative forms of networking, such as UNIX domain sockets or Windows named pipes. To create a connection using a UNIX domain socket, simply change the address to a filename such as this:s = Listener(&#39;/tmp/myconn&#39;, authkey=b&#39;peekaboo&#39;) To create a connection using a Windows named pipe, use a filename such as this:s = Listener(r&#39;\\.\pipe\myconn&#39;, authkey=b&#39;peekaboo&#39;) As a general rule, you would not be using multiprocessing to implement public-facing services. The authkey parameter to Client() and Listener() is there to help authenticate the end points of the connection. Don’t use multiprocessing if you need more low-level control over aspects of the connection. For example, if you needed to support timeouts, nonblocking I/O, or anything similar, you’re probably better off using a different library or implementing such features on top of sockets instead. Implementing Remote Procedure CallsYou want to implement simple remote procedure call (RPC) on top of a message passing layer, such as sockets, multiprocessing connections, or ZeroMQ. RPC is easy to implement by encoding function requests, arguments, and return values using pickle, and passing the pickled byte strings between interpreters. 123456789101112131415161718&gt;&gt;&gt; import pickle&gt;&gt;&gt; class RPCHandler:... def __init__(self):... self._functions= &#123;&#125;... def register_function(self, func):... self._functions[func.__name__] = func... def handle_connection(self, connection):... try:... while True:... func_name, args, kwargs= pickle.loads(connection.recv())... try:... r= self._functions[func_name](*arg, **kwargs)... connection.send(pickle.dumps(r))... except Exception as e:... connetion.send(pickle.dumps(e))... except EOFError:... pass... To use this handler, you need to add it into a messaging server. There are many possible choices, but the multiprocessing library provides a simple option. 1234567891011121314151617181920&gt;&gt;&gt; from multiprocessing.connection import Listener&gt;&gt;&gt; from threading import Thread&gt;&gt;&gt; def rpc_server(handler, address, authkey):... sock= Listener(address, authkey= authkey)... while True:... client= sock.accept()... t= Thread(target= handler.handle_connection, args=(client,))... t.daemon= True... t.start()... &gt;&gt;&gt; def add(x,y):... return x+y... &gt;&gt;&gt; def sub(x, y):... return x- y... &gt;&gt;&gt; handler= RPCHandler()&gt;&gt;&gt; handler.register_function(add)&gt;&gt;&gt; handler.register_function(sub)&gt;&gt;&gt; rpc_server(handler, ('localhost', 17000), authkey= b'peekaboo') To access the server from a remote client, you need to create a corresponding RPC proxy class that forwards requests. 12345678910111213141516171819202122232425&gt;&gt;&gt; import pickle&gt;&gt;&gt; class RPCProxy:... def __init__(self, connection):... self._connection= connection... def __getattr__(self, name):... def do_rpc(*args, **kwargs):... self._connection.send(pickle.dumps((name, args, kwargs)))... results= pickle.loads(self._connection.recv())... if isinstance(result, Exception):... raise result... return result... return do_rpc...&gt;&gt;&gt; from multiprocessing.connection import Client&gt;&gt;&gt; c=Client((&apos;localhost&apos;, 17000), authkey= b&apos;peekaboo&apos;)&gt;&gt;&gt; proxy= RPCProxy(c)&gt;&gt;&gt; proxy.add(2,3)-1&gt;&gt;&gt; proxy.sub([1, 2], 4)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;rpcserver.py&quot;, line 37, in do_rpc raise resultTypeError: unsupported operand type(s) for -: &apos;list&apos; and &apos;int&apos;&gt;&gt;&gt; The general idea of the RPCHandler and RPCProxy classes is relatively simple. If a client wants to call a remote function, such as foo(1, 2, z=3), the proxy class creates a tuple (‘foo’, (1, 2), {‘z’: 3}) that contains the function name and arguments. This tuple is pickled and sent over the connection. This is performed in the do_rpc() closure that’s returned by the __getattr__() method of RPCProxy. Authenticating Clients SimplyYou want a simple way to authenticate the clients connecting to servers in a distributed system, but don’t need the complexity of something like SSL. Simple but effective authentication can be performed by implementing a connection handshake using the hmac module. 12345678910111213141516&gt;&gt;&gt; import hmac&gt;&gt;&gt; import os&gt;&gt;&gt; def client_authenticate(connection, secret_key):... message= connection.recv(32)... hash= hmac.new(secret_key, message)... digest= hash.digest()... connection.send(digest)... &gt;&gt;&gt; def server_authenticate(connection, secret_key):... message= os.urandom(32)... connection.send(message)... hash= hmac.new(secret_key, message)... digest= hash.digest()... response= connection.recv(len(digest))... return hmac.compare_digest(digest, response)... The general idea is that upon connection, the server presents the client with a message of random bytes (returned by os.urandom(), in this case). The client and server both compute a cryptographic hash of the random data using hmac and a secret key known only to both ends. The client sends its computed digest back to the server, where it is compared and used to decide whether or not to accept or reject the connection. Comparison of resulting digests should be performed using the hmac.compare_digest() function. This function has been written in a way that avoids timing-analysisbased attacks and should be used instead of a normal comparison operator (==). To use these functions, you would incorporate them into existing networking or messaging code. For example, with sockets, the server code might look something like this: 123456789101112131415161718192021&gt;&gt;&gt; from socket import socket, AF_INET, SOCK_STREAM&gt;&gt;&gt; secret_key= b'peekaboo'&gt;&gt;&gt; def echo_handler(client_sock):... if not server_authenticate(client_sock, secret_key):... client_sock.close()... return... while True:... msg= client_sock.recv(8192)... if not msg:... break... client_sock.sendall(msg)... &gt;&gt;&gt; def echo_server(address):... s= socket(AF_INET, SOCK_STREAM)... s.bind(address)... s.listen(5)... while True:... c, a= s.accept()... echo_handler(c)... &gt;&gt;&gt; echo_server(('', 18000)) Within a client, you would do this: 12345678from socket import socket, AF_INET, SOCK_STREAMsecret_key = b'peekaboo's = socket(AF_INET, SOCK_STREAM)s.connect(('localhost', 18000))client_authenticate(s, secret_key)s.send(b'Hello World')resp = s.recv(1024)... Adding SSL to Network ServicesThe ssl module provides support for adding SSL to low-level socket connections. In particular, the ssl.wrap_socket() function takes an existing socket and wraps an SSL layer around it. A simple echo server that presents a server certificate to connecting clients: 123456789101112131415161718192021222324252627282930from socket import socket, AF_INET, SOCK_STREAMimport sslKEYFILE= 'server_key.pem'CERTFILE= 'server_cert.pem'def echo_client(s): while True: data= s.recv(8192) if data== b'': break s.send(data) s.close() print('Connection closed')def echo_server(address): s= socket(AF_INET, SOCK_STREAM) s.bind(address) s.listen(1) s_ssl= ssl.wrap_socket(s, keyfile= KEYFILE, certfile= CERTFILE, server_side= True) while True: try: c, a= s_ssl.accept() print('Got connection', c, a) echo_client(c) except Exception as e: print('&#123;&#125;: &#123;&#125;'.format(e.__class__.__name__, e))echo_server(('', 20000)) Here’s an interactive session that shows how to connect to the server as a client. The client requires the server to present its certificate and verifies it: 123456789101112&gt;&gt;&gt; from socket import socket, AF_INET, SOCK_STREAM&gt;&gt;&gt; import ssl&gt;&gt;&gt; s = socket(AF_INET, SOCK_STREAM)&gt;&gt;&gt; s_ssl = ssl.wrap_socket(s,... cert_reqs=ssl.CERT_REQUIRED,... ca_certs = &apos;server_cert.pem&apos;)&gt;&gt;&gt; s_ssl.connect((&apos;localhost&apos;, 20000))&gt;&gt;&gt; s_ssl.send(b&apos;Hello World?&apos;)12&gt;&gt;&gt; s_ssl.recv(8192)b&apos;Hello World?&apos;&gt;&gt;&gt; The problem with all of this low-level socket hacking is that it doesn’t play well with existing network services already implemented in the standard library. For example, most server code (HTTP, XML-RPC, etc.) is actually based on the socketserver library. Client code is also implemented at a higher level. It is possible to add SSL to existing services, but a slightly different approach is needed. First, for servers, SSL can be added through the use of a mixin class like this: 12345678910111213141516171819import sslclass SSLMixin: '''Mixin class that adds support for SSL to existing servers based on the socketserver module.''' def __init__(self, *args, keyfile= None, certfile= None, ca_certs= None, cert_reqs= ssl.CERT_NONE, **kwargs): self._keyfile= keyfile self._certfile= certfile self._ca_certs= ca_certs self._cert_reqs= cert_reqs super().__init__(*args, **kwargs) def get_request(self): client, addr= super().get_request() client_ssl= ssl.wrap_socket(client, keyfile= self._keyfile, certfile=self._certfile, ca_certs= self._ca_certs, cert_reqs= self._cert_reqs, server_side= True ) return client_ssl, addr To use this mixin class, you can mix it with other server classes. For example, here’s an example of defining an XML-RPC server that operates over SSL:123from xmlrpc.server import SimpleXMLRPCServerclass SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer): pass Here’s the XML-RPC server modified only slightly to use SSL: 1234567891011121314151617181920class KeyValueServer: _rpc_methods= ['get', 'set', 'delete', 'exists', 'keys'] def __init__(self, *args, **kwargs): self._data= &#123;&#125; self._serv= SSLSimpleXMLRPCServer(*args, allow_none= True, **kwargs) for name in self._rpc_methods: self._serv.register_function(getattr(self, name)) def get(self, name): self._data[name] def set(self, name, value): self._data[name]= value def delete(self, name): del self._data[name] def exists(self, name): return name in self._data def keys(self): return list(self._data) def serve_forever(self): self._serv.serve_forever()if __name__== '__main__': KEYFILE= 'server_key.pem' CERTFILE= 'server_cert.pem' kvserv= KeyValueServer(('', 15000), keyfile= KEYFILE, certfile= CERTFILE) kvserv.serve_forever() To use this server, you can connect using the normal xmlrpc.client module. Just specify a https: in the URL. For example: 1234567891011121314&gt;&gt;&gt; from xmlrpc.client import ServerProxy&gt;&gt;&gt; s = ServerProxy('https://localhost:15000', allow_none=True)&gt;&gt;&gt; s.set('foo','bar')&gt;&gt;&gt; s.set('spam', [1, 2, 3])&gt;&gt;&gt; s.keys()['spam', 'foo']&gt;&gt;&gt; s.get('foo')'bar'&gt;&gt;&gt; s.get('spam')[1, 2, 3]&gt;&gt;&gt; s.delete('spam')&gt;&gt;&gt; s.exists('spam')False&gt;&gt;&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C09_Metaprogramming]]></title>
    <url>%2F2018%2F12%2F15%2FC09-Metaprogramming%2F</url>
    <content type="text"><![CDATA[Putting a Wrapper Around a FunctionIf you ever need to wrap a function with extra code, define a decorator function. 123456789101112&gt;&gt;&gt; import time&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def timethis(func):... @wraps(func)... def wrapper(*args, **kwargs):... start= time.time()... result= func(*args, **kwargs)... end= time.time()... print(func.__name__, end- start)... return result... return wrapper... Here is an example of using the decorator: 12345678910&gt;&gt;&gt; @timethis... def countdown(n):... while n &gt; 0:... n-= 1... &gt;&gt;&gt; countdown(100000)countdown 0.012996196746826172&gt;&gt;&gt; countdown(10000000)countdown 0.8619716167449951&gt;&gt;&gt; The return value of a decorator is almost always the result of calling func(*args, **kwargs), where func is the original unwrapped function. The use of the decorator @wraps(func) in the solution is an easy to forget but important technicality related to preserving function metadata. Preserving Function Metadata When Writing DecoratorsWhenever you define a decorator, you should always remember to apply the @wraps decorator from the functools library to the underlying wrapper function. 1234567891011121314151617&gt;&gt;&gt; @timethis... def countdown(n:int):... '''... Counts down... '''... while n &gt; 0:... n -= 1...&gt;&gt;&gt; countdown(100000)countdown 0.008917808532714844&gt;&gt;&gt; countdown.__name__'countdown'&gt;&gt;&gt; countdown.__doc__'\n\tCounts down\n\t'&gt;&gt;&gt; countdown.__annotations__&#123;'n': &lt;class 'int'&gt;&#125;&gt;&gt;&gt; Copying decorator metadata is an important part of writing decorators. If you forget to use @wraps, you’ll find that the decorated function loses all sorts of useful information. For instance, if omitted, the metadata in the last example would look like this: 12345&gt;&gt;&gt; countdown.__name__'wrapper'&gt;&gt;&gt; countdown.__doc__&gt;&gt;&gt; countdown.__annotations__&#123;&#125; An important feature of the @wraps decorator is that it makes the wrapped function available to you in the __wrapped__ attribute. 12&gt;&gt;&gt; countdown.__wrapped__(100000)&gt;&gt;&gt; The presence of the __wrapped__ attribute also makes decorated functions properly expose the underlying signature of the wrapped function. 1234&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; print(signature(countdown))(n:int)&gt;&gt;&gt; Unwrapping a DecoratorAssuming that the decorator has been implemented properly using @wraps, you can usually gain access to the original function by accessing the __wrapped__ attribute. If multiple decorators have been applied to a function, the behavior of accessing __wrapped__ is currently undefined and should probably be avoided. 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def decorator1(func):... @wraps(func)... def wrapper(*args, **kwargs):... print('Decorator 1')... return func(*args, **kwargs)... return wrapper... &gt;&gt;&gt; def decorator2(func):... @wraps(func)... def wrapper(*args, **kwargs):... print('Decorator 2')... return func(*args, **kwargs)... return wrapper... &gt;&gt;&gt; @decorator1... @decorator2... def add(x,y): return x+ y... &gt;&gt;&gt; add(2,3)Decorator 1Decorator 25&gt;&gt;&gt; add.__wrapped__(2,3)Decorator 25&gt;&gt;&gt; add.__wrapped__.__wrapped__(2,3)5&gt;&gt;&gt; Be aware that not all decorators utilize @wraps, and thus, they may not work as described. In particular, the built-in decorators @staticmethod and @classmethod create descriptor objects that don’t follow this convention (instead, they store the original function in a __func__ attribute). Defining a Decorator That Takes ArgumentsSuppose you want to write a decorator that adds logging to a function, but allows the user to specify the logging level and other details as arguments. Here is how you might define the decorator: 123456789101112131415161718192021&gt;&gt;&gt; import logging&gt;&gt;&gt; &gt;&gt;&gt; def logged(level, name= None, message= None):... '''... Add logging to a function. level is the logging... level, name is the logger name, and message is the... log message. If name and message aren't specified,... they default to the function's module and name.... '''... def decorate(func):... logname= name if name else func.__module__... log= logging.getLogger(logname)... logmsg= message if message else func.__name__... ... @wraps(func)... def wrapper(*args, **kwargs):... log.log(level, logmsg)... return func(*args, **kwargs)... return wrapper... return decorate... The outermost function logged() accepts the desired arguments and simply makes them available to the inner functions of the decorator. The inner function decorate() accepts a function and puts a wrapper around it as normal. The key part is that the wrapper is allowed to use the arguments passed to logged(). Defining a Decorator with User Adjustable AttributesYou want to write a decorator function that wraps a function, but has user adjustable attributes that can be used to control the behavior of the decorator at runtime. Here is a solution that expands on the last recipe by introducing accessor functions that change internal variables through the use of nonlocal variable declarations. The accessor functions are then attached to the wrapper function as function attributes. 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def attach_wrapper(obj, func= None):... if func is None:... return partial(attach_wrapper, obj)... setattr(obj, func.__name__, func)... return func... &gt;&gt;&gt; def logged(level, name= None, message= None):... def decorate(func):... logname= name if name else func.__name__... log= logging.getLogger(logname)... logmsg= message if message else func.__name__... ... @wraps(func)... def wrapper(*args, **kwargs):... log.log(level, logmsg)... return func(*args, **kwargs)... @attach_wrapper(wrapper)... def set_level(newlevel):... nonlocal level... level= newlevel... @attach_wrapper(wrapper)... def set_message(newmsg):... nonlocal logmsg... logmsg= newmsg... ... return wrapper... return decorate... Here is an interactive session that shows the various attributes being changed after definition: 12345678910111213141516&gt;&gt;&gt; @logged(logging.DEBUG)... def add(x, y):... return x+y... &gt;&gt;&gt; logging.basicConfig(level= logging.DEBUG)&gt;&gt;&gt; add(2,3)DEBUG:add:add5&gt;&gt;&gt; add.set_message('Add called')&gt;&gt;&gt; add(2, 3)DEBUG:add:Add called5&gt;&gt;&gt; add.set_level(logging.WARNING)&gt;&gt;&gt; add(2, 3)WARNING:add:Add called5 The key to this recipe lies in the accessor functions [e.g., set_message() and set_level()] that get attached to the wrapper as attributes. Each of these accessors allows internal parameters to be adjusted through the use of nonlocal assignments. An amazing feature of this recipe is that the accessor functions will propagate through multiple levels of decoration (if all of your decorators utilize **@functools.wraps**).1234567891011121314151617181920212223242526&gt;&gt; def timethis(func):... @wraps(func)... def wrapper(*args, **kwargs):... start= time.time()... result= func(*args, **kwargs)... end= time.time()... print(func.__name__, end- start)... return result... return wrapper... &gt;&gt;&gt; @timethis... @logged(logging.DEBUG)... def countdown(n):... while n &gt; 0:... n-= 1... &gt;&gt;&gt; countdown(10000)DEBUG:countdown:countdowncountdown 0.0149993896484375&gt;&gt;&gt; countdown.set_level(logging.WARNING)&gt;&gt;&gt; countdown.set_message('Counting down to 0')&gt;&gt;&gt; countdown(10000)WARNING:countdown:Counting down to 0countdown 0.0010001659393310547&gt;&gt;&gt; You’ll also find that it all still works exactly the same way if the decorators are composed in the opposite order, like this: 12345@logged(logging.DEBUG)@timethisdef countdown(n): while n &gt; 0: n -= 1 One extremely subtle facet of this recipe is the choice to use accessor functions in the first place. For example, you might consider an alternative formulation solely based on direct access to function attributes like this: 12345678910...@wraps(func)def wrapper(*args, **kwargs): wrapper.log.log(wrapper.level, wrapper.logmsg) return func(*args, **kwargs)# Attach adjustable attributeswrapper.level = levelwrapper.logmsg = logmsgwrapper.log = log... This approach would work to a point, but only if it was the topmost decorator. If you had another decorator applied on top (such as the @timethis example), it would shadow the underlying attributes and make them unavailable for modification. The use of accessor functions avoids this limitation. Defining a Decorator That Takes an Optional ArgumentYou would like to write a single decorator that can be used without arguments, such as @decorator, or with optional arguments, such as @decorator(x,y,z). Here is a variant of the logging code: 12345678910111213141516171819&gt;&gt;&gt; def logged(func= None, *, level= logging.DEBUG, name= None, message= None):... if func is None:... return partial(logged, level= level, name= name, message= message)... logname= name if name else func.__module__... log= logging.getLogger(logname)... logmsg= message if message else func.__name__... @wraps(func)... def wrapper(*args, **kwargs):... log.log(level, logmsg)... return func(*args, **kwargs)... return wrapper... # Example use@loggeddef add(x, y): return x + y@logged(level=logging.CRITICAL, name='example')def spam(): print('Spam!') As you can see from the example, the decorator can be used in both a simple form (i.e.,@logged) or with optional arguments supplied. For a decorator taking arguments such as this: 123@logged(level=logging.CRITICAL, name='example')def spam(): print('Spam!') The calling sequence is as follows:1234def spam(): print('Spam!')spam = logged(level=logging.CRITICAL, name='example')(spam) On the initial invocation of logged(), the function to be wrapped is not passed. Thus, in the decorator, it has to be optional. This, in turn, forces the other arguments to be specified by keyword. Furthermore, when arguments are passed, a decorator is supposed to return a function that accepts the function and wraps it. To do this, the solution uses a clever trick involving functools.partial. Specifically, it simply returns a partially applied version of itself where all arguments are fixed except for the function to be wrapped. Enforcing Type Checking on a Function Using a DecoratorYou want to optionally enforce type checking of function arguments as a kind of assertion or contract. Here is an implementation of the @typeassert decorator: 1234567891011121314151617181920&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def typeassert(*ty_args, **ty_kwargs):... def decorate(func):... if not __debug__:... return func... sig= signature(func)... bound_types=sig.bind_partial(*ty_args, **ty_kwargs).arguments... ... @wraps(func)... def wrapper(*args, **kwargs):... bound_values= sig.bind(*args, **kwargs)... for name, value in bound_values.arguments.items():... if name in bound_types:... if not isinstance(value, bound_types[name]):... raise TypeError('Argument &#123;&#125; must be &#123;&#125;'.format(name, bound_types[name]))... return func(*args, **kwargs)... return wrapper... return decorate... You will find that this decorator is rather flexible, allowing types to be specified for all or a subset of a function’s arguments. Moreover, types can be specified by position or by keyword. 1234567891011121314&gt;&gt;&gt; @typeassert(int, z=int)... def spam(x, y, z=42):... print(x, y, z)...&gt;&gt;&gt; spam(1, 2, 3)1 2 3&gt;&gt;&gt; spam(1, 'hello', 3)1 hello 3&gt;&gt;&gt; spam(1, 'hello', 'world')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "contract.py", line 33, in wrapperTypeError: Argument z must be &lt;class 'int'&gt;&gt;&gt;&gt; In the solution, the following code fragment returns the function unmodified if the value of the global __debug__ variable is set to False. inspect.signature() function, it allows you to extract signature information from a callable. 123456789101112131415161718&gt;&gt;&gt; def spam(x,y,z=42):... pass... &gt;&gt;&gt; sig= signature(spam)&gt;&gt;&gt; print(sig)(x, y, z=42)&gt;&gt;&gt; sig.parametersmappingproxy(OrderedDict([('x', &lt;Parameter "x"&gt;), ('y', &lt;Parameter "y"&gt;), ('z', &lt;Parameter "z=42"&gt;)]))&gt;&gt;&gt; sig.parameters['x'].name'x'&gt;&gt;&gt; sig.parameters['x'].default&lt;class 'inspect._empty'&gt;&gt;&gt;&gt; sig.parameters['z'].default42&gt;&gt;&gt; sig.parameters['x'].kind&lt;_ParameterKind.POSITIONAL_OR_KEYWORD: 1&gt;&gt;&gt;&gt; sig.parameters['z'].kind&lt;_ParameterKind.POSITIONAL_OR_KEYWORD: 1&gt; We use the bind_partial() method of signatures to perform a partial binding of the supplied types to argument names. 12345&gt;&gt; bound_types= sig.bind_partial(int, z= int)&gt;&gt;&gt; bound_types&lt;BoundArguments (x=&lt;class 'int'&gt;, z=&lt;class 'int'&gt;)&gt;&gt;&gt;&gt; bound_types.argumentsOrderedDict([('x', &lt;class 'int'&gt;), ('z', &lt;class 'int'&gt;)]) The most important part of the binding is the creation of the ordered dictionary bound_types.arguments. This dictionary maps the argument names to the supplied values in the same order as the function signature. In the actual wrapper function made by the decorator, the sig.bind() method is used. bind() is like bind_partial() except that it does not allow for missing arguments. 1234&gt;&gt;&gt; bound_values= sig.bind(1,2,3)&gt;&gt;&gt; bound_values.argumentsOrderedDict([('x', 1), ('y', 2), ('z', 3)])&gt;&gt;&gt; Defining Decorators As ClassesTo define a decorator as an instance, you need to make sure it implements the __call__() and __get__() methods. 12345678910111213&gt;&gt;&gt; class Profiles:... def __init__(self, func):... wraps(func)(self)... self.ncalls= 0... def __call__(self, *args, **kwargs):... self.ncalls += 1... return self.__wrapped__(*args, **kwargs)... def __get__(self, instance, cls):... if instance is None:... return self... else:... return types.MethodType(self, instance)... To use this class, you use it like a normal decorator, either inside or outside of a class: 123456789101112131415161718192021222324&gt;&gt;&gt; @Profiles... def add(x, y):... return x +y... &gt;&gt;&gt; class Spam:... @Profiles... def bar(self, x):... print(self, x)... &gt;&gt;&gt; add(2,3)5&gt;&gt;&gt; add(2,3)5&gt;&gt;&gt; add.ncalls2&gt;&gt;&gt; s=Spam()&gt;&gt;&gt; s.bar(1)&lt;__main__.Spam object at 0x03D8BEF0&gt; 1&gt;&gt;&gt; s.bar(2)&lt;__main__.Spam object at 0x03D8BEF0&gt; 2&gt;&gt;&gt; s.bar.ncalls2&gt;&gt;&gt; Spam.bar.ncalls2 First, the use of the functools.wraps() function serves the same purpose here as it does in normal decorators—namely to copy important metadata from the wrapped function to the callable instance. Second, it is common to overlook the __get__() method shown in the solution. If you omit the __get__() and keep all of the other code the same, you’ll find that bizarre things happen when you try to invoke decorated instance methods. 12345&gt;&gt;&gt; s = Spam()&gt;&gt;&gt; s.bar(3)Traceback (most recent call last):...TypeError: spam() missing 1 required positional argument: 'x' The reason it breaks is that whenever functions implementing methods are looked up in a class, their __get__() method is invoked as part of the descriptor protocol. In this case, the purpose of __get__() is to create a bound method object (which ultimately supplies the self argument to the method). Here is an example that illustrates the underlying mechanics: 1234567&gt;&gt;&gt; s = Spam()&gt;&gt;&gt; def grok(self, x):... pass...&gt;&gt;&gt; grok.__get__(s, Spam)&lt;bound method Spam.grok of &lt;__main__.Spam object at 0x100671e90&gt;&gt;&gt;&gt;&gt; In this recipe, the __get__() method is there to make sure bound method objects get created properly. __get__()creates a bound method manually for use here. Bound methods only get created if an instance is being used. If the method is accessed on a class, the instance argument to __get__() is set to None and the Profiled instance itself is just returned. If you want to avoid some of this of this mess, you might consider an alternative formulation of the decorator using closures and nonlocal variables. 1234567891011121314&gt;&gt;&gt; def profiles(func):... ncalls= 0... @wraps(func)... def wrapper(*args, **kwargs):... nonlocal ncalls... ncalls+= 1... return func(*args, **kwargs)... wrapper.ncalls= lambda: ncalls... return wrapper... &gt;&gt;&gt; @profiles... def add(x, y):... return x + y... This example almost works in exactly the same way except that access to ncalls is now provided through a function attached as a function attribute. For example: 1234567&gt;&gt;&gt; add(2, 3)5&gt;&gt;&gt; add(4, 5)9&gt;&gt;&gt; add.ncalls()2&gt;&gt;&gt; Applying Decorators to Class and Static MethodsApplying decorators to class and static methods is straightforward, but make sure that your decorators are applied before @classmethod or @staticmethod. 12345678910&gt;&gt;&gt; def timethis(func):... @wraps(func)... def wrapper(*args, **kwargs):... start= time.time()... r= func(*args, **kwargs)... end= time.time()... print(end- start)... return r... return wrapper... If you get the order of decorators wrong, you’ll get an error. For example, if you use the following: 12345678class Spam: ... @timethis @staticmethod def static_method(n): print(n) while n &gt; 0: n -= 1 Then the static method will crash: 1234567&gt;&gt;&gt; Spam.static_method(1000000)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "timethis.py", line 6, in wrapper start = time.time()TypeError: 'staticmethod' object is not callable&gt;&gt;&gt; One situation where this recipe is of critical importance is in defining class and static methods in abstract base classes, if you want to define an abstract class method, you can use this code:1234567from abc import ABCMeta, abstractmethodclass A(metaclass=ABCMeta): @classmethod @abstractmethod def method(cls): pass Writing Decorators That Add Arguments to Wrapped FunctionsExtra arguments can be injected into the calling signature using keyword-only arguments: 1234567891011121314151617&gt;&gt;&gt; def optional_debug(func):... @wraps(func)... def wrapper(*args, debug= False, **kwargs):... if debug:... print('Calling', func.__name__)... return func(*args, **kwargs)... return wrapper... &gt;&gt;&gt; @optional_debug... def spam(a, b, c):... print(a, b, c)... &gt;&gt;&gt; spam(1,2,3)1 2 3&gt;&gt;&gt; spam(1,2,3, debug= True)Calling spam1 2 3 One tricky part here concerns a potential name clash between the added argument and the arguments of the function being wrapped. For example, if the @optional_debug decorator was applied to a function that already had a debug argument, then it would break. If that’s a concern, an extra check could be added: 12345678910&gt;&gt;&gt; def optional_debug(func):... if 'debug' in inspect.getargspec(func).args:... raise TypeError('debug argument already defined')... @wraps(func)... def wrapper(*args, debug= False, **kwargs):... if debug:... print('Calling', func.__name__)... return func(*args, **kwargs)... return wrapper... A final refinement to this recipe concerns the proper management of function signatures. An astute programmer will realize that the signature of wrapped functions is wrong. For example: 12345678&gt;&gt;&gt; @optional_debug... def add(x,y):... return x+y...&gt;&gt;&gt; import inspect&gt;&gt;&gt; print(inspect.signature(add))(x, y)&gt;&gt;&gt; This can be fixed by making the following modification: 12345678910111213141516171819&gt;&gt;&gt; def optional_debug(func):... if 'debug' in inspect.getargspec(func).args:... raise TypeError('debug argument already defined')... ... @wraps(func)... def wrapper(*args, debug= False, **kwargs):... if debug:... print('Calling', func.__name__)... return func(*args, **kwargs)... ... ... sig= inspect.signature(func)... params= list(sig.parameters.values())... params.append(inspect.Parameter('debug', inspect.Parameter.KEYWORD_ONLY, default= False))... wrapper.__signature__= sig.replace(parameters= params)... ... return wrapper... &gt;&gt;&gt; With this change, the signature of the wrapper will now correctly reflect the presence of the debug argument. For example: 123456789&gt;&gt;&gt; @optional_debug... def add(x,y):... return x+y...&gt;&gt;&gt; print(inspect.signature(add))(x, y, *, debug=False)&gt;&gt;&gt; add(2,3)5&gt;&gt;&gt; Using Decorators to Patch Class DefinitionsYou want to inspect or rewrite portions of a class definition to alter its behavior, but without usinginheritance or metaclasses. 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; def log_getattribute(cls):... orig_attribute= cls.__getattribute__... ... def new_getattribute(self, name):... print('getting:', name)... return orig_attribute(self, name)... ... cls.__getattribute__= new_getattribute... return cls... &gt;&gt;&gt; @log_getattribute... class A:... def __init__(self, x):... self.x= x... def spam(self): pass... &gt;&gt;&gt; a=A(44)getting: __dict__getting: __class__getting: __dict__getting: __dict__getting: __dict__&gt;&gt;&gt; a.xgetting: x44getting: __dict__getting: __class__getting: __dict__getting: __dict__getting: __dict__getting: spam&gt;&gt;&gt; a.spam()getting: spam An alternative implementation of the solution might involve inheritance, as in the following: 123456789101112131415161718192021&gt;&gt;&gt; class LoggedGetattribute:... def __getattribute__(self, name):... print('getting:', name)... return super().__getattribute__(name)... &gt;&gt;&gt; class A(LoggedGetattribute):... def __init__(self, x):... self.x= x... def spam(self):... pass... &gt;&gt;&gt; b=A(4)getting: __dict__getting: __class__getting: __dict__getting: __dict__getting: __dict__&gt;&gt;&gt; b.xgetting: x4&gt;&gt;&gt; Using a Metaclass to Control Instance CreationYou want to change the way in which instances are created in order to implement singletons, caching, or other similar features. As Python programmers know, if you define a class, you call it like a function to create instances. For example: 12345class Spam: def __init__(self, name): self.name = namea = Spam('Guido')b = Spam('Diana') If you want to customize this step, you can do it by defining a metaclass and reimplementing its __call__() method in some way. Now, suppose you want to implement the singleton pattern: 123456789101112131415&gt;&gt;&gt; class Singleton(type):... def __init__(self, *args, **kwargs):... self.__instance= None... super().__init__(*args, **kwargs)... def __call__(self, *args, **kwargs):... if self.__instance is None:... self.__instance= super().__call__(*args, **kwargs)... return self.__instance... else:... return self.__instance... &gt;&gt;&gt; class Spam(metaclass= Singleton):... def __init__(self):... print('Creating Spam')... In this case, only one instance ever gets created. 12345678&gt;&gt;&gt; a=Spam()Creating Spam&gt;&gt;&gt; b=Spam()&gt;&gt;&gt; c=Spam()&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a is cTrue Suppose you want to create cached instances: 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import weakref&gt;&gt;&gt; class Cached(type):... def __init__(self, *args, **kwargs):... super().__init__(*args, **kwargs)... self.__cache= weakref.WeakValueDictionary()... def __call__(self, *args):... if args in self.__cache:... return self.__cache[args]... else:... obj= super().__call__(*args)... self.__cache[args]= obj... return obj... &gt;&gt;&gt; class Spam(metaclass= Cached):... def __init__(self, name):... print('Creating Spam(&#123;!r&#125;)'.format(name))... self.name= name... &gt;&gt;&gt; a=Spam('Guido')Creating Spam('Guido')&gt;&gt;&gt; b=Spam('Guido')&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; c=Spam('Diana')Creating Spam('Diana')&gt;&gt;&gt; a.cc='de'&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; b.cc'de'&gt;&gt;&gt; Capturing Class Attribute Definition OrderYou want to automatically record the order in which attributes and methods are defined inside a class body so that you can use it in various operations (e.g., serializing, mapping to databases, etc.). 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; class Typed:... _expected_type= type(None)... def __init__(self, name= None):... self._name= name... def __set__(self, instance, value):... if not isinstance(value, self._expected_type):... raise TypeError('Expected '+ str(self._expected_type))... instance.__dict__[self._name]= value... &gt;&gt;&gt; class Integer(Typed): _expected_type = int... &gt;&gt;&gt; class Float(Typed): _expected_type = float... &gt;&gt;&gt; class String(Typed): _expected_type = str... &gt;&gt;&gt; class OrderedMeta(type):... def __new__(cls, clsname, bases, clsdict):... print('cls:&#123;&#125;\nclsname: &#123;&#125;\nbases: &#123;&#125;\nclsdict: &#123;&#125;'.format(cls, clsname,bases,clsdict))... d= dict(clsdict)... order=[]... for name, value in clsdict.items():... if isinstance(value, Typed):... value._name= name... order.append(name)... d['_order'] = order... return type.__new__(cls, clsname, bases, d)... ... @classmethod... def __prepare__(cls, clsname, bases):... return OrderedDict()... In this metaclass, the definition order of descriptors is captured by using an OrderedDict during the execution of the class body. The resulting order of names is then extracted from the dictionary and stored into a class attribute _order. 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; class Structure(metaclass= OrderedMeta):... def as_csv(self):... return ','.join(str(getattr(self, name)) for name in self._order)... cls:&lt;class '__main__.OrderedMeta'&gt;clsname: Structurebases: ()clsdict: OrderedDict([('__module__', '__main__'), ('__qualname__', 'Structure'), ('as_csv', &lt;function Structure.as_csv at 0x0399F108&gt;)])&gt;&gt;&gt; class Stock(Structure):... name= String()... shares=Integer()... price= Float()... def __init__(self, name, shares, price):... self.name= name... self.shares= shares... self.price= price... cls:&lt;class '__main__.OrderedMeta'&gt;clsname: Stockbases: (&lt;class '__main__.Structure'&gt;,)clsdict: OrderedDict([('__module__', '__main__'), ('__qualname__', 'Stock'), ('name', &lt;__main__.String object at 0x039A4370&gt;), ('shares', &lt;__main__.Integer object at 0x039A46B0&gt;), ('price', &lt;__main__.Float object at 0x039A47B0&gt;), ('__init__', &lt;function Stock.__init__ at 0x0399F660&gt;)])&gt;&gt;&gt; s= Stock('ff',12,3.5)&gt;&gt;&gt; s.name'ff'&gt;&gt;&gt; s.as_csv()'ff,12,3.5'&gt;&gt;&gt; t= Stock(3,3,3.5)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 6, in __init__ File "&lt;stdin&gt;", line 7, in __set__TypeError: Expected &lt;class 'str'&gt;&gt;&gt;&gt; The entire key to this recipe is the __prepare__() method, which is defined in the OrderedMeta metaclass. This method is invoked immediately at the start of a class definition with the class name and base classes. It must then return a mapping object to use when processing the class body. By returning an OrderedDict instead of a normal dictionary, the resulting definition order is easily captured. It is possible to extend this functionality even further if you are willing to make your own dictionary-like objects. For example, consider this variant of the solution that rejects duplicate definitions: 12345678910111213141516171819&gt;&gt;&gt; class NoDupOrderedDict(OrderedDict):... def __init__(self, clsname):... self.clsname= clsname... super().__init__()... def __setitem__(self, name, value):... if name in self:... raise TypeError('&#123;&#125; already defined in &#123;&#125;'.format(name, self.clsname))... super().__setitem__(name, value)... &gt;&gt;&gt; class OrderedMeta(type):... def __new__(cls, clsname, bases, clsdict):... d= dict(clsdict)... d['_order']= [name for name in clsdict if name[0]!= '_']... return type.__new__(cls, clsname, bases, d)... ... @classmethod... def __prepare__(cls, clsname, bases):... return NoDupOrderedDict(clsname)... Here’s what happens if you use this metaclass and make a class with duplicate entries: 12345678910&gt;&gt;&gt; class A(metaclass= OrderedMeta):... def spam(self): pass... def spam(self): pass... Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in A File "&lt;stdin&gt;", line 7, in __setitem__TypeError: spam already defined in A&gt;&gt;&gt; Defining a Metaclass That Takes Optional ArgumentsIn custom metaclasses, additional keyword arguments can be supplied, like this: 12class Spam(metaclass=MyMeta, debug=True, synchronize=True): ... To support such keyword arguments in a metaclass, make sure you define them on the __prepare__(), __new__(), and __init__() methods using keyword-only arguments, like this: 1234567891011121314151617class MyMeta(type): # Optional @classmethod def __prepare__(cls, name, bases, *, debug=False, synchronize=False): # Custom processing ... return super().__prepare__(name, bases) # Required def __new__(cls, name, bases, ns, *, debug=False, synchronize=False): # Custom processing ... return super().__new__(cls, name, bases, ns) # Required def __init__(self, name, bases, ns, *, debug=False, synchronize=False): # Custom processing ... super().__init__(name, bases, ns) The __prepare__() method is called first and used to create the class namespace prior to the body of any class definition being processed. Normally, this method simply returns a dictionary or other mapping object. The __new__() method is used to instantiate the resulting type object. It is called after the class body has been fully executed. The __init__() method is called last and used to perform any additional initialization steps. When writing metaclasses, it is somewhat common to only define a __new__() or __init__() method, but not both. However, if extra keyword arguments are going to be accepted, then both methods must be provided and given compatible signatures. The default __prepare__() method accepts any set of keyword arguments, but ignores them. You only need to define it yourself if the extra arguments would somehow affect management of the class namespace creation. Enforcing an Argument Signature on *args and **kwargsYou’ve written a function or method that uses *args and **kwargs, so that it can be general purpose, but you would also like to check the passed arguments to see if they match a specific function calling signature. Two classes, Signature and Parameter, are of particular interest here. Here is an interactive example of creating a function signature:12345678910&gt;&gt;&gt; from inspect import Signature, Parameter&gt;&gt;&gt; params=[Parameter('x', Parameter.POSITIONAL_OR_KEYWORD),... Parameter('y', Parameter.POSITIONAL_OR_KEYWORD, default= 2),... Parameter('z', Parameter.KEYWORD_ONLY, default= None)... ]... &gt;&gt;&gt; sig= Signature(params)&gt;&gt;&gt; print(sig)(x, y=2, *, z=None) Once you have a signature object, you can easily bind it to *args and **kwargs using the signature’s bind() method. 123456789101112131415161718192021222324252627&gt;&gt;&gt; def func(*args, **kwargs):... bound_values= sig.bind(*args, **kwargs)... for name, value in bound_values.arguments.items():... print(name, value)... &gt;&gt;&gt; func(y=2, x=1)x 1y 2&gt;&gt;&gt; func(1, 2, 3, 4)Traceback (most recent call last):... File "/usr/local/lib/python3.3/inspect.py", line 1972, in _bind raise TypeError('too many positional arguments')TypeError: too many positional arguments&gt;&gt;&gt; func(y=2)Traceback (most recent call last):... File "/usr/local/lib/python3.3/inspect.py", line 1961, in _bind raise TypeError(msg) from NoneTypeError: 'x' parameter lacking default value&gt;&gt;&gt; func(1, y=2, x=3)Traceback (most recent call last):... File "/usr/local/lib/python3.3/inspect.py", line 1985, in _bind '&#123;arg!r&#125;'.format(arg=param.name))TypeError: multiple values for argument 'x'&gt;&gt;&gt; Here is a more concrete example of enforcing function signatures. In this code, a base class has defined an extremely general-purpose version of __init__(), but subclasses are expected to supply an expected signature. 1234567891011121314151617181920&gt;&gt;&gt; def make_sig(*names):... params=[Parameter(name, Parameter.POSITIONAL_OR_KEYWORD) for name in names]... return Signature(parameters=params)... &gt;&gt;&gt; class Structure:... __signature__= make_sig()... def __init__(self, *args, **kwargs):... bound_values= self.__signature__.bind(*args, **kwargs)... for name, value in bound_values.arguments.items():... setattr(self, name, value)... &gt;&gt;&gt; class Stock(Structure):... __signature__= make_sig('name', 'shares', 'price')... &gt;&gt;&gt; class Point(Structure):... __signature__= make_sig('x', 'y')... &gt;&gt;&gt; import inspect&gt;&gt;&gt; print(inspect.signature(Stock))(name, shares, price) The use of functions involving *args and **kwargs is very common when trying to make general-purpose libraries, write decorators or implement proxies. However, one downside of such functions is that if you want to implement your own argument checking, it can quickly become an unwieldy mess. In the last example of the solution, it might make sense to create signature objects through the use of a custom metaclass. 1234567891011121314151617181920212223&gt;&gt;&gt; class StructureMeta(type):... def __new__(cls, clsname, bases, clsdict):... clsdict['__signature__']= make_sig(*clsdict.get('_fields', []))... return super().__new__(cls, clsname, bases, clsdict)... &gt;&gt;&gt; class Structure(metaclass= StructureMeta):... _fields=[]... def __init__(self, *args, **kwargs):... bound_values= self.__signature__.bind(*args, **kwargs)... for name, value in bound_values.arguments.items():... setattr(self, name, value)... &gt;&gt;&gt; class Stock(Structure):... _fields=['name', 'shares', 'price']... &gt;&gt;&gt; class Point(Structure):... _fields= ['x', 'y']... &gt;&gt;&gt; inspect.signature(Stock)&lt;Signature (name, shares, price)&gt;&gt;&gt;&gt; print(inspect.signature(Point))(x, y)&gt;&gt;&gt; When defining custom signatures, it is often useful to store the signature in a special attribute __signature__, as shown. If you do this, code that uses the inspect module to perform introspection will see the signature and report it as the calling convention. Enforcing Coding Conventions in ClassesA key feature of a metaclass is that it allows you to examine the contents of a class at the time of definition. Inside the redefined __init__() method, you are free to inspect the class dictionary, base classes, and more. Moreover, once a metaclass has been specified for a class, it gets inherited by all of the subclasses. Thus, a sneaky framework builder can specify a metaclass for one of the top-level classes in a large hierarchy and capture the definition of all classes under it. Here is a metaclass that rejects any class definition containing methods with mixed-case names: 12345678910111213141516171819&gt;&gt;&gt; class NoMixedCaseMeta(type):... def __new__(cls, clsname, bases, clsdict):... print(clsdict)... for name in clsdict:... if name.lower() != name:... raise TypeError('Bad attribute name: '+name)... return super().__new__(cls, clsname, bases, clsdict)... &gt;&gt;&gt; class Root(metaclass= NoMixedCaseMeta): pass... &#123;'__qualname__': 'Root', '__module__': '__main__'&#125;&gt;&gt;&gt; class B(Root):... def fooBar(self): pass... &#123;'__qualname__': 'B', '__module__': '__main__', 'fooBar': &lt;function B.fooBar at 0x0399FF60&gt;&#125;Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 6, in __new__TypeError: Bad attribute name: fooBar As a more advanced and useful example, here is a metaclass that checks the definition of redefined methods to make sure they have the same calling signature as the original method in the superclass. 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; import logging&gt;&gt;&gt; class MatchSignaturesMeta(type):... def __init__(self, clsname, bases, clsdict):... super().__init__(clsname, bases, clsdict)... sup=super(self, self)... print(clsdict.items())... for name, value in clsdict.items():... if name.startswith('_') or not callable(value):... continue... prev_dfn= getattr(sup, name, None)... if prev_dfn:... prev_sig= signature(prev_dfn)... val_sig= signature(value)... if prev_sig != val_sig:... logging.warning('Signature mismatch in %s. %s != %s', value.__qualname__, prev_sig, val_sig)... &gt;&gt;&gt; class Root(metaclass= MatchSignaturesMeta): pass... dict_items([('__qualname__', 'Root'), ('__module__', '__main__')])&gt;&gt;&gt; class A(Root): ... def foo(self, x, y):pass... def spam(self, x, *, z): pass... dict_items([('foo', &lt;function A.foo at 0x039A8270&gt;), ('spam', &lt;function A.spam at 0x039A8228&gt;), ('__qualname__', 'A'), ('__module__', '__main__')])&gt;&gt;&gt; super(A, A)&lt;super: &lt;class 'A'&gt;, &lt;A object&gt;&gt;&gt;&gt;&gt; class B(A):... def foo(self, a, b): pass... def spam(self,x, z): pass... dict_items([('foo', &lt;function B.foo at 0x039A88E8&gt;), ('spam', &lt;function B.spam at 0x039A88A0&gt;), ('__qualname__', 'B'), ('__module__', '__main__')])WARNING:root:Signature mismatch in B.foo. (self, x, y) != (self, a, b)WARNING:root:Signature mismatch in B.spam. (self, x, *, z) != (self, x, z)&gt;&gt;&gt; As a more advanced and useful example, here is a metaclass that checks the definition of redefined methods to make sure they have the same calling signature as the original method in the superclass. 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; class MatchSignaturesMeta(type):... def __init__(self, clsname, bases, clsdict):... super().__init__(clsname, bases, clsdict)... sup=super(self, self)... print(clsdict.items())... for name, value in clsdict.items():... if name.startswith('_') or not callable(value):... continue... prev_dfn= getattr(sup, name, None)... if prev_dfn:... prev_sig= signature(prev_dfn)... val_sig= signature(value)... if prev_sig != val_sig:... logging.warning('Signature mismatch in %s. %s != %s', value.__qualname__, prev_sig, val_sig)... &gt;&gt;&gt; class Root(metaclass= MatchSignaturesMeta): pass... dict_items([('__qualname__', 'Root'), ('__module__', '__main__')])&gt;&gt;&gt; class A(Root): ... def foo(self, x, y):pass... def spam(self, x, *, z): pass... dict_items([('foo', &lt;function A.foo at 0x039A8270&gt;), ('spam', &lt;function A.spam at 0x039A8228&gt;), ('__qualname__', 'A'), ('__module__', '__main__')])&gt;&gt;&gt; super(A, A)&lt;super: &lt;class 'A'&gt;, &lt;A object&gt;&gt;&gt;&gt;&gt; class B(A):... def foo(self, a, b): pass... def spam(self,x, z): pass... dict_items([('foo', &lt;function B.foo at 0x039A88E8&gt;), ('spam', &lt;function B.spam at 0x039A88A0&gt;), ('__qualname__', 'B'), ('__module__', '__main__')])WARNING:root:Signature mismatch in B.foo. (self, x, y) != (self, a, b)WARNING:root:Signature mismatch in B.spam. (self, x, *, z) != (self, x, z)&gt;&gt;&gt; The choice of redefining __new__() or __init__() in a metaclass depends on how you want to work with the resulting class. __new__() is invoked prior to class creation and is typically used when a metaclass wants to alter the class definition in some way (by changing the contents of the class dictionary). The __init__() method is invoked after a class has been created, and is useful if you want to write code that works with the fully formed class object. In the last example, this is essential since it is using the super() function to search for prior definitions. This only works once the class instance has been created and the underlying method resolution order has been set. The line of code that uses super(self, self) is not a typo. When working with a metaclass, it’s important to realize that the self is actually a class object. So, that statement is actually being used to find definitions located further up the class hierarchy that make up the parents of self. Defining Classes ProgrammaticallyYou can use the function types.new_class() to instantiate new class objects. All you need to do is provide the name of the class, tuple of parent classes, keyword arguments, and a callback that populates the class dictionary with members. 1234567891011121314&gt;&gt;&gt; def __init__(self, name, shares, price):... self.name= name... self.shares= shares... self.price= price... &gt;&gt;&gt; def cost(self):... return self.shares * self.price... &gt;&gt;&gt; cls_dict=&#123; '__init__': __init__,... 'cost': cost&#125;... &gt;&gt;&gt; import types&gt;&gt;&gt; Stock= types.new_class('Stock', (), &#123;&#125;, lambda ns: ns.update(cls_dict))&gt;&gt;&gt; Stock.__module__= __name__ This makes a normal class object that works just like you expect: 12345&gt;&gt;&gt; s= Stock('Acme', 10, 4.4)&gt;&gt;&gt; s&lt;__main__.Stock object at 0x0399CE70&gt;&gt;&gt;&gt; s.cost()44.0 A subtle facet of the solution is the assignment to Stock.__module__ after the call to types.new_class(). Whenever a class is defined, its __module__ attribute contains the name of the module in which it was defined. This name is used to produce the output made by methods such as __repr__(). It’s also used by various libraries, such as pickle. Thus, in order for the class you make to be “proper,” you need to make sure this attribute is set accordingly. If the class you want to create involves a different metaclass, it would be specified in the third argument to types.new_class(). 1234567&gt;&gt;&gt; import abc&gt;&gt;&gt; Stock= types.new_class('Stock', (), &#123;'metaclass': abc.ABCMeta&#125;, lambda ns: ns.update(cls_dict))&gt;&gt;&gt; Stock.__module__= __name__&gt;&gt;&gt; Stock&lt;class '__main__.Stock'&gt;&gt;&gt;&gt; type(Stock)&lt;class 'abc.ABCMeta'&gt; The fourth argument to new_class() is the most mysterious, but it is a function that receives the mapping object being used for the class namespace as input. This is normally a dictionary, but it’s actually whatever object gets returned by the __prepare__() method. 12345678910111213141516171819&gt;&gt;&gt; import operator&gt;&gt;&gt; import types&gt;&gt;&gt; import sys&gt;&gt;&gt; def named_tuple(classname, fieldnames):... cls_dict= &#123;name: property(operator.itemgetter(n)) for n, name in enumerate(fieldnames)&#125;... ... def __new__(cls, *args):... if len(args) != len(fieldnames):... raise TypeError('Expected &#123;&#125; arguments'.format(len(fieldnames)))... return tuple.__new__(cls, args)... ... cls_dict['__new__']= __new__... ... cls= types.new_class(classname, (tuple,),&#123;&#125;,lambda ns: ns.update(cls_dict))... ... cls.__module__= sys._getframe(1).f_globals['__name__']... ... return cls... The last part of this code uses a so-called “frame hack” involving sys._getframe() to obtain the module name of the caller. 1234567891011121314151617181920&gt;&gt;&gt; Point= named_tuple('Point', ['x', 'y'])&gt;&gt;&gt; Point&lt;class '__main__.Point'&gt;&gt;&gt;&gt; p= Point(4,5)&gt;&gt;&gt; len(p)2&gt;&gt;&gt; p.x4&gt;&gt;&gt; p.x= 3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; print(p)(4, 5)&gt;&gt;&gt; Point(2,3,4)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 6, in __new__TypeError: Expected 2 arguments&gt;&gt;&gt; You might be inclined to create a class directly by instantiating a metaclass directly. For example: Stock = type(&#39;Stock&#39;, (), cls_dict) The problem is that this approach skips certain critical steps, such as invocation of the metaclass __prepare__() method. By using types.new_class() instead, you ensure that all of the necessary initialization steps get carried out. For instance, the callback function that’s given as the fourth argument to types.new_class() receives the mapping object that’s returned by the __prepare__() method. If you only want to carry out the preparation step, use types.prepare_class(). For example: 12import typesmetaclass, kwargs, ns = types.prepare_class('Stock', (), &#123;'metaclass': type&#125;) This finds the appropriate metaclass and invokes its __prepare__() method. The metaclass, remaining keyword arguments, and prepared namespace are then returned. Initializing Class Members at Definition TimePerforming initialization or setup actions at the time of class definition is a classic use of metaclasses. Essentially, a metaclass is triggered at the point of a definition, at which point you can perform additional steps. Here is an example that uses this idea to create classes similar to named tuples from the collections module: 1234567891011121314151617181920&gt;&gt;&gt; import operator&gt;&gt;&gt; class StructTupleMeta(type):... def __init__(cls, *args, **kwargs):... super().__init__(*args, **kwargs)... for n, name in enumerate(cls._fields):... setattr(cls, name, property(operator.itemgetter(n)))... &gt;&gt;&gt; class StructTuple(tuple, metaclass= StructTupleMeta):... _fields= []... def __new__(cls, *args):... if len(args) != len(cls._fields):... raise ValueError('&#123;&#125; arguments required'.format(len(cls._fields)))... return super().__new__(cls, args)... &gt;&gt;&gt; class Stock(StructTuple):... _fields= ['name', 'shares', 'price']... &gt;&gt;&gt; class Point(StructTuple):... _fields= ['x', 'y']... Here’s how they work: 1234567891011121314&gt;&gt;&gt; s = Stock('ACME', 50, 91.1)&gt;&gt;&gt; s('ACME', 50, 91.1)&gt;&gt;&gt; s[0]'ACME'&gt;&gt;&gt; s.name'ACME'&gt;&gt;&gt; s.shares * s.price4555.0&gt;&gt;&gt; s.shares = 23Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; In this recipe, the StructTupleMeta class takes the listing of attribute names in the _fields class attribute and turns them into property methods that access a particular tuple slot. The operator.itemgetter() function creates an accessor function and the property() function turns it into a property. The trickiest part of this recipe is knowing when the different initialization steps occur. The __init__() method in StructTupleMeta is only called once for each class that is defined. The cls argument is the class that has just been defined. Essentially, the code is using the _fields class variable to take the newly defined class and add some new parts to it. The StructTuple class serves as a common base class for users to inherit from. The __new__() method in that class is responsible for making new instances. The use of __new__() here is a bit unusual, but is partly related to the fact that we’re modifying the calling signature of tuples so that we can create instances with code that uses a normal looking calling convention like this: 12s = Stock('ACME', 50, 91.1) # OKs = Stock(('ACME', 50, 91.1)) # Error Unlike __init__(), the __new__() method gets triggered before an instance is created. Since tuples are immutable, it’s not possible to make any changes to them once they have been created. An init() function gets triggered too late in the instance creation process to do what we want. That’s why __new__() has been defined. Implementing Multiple Dispatch with Function AnnotationsYou’ve learned about function argument annotations and you have a thought that you might be able to use them to implement multiple-dispatch (method overloading) based on types. Here is the start of a solution that does just that, using a combination of metaclasses and descriptors: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121import inspectimport typesclass MultiMethod: ''' Represents a single multimethod. ''' def __init__(self, name): self._methods = &#123;&#125; self.__name__ = name def register(self, meth): ''' Register a new method as a multimethod ''' sig = inspect.signature(meth) # Build a type-signature from the method's annotations types = [] for name, parm in sig.parameters.items(): if name == 'self': continue if parm.annotation is inspect.Parameter.empty: raise TypeError( 'Argument &#123;&#125; must be annotated with a type'.format(name) ) if not isinstance(parm.annotation, type): raise TypeError( 'Argument &#123;&#125; annotation must be a type'.format(name) ) if parm.default is not inspect.Parameter.empty: self._methods[tuple(types)] = meth types.append(parm.annotation) self._methods[tuple(types)] = meth def __call__(self, *args): ''' Call a method based on type signature of the arguments ''' types = tuple(type(arg) for arg in args[1:]) meth = self._methods.get(types, None) if meth: return meth(*args) else: raise TypeError('No matching method for types &#123;&#125;'.format(types)) def __get__(self, instance, cls): ''' Descriptor method needed to make calls work in a class ''' if instance is not None: return types.MethodType(self, instance) else: return self class MultiDict(dict): ''' Special dictionary to build multimethods in a metaclass ''' def __setitem__(self, key, value): if key in self: # If key already exists, it must be a multimethod or callable current_value = self[key] if isinstance(current_value, MultiMethod): current_value.register(value) else: mvalue = MultiMethod(key) mvalue.register(current_value) mvalue.register(value) super().__setitem__(key, mvalue) else: super().__setitem__(key, value)class MultipleMeta(type): ''' Metaclass that allows multiple dispatch of methods ''' def __new__(cls, clsname, bases, clsdict): return type.__new__(cls, clsname, bases, dict(clsdict)) @classmethod def __prepare__(cls, clsname, bases): return MultiDict()# Some example classes that use multiple dispatchclass Spam(metaclass=MultipleMeta): def bar(self, x:int, y:int): print('Bar 1:', x, y) def bar(self, s:str, n:int = 0): print('Bar 2:', s, n)# Example: overloaded __init__import timeclass Date(metaclass=MultipleMeta): def __init__(self, year: int, month:int, day:int): self.year = year self.month = month self.day = day def __init__(self): t = time.localtime() self.__init__(t.tm_year, t.tm_mon, t.tm_mday)if __name__ == '__main__': s = Spam() s.bar(2, 3) s.bar('hello') s.bar('hello', 5) try: s.bar(2, 'hello') except TypeError as e: print(e) # Overloaded __init__ d = Date(2012, 12, 21) print(d.year, d.month, d.day) # Get today's date e = Date()print(e.year, e.month, e.day) The main idea in the implementation is relatively simple. The MutipleMeta metaclass uses its __prepare__() method to supply a custom class dictionary as an instance of MultiDict. Unlike a normal dictionary, MultiDict checks to see whether entries already exist when items are set. If so, the duplicate entries get merged together inside an instance of MultiMethod. Instances of MultiMethod collect methods by building a mapping from type signatures to functions. During construction, function annotations are used to collect these signatures and build the mapping. This takes place in the MultiMethod.register() method. One critical part of this mapping is that for multimethods, types must be specified on all of the arguments or else an error occurs. To make MultiMethod instances emulate a callable, the __call__() method is implemented. This method builds a type tuple from all of the arguments except self, looks up the method in the internal map, and invokes the appropriate method. The __get__() is required to make MultiMethod instances operate correctly inside class definitions. In the implementation, it’s being used to create proper bound methods. 123456789101112&gt;&gt;&gt; b = s.bar&gt;&gt;&gt; b&lt;bound method Spam.bar of &lt;__main__.Spam object at 0x1006a46d0&gt;&gt;&gt;&gt;&gt; b.__self__&lt;__main__.Spam object at 0x1006a46d0&gt;&gt;&gt;&gt; b.__func__&lt;__main__.MultiMethod object at 0x1006a4d50&gt;&gt;&gt;&gt; b(2, 3)Bar 1: 2 3&gt;&gt;&gt; b('hello')Bar 2: hello 0&gt;&gt;&gt; As an alternative to using metaclasses and annotations, it is possible to implement a similar recipe using decorators. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import typesclass multimethod: def __init__(self, func): self._methods = &#123;&#125; self.__name__ = func.__name__ self._default = func def match(self, *types): def register(func): ndefaults = len(func.__defaults__) if func.__defaults__ else 0 for n in range(ndefaults+1): self._methods[types[:len(types) - n]] = func return self return register def __call__(self, *args): types = tuple(type(arg) for arg in args[1:]) meth = self._methods.get(types, None) if meth: return meth(*args) else: return self._default(*args) def __get__(self, instance, cls): if instance is not None: return types.MethodType(self, instance) else: return self# Example useclass Spam: @multimethod def bar(self, *args): # Default method called if no match raise TypeError('No matching method for bar') @bar.match(int, int) def bar(self, x, y): print('Bar 1:', x, y) @bar.match(str, int) def bar(self, s, n = 0): print('Bar 2:', s, n)if __name__ == '__main__': s = Spam() s.bar(2, 3) s.bar('hello') s.bar('hello', 5) try: s.bar(2, 'hello') except TypeError as e: print(e) Avoiding Repetitive Property MethodsYou are writing classes where you are repeatedly having to define property methods that perform common tasks, such as type checking. You would like to simplify the code so there is not so much code repetition. One possible approach is to make a function that simply defines the property for you and returns it. For example: 1234567891011121314151617181920&gt;&gt;&gt; def typed_property(name, expected_type):... storage_name= '_'+ name... @property... def prop(self):... return getattr(self, storage_name)... @prop.setter... def prop(self, value):... if not isinstance(value, expected_type):... raise TypeError('&#123;&#125; must be a &#123;&#125;'.format(name, expected_type))... setattr(self, storage_name, value)... return prop... &gt;&gt;&gt; class Person:... name = typed_property('name', str)... age= typed_property('age', int)... def __init__(self, name, age):... self.name= name... self.age= age... &gt;&gt;&gt; The typed_property() function in this example may look a little weird, but it’s really just generating the property code for you and returning the resulting property object. Thus, when it’s used in a class, it operates exactly as if the code appearing inside typed_property() was placed into the class definition itself. Even though the property getter and setter methods are accessing local variables such as name, expected_type, and storage_name, that is fine—those values are held behind the scenes in a closure. This recipe can be tweaked in an interesting manner using the functools.partial() function. 12345678910from functools import partialString = partial(typed_property, expected_type=str)Integer = partial(typed_property, expected_type=int)# Example:class Person: name = String('name') age = Integer('age') def __init__(self, name, age): self.name = name self.age = age Defining Context Managers the Easy WayOne of the most straightforward ways to write a new context manager is to use the @contextmanager decorator in the contextlib module. 1234567891011121314151617&gt;&gt;&gt; import time&gt;&gt;&gt; from contextlib import contextmanager&gt;&gt;&gt; @contextmanager... def timtethis(label):... start= time.time()... try:... yield... finally:... end=time.time()... print('&#123;&#125;: &#123;&#125;'.format(label, end- start))... &gt;&gt;&gt; with timtethis('counting'):... n= 100000... while n &gt; 0:... n -= 1... counting: 0.014844417572021484 In the timethis() function, all of the code prior to the yield executes as the __enter__() method of a context manager. All of the code after the yield executes as the __exit__() method. If there was an exception, it is raised at the yield statement. Here is a slightly more advanced context manager that implements a kind of transaction on a list object: 12345678910111213&gt;&gt;&gt; @contextmanager... def list_transaction(orig_list):... working= list(orig_list)... yield working... orig_list[:]= working... &gt;&gt;&gt; items=[1, 2, 3]&gt;&gt;&gt; with list_transaction(items) as working:... working.append(4)... working.append(5)... &gt;&gt;&gt; items[1, 2, 3, 4, 5] Normally, to write a context manager, you define a class with an __enter__() and __exit__() method: 123456789import timeclass timethis: def __init__(self, label): self.label = label def __enter__(self): self.start = time.time() def __exit__(self, exc_ty, exc_val, exc_tb): end = time.time() print('&#123;&#125;: &#123;&#125;'.format(self.label, end - self.start))]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C08_Classes_and_Objects]]></title>
    <url>%2F2018%2F12%2F15%2FC08-Classes-and-Objects%2F</url>
    <content type="text"><![CDATA[Changing the String Representation of InstancesTo change the string representation of an instance, define the __str__() and __repr__() methods.1234567891011121314151617&gt;&gt;&gt; class Pair:... def __init__(self, x, y):... self.x= x... self.y =y... def __repr__(self):... return 'Pair(&#123;0.x!r&#125;, &#123;0.y!r&#125;)'.format(self)... def __str__(self):... return '(&#123;0.x!s&#125;, &#123;0.y!s&#125;)'.format(self)... &gt;&gt;&gt; p= Pair(3,4)&gt;&gt;&gt; pPair(3, 4)&gt;&gt;&gt; print(p)(3, 4)&gt;&gt;&gt; print('p is &#123;0!r&#125;'.format(p))p is Pair(3, 4) Specifically, the special !r formatting code indicates that the output of __repr__() should be used instead of __str__(). The use of format() in the solution might look a little funny, but the format code {0.x} specifies the x-attribute of argument 0. Customizing String FormattingTo customize string formatting, define the __format__() method on a class. 1234567891011121314151617&gt;&gt;&gt; _formats = &#123;... 'ymd' : '&#123;d.year&#125;-&#123;d.month&#125;-&#123;d.day&#125;',... 'mdy' : '&#123;d.month&#125;/&#123;d.day&#125;/&#123;d.year&#125;',... 'dmy' : '&#123;d.day&#125;/&#123;d.month&#125;/&#123;d.year&#125;'... &#125;... &gt;&gt;&gt; class Date:... def __init__(self, year, month, day):... self.year= year... self.month= month... self.day= day... def __format__(self,code):... if code == '':... code= 'ymd'... fmt= _formats[code]... return fmt.format(d= self)... Instances of the Date class now support formatting operations such as the following: 1234567&gt;&gt;&gt; d= Date(2012,3,5)&gt;&gt;&gt; format(d)'2012-3-5'&gt;&gt;&gt; format(d,'mdy')'3/5/2012'&gt;&gt;&gt; 'The date is &#123;:ymd&#125;'.format(d)'The date is 2012-3-5' The __format__() method provides a hook into Python’s string formatting functionality. It’s important to emphasize that the interpretation of format codes is entirely up to the class itself. Thus, the codes can be almost anything at all. Making Objects Support the Context-Management ProtocolIn order to make an object compatible with the with statement, you need to implement __enter__() and __exit__() methods 1234567891011121314151617from socket import socket, AF_INET, SOCK_STREAM&gt;&gt;&gt; class LazyConnection:... def __init__(self, address, family= AF_INET, type= SOCK_STREAM):... self.address= address... self.family= AF_INET... self.type= SOCK_STREAM... self.sock= None... def __enter__(self):... if self.sock is not None:... raise RuntimeError('Already connected')... self.sock= socket(self.family, self.type)... self.sock.connect(self.address)... return self.sock... def __exit__(self, exc_ty, exc_val, tb):... self.sock.close()... self.sock= None... The key feature of this class is that it represents a network connection, but it doesn’t actually do anything initially (e.g., it doesn’t establish a connection). Instead, the connection is established and closed using the with statement. 123456789from functools import partial&gt;&gt;&gt; cnn= LazyConnection(('www.python.org', 80))&gt;&gt;&gt; with cnn as s:... s.send(b'GET /index.html HTTP/1.0\r\n')... s.send(b'\r\n')... resp = b''.join(iter(partial(s.recv, 8192), b''))... 262 In fact, the three arguments to the __exit__() method contain the exception type, value, and traceback for pending exceptions (if any). The __exit__() method can choose to use the exception information in some way or to ignore it by doing nothing and returning None as a result. If __exit__() returns True, the exception is cleared as if nothing happened and the program continues executing statements immediately after the with block. One subtle aspect of this recipe is whether or not the LazyConnection class allows nested use of the connection with multiple with statements. You can work around this limitation with a slightly different implementation, as shown here: 123456789101112131415161718192021&gt;&gt;&gt; class LazyConnection:... def __init__(self, address, family= AF_INET, type= SOCK_STREAM):... self.address= address... self.family= AF_INET... self.type= SOCK_STREAM... self.connections= []... def __enter__(self):... sock= socket(self.family, self.type)... sock.connect(self.address)... self.connections.append(sock)... return sock... def __exit__(self, exc_ty, exc_val, tb):... self.connections.pop().close()... ... conn = LazyConnection(('www.python.org', 80))with conn as s1: ... with conn as s2: ... # s1 and s2 are independent sockets Saving Memory When Creating a Large Number of InstancesFor classes that primarily serve as simple data structures, you can often greatly reduce the memory footprint of instances by adding the __slots__ attribute to the class definition. 12345678910111213141516171819202122&gt;&gt; class Date:... __slots__=['year', 'month','day']... def __init__(self, year, month, day):... self.year= year... self.month= month... self.day= day... &gt;&gt;&gt; d=Date(1,2,3)&gt;&gt;&gt; d&lt;__main__.Date object at 0x010C0D50&gt;&gt;&gt;&gt; d.exp=4Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Date' object has no attribute 'exp'&gt;&gt;&gt; d.__dict__Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Date' object has no attribute '__dict__'&gt;&gt;&gt; vars(d)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: vars() argument must have __dict__ attribute A side effect of using slots is that it is no longer possible to add new attributes to instances—you are restricted to only those attribute names listed in the __slots__ specifier. Encapsulating Names in a ClassRather than relying on language features to encapsulate data, Python programmers are expected to observe certain naming conventions concerning the intended usage of data and methods. The first convention is that any name that starts with a single leading underscore (_) should always be assumed to be internal implementation. 1234567891011class A: def __init__(self): self._internal = 0 # An internal attribute self.public = 1 # A public attribute def public_method(self): ''' A public method ''' ... def _internal_method(self): ... Python doesn’t actually prevent someone from accessing internal names. However, doing so is considered impolite, and may result in fragile code. It should be noted, too, that the use of the leading underscore is also used for module names and module-level functions. For example, if you ever see a module name that starts with a leading underscore (e.g., _socket), it’s internal implementation. You may also encounter the use of two leading underscores (__) on names within class definitions. 123456789class B: def __init__(self): self.__private = 0 def __private_method(self): ... def public_method(self): ... self.__private_method() ... The use of double leading underscores causes the name to be mangled to something else. Specifically, the private attributes in the preceding class get renamed to _B__private and _B__private_method, respectively. At this point, you might ask what purpose such name mangling serves. The answer is inheritance—such attributes cannot be overridden via inheritance.12345678class C(B): def __init__(self): super().__init__() self.__private = 1 # Does not override B.__private # Does not override B.__private_method() def __private_method(self): ... Here, the private names __private and __private_method get renamed to _C__private and _C__private_method, which are different than the mangled names in the base class B. Creating Managed AttributesA simple way to customize access to an attribute is to define it as a “property.” 12345678910111213141516171819202122232425262728&gt;&gt;&gt; class Person:... def __init__(self, first_name):... self.first_name= first_name... @property... def first_name(self):... return self._first_name... @first_name.setter... def first_name(self,value):... if not isinstance(value, str):... raise TypeError('Expected a string')... self._first_name= value... @first_name.deleter... def first_name(self):... raise AttributeError('Can not delete attribute')... &gt;&gt;&gt; a = Person('Guido')&gt;&gt;&gt; a.first_name'Guido'&gt;&gt;&gt; a.first_name=42Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 10, in first_nameTypeError: Expected a string&gt;&gt;&gt; del a.first_nameTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 14, in first_nameAttributeError: Can not delete attribute When implementing a property, the underlying data (if any) still needs to be stored somewhere. Thus, in the get and set methods, you see direct manipulation of a _first_name attribute, which is where the actual data lives. In addition, you may ask why the __init__() method sets self.first_name instead of self._first_name. In this example, the entire point of the property is to apply type checking when setting an attribute. Thus, chances are you would also want such checking to take place during initialization. By setting self.first_name, the set operation uses the setter method (as opposed to bypassing it by accessing self._first_name). A property attribute is actually a collection of methods bundled together. If you inspect a class with a property, you can find the raw methods in the fget, fset, and fdel attributes of the property itself. For example: 123456&gt;&gt;&gt; Person.first_name.fget&lt;function Person.first_name at 0x1006a60e0&gt;&gt;&gt;&gt; Person.first_name.fset&lt;function Person.first_name at 0x1006a6170&gt;&gt;&gt;&gt; Person.first_name.fdel&lt;function Person.first_name at 0x1006a62e0&gt; Properties can also be a way to define computed attributes. These are attributes that are not actually stored, but computed on demand. 123456789101112131415161718192021&gt;&gt;&gt; import math&gt;&gt;&gt; class Circle:... def __init__(self, radius):... self.radius= radius... @property... def area(self):... return math.pi * self.radius ** 2... @property... def perimeter(self):... return 2 * math.pi * self.radius... &gt;&gt;&gt; c= Circle(3)&gt;&gt;&gt; c.radius3&gt;&gt;&gt; c.area28.274333882308138&gt;&gt;&gt; c.area= 4Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; Don’t write Python code that features a lot of repetitive property definitions. For example:12345678910111213141516171819202122class Person: def __init__(self, first_name, last_name): self.first_name = first_name self.last_name = last_name @property def first_name(self): return self._first_name @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._first_name = value # Repeated property code, but for a different name (bad!) @property def last_name(self): return self._last_name @last_name.setter def last_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._last_name = value Code repetition leads to bloated, error prone, and ugly code. As it turns out, there are much better ways to achieve the same thing using descriptors or closures. Calling a Method on a Parent ClassTo call a method in a parent (or superclass), use the super() function. A very common use of super() is in the handling of the __init__() method to make sure that parents are properly initialized: 1234567class A: def __init__(self): self.x = 0class B(A): def __init__(self): super().__init__() self.y = 1 Another common use of super() is in code that overrides any of Python’s special methods. 1234567891011121314151617181920212223242526272829class Proxy: def __init__(self, obj): self._obj = obj # Delegate attribute lookup to internal obj def __getattr__(self, name): return getattr(self._obj, name) # Delegate attribute assignment def __setattr__(self, name, value): if name.startswith('_'): super().__setattr__(name, value) # Call original __setattr__ else: setattr(self._obj, name, value)class Base: def __init__(self): print('Base.__init__')class A(Base): def __init__(self): super().__init__() print('A.__init__')class B(Base): def __init__(self): super().__init__() print('B.__init__')class C(A,B): def __init__(self): super().__init__() # Only one call to super() here print('C.__init__') You’ll find that each __init__() method only gets called once:1234567&gt;&gt;&gt; c = C()Base.__init__B.__init__A.__init__C.__init__&gt;&gt;&gt; For every class that you define, Python computes what’s known as a method resolution order (MRO) list. The MRO list is simply a linear ordering of all the base classes. 1234&gt;&gt;&gt; C.__mro__(&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;,&lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;)&gt;&gt;&gt; When you use the super() function, Python continues its search starting with the next class on the MRO. As long as every redefined method consistently uses super() and only calls it once, control will ultimately work its way through the entire MRO list and each method will only be called once. A somewhat surprising aspect of super() is that it doesn’t necessarily go to the direct parent of a class next in the MRO and that you can even use it in a class with no direct parent at all. 1234class A: def spam(self): print('A.spam') super().spam() If you try to use this class, you’ll find that it’s completely broken: 12345678&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.spam()A.spamTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 4, in spamAttributeError: 'super' object has no attribute 'spam'&gt;&gt;&gt; Watch what happens if you start using the class with multiple inheritance: 123456789101112&gt;&gt;&gt; class B:... def spam(self):... print('B.spam')...&gt;&gt;&gt; class C(A,B):... pass...&gt;&gt;&gt; c = C()&gt;&gt;&gt; c.spam()A.spamB.spam&gt;&gt;&gt; Here you see that the use of super().spam() in class A has, in fact, called the spam() method in class B—a class that is completely unrelated to A! This is all explained by the MRO of class C: 1234&gt;&gt;&gt; C.__mro__(&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;,&lt;class 'object'&gt;)&gt;&gt;&gt; Extending a Property in a SubclassWithin a subclass, you want to extend the functionality of a property defined in a parent class. 123456789101112131415&gt;&gt;&gt; class Person:... def __init__(self,name):... self.name= name... @property... def name(self):... return self._name... @name.setter... def name(self, value):... if not isinstance(value, str):... raise TypeError('Expected a string')... self._name = value... @name.deleter... def name(self):... raise AttributeError('Can not delete attribute')... Here is an example of a class that inherits from Person and extends the name property with new functionality. 1234567891011121314&gt;&gt;&gt; class SubPerson(Person):... @property... def name(self):... print('Getting name')... return super().name... @name.setter... def name(self, value):... print('Setting name to ', value)... super(SubPerson, SubPerson).name.__set__(self, value)... @name.deleter... def name(self):... print('Deleting name')... super(SubPerson, SubPerson).name.__delete__(self)... Here is an example of the new class in use: 1234567891011121314&gt;&gt;&gt; s = SubPerson('Guido')Setting name to Guido&gt;&gt;&gt; s.nameGetting name'Guido'&gt;&gt;&gt; s.name = 'Larry'Setting name to Larry&gt;&gt;&gt; s.name = 42Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 16, in name raise TypeError('Expected a string')TypeError: Expected a string&gt;&gt;&gt; The use of super(SubPerson, SubPerson).name.__set__(self, value) in the setter function is no mistake. To delegate to the previous implementation of the setter, control needs to pass through the __set__() method of the previously defined name property. However, the only way to get to this method is to access it as a class variable instead of an instance variable. This is what happens with the super(SubPerson, SubPerson) operation. 123456&gt;&gt;&gt; class SubPerson(Person):... @Person.name.getter... def name(self):... print('Getting name')... return super().name... When you do this, all of the previously defined methods of the property are copied, and the getter function is replaced. It now works as expected: 123456789101112131415&gt;&gt;&gt; s = SubPerson('Guido')&gt;&gt;&gt; s.nameGetting name'Guido'&gt;&gt;&gt; s.name = 'Larry'&gt;&gt;&gt; s.nameGetting name'Larry'&gt;&gt;&gt; s.name = 42Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 16, in name raise TypeError('Expected a string')TypeError: Expected a string&gt;&gt;&gt; It’s worth noting that the first technique shown in this recipe can also be used to extend a descriptor. 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; class String:... def __init__(self, name):... self.name= name... def __get__(self, instance, cls):... if instance is None:... return self... return instance.__dict__[self.name]... def __set__(self, instance, value):... if not isinstance(value, str):... raise TypeError('Expected a string')... instance.__dict__[self.name]= value... &gt;&gt;&gt; class Person:... name= String('name')... def __init__(self, name):... self.name= name... &gt;&gt;&gt; class SubPerson(Person):... @property... def name(self):... print('Getting name')... return super().name... @name.setter... def name(self, value):... print('Setting name to', value)... super(SubPerson, SubPerson).name.__set__(self, value)... @name.deleter... def name(self):... print('Deleting name')... super(SubPerson, SubPerson).name.__delete__(self)... &gt;&gt;&gt; s= SubPerson('gu')Setting name to gu&gt;&gt;&gt; s.name='rr'Setting name to rr&gt;&gt;&gt; Creating a New Kind of Class or Instance AttributeIf you want to create an entirely new kind of instance attribute, define its functionality in the form of a descriptor class. 123456789101112131415# Descriptor attribute for an integer type-checked attributeclass Integer: def __init__(self, name): self.name = name def __get__(self, instance, cls): if instance is None: return self else: return instance.__dict__[self.name] def __set__(self, instance, value): if not isinstance(value, int): raise TypeError('Expected an int') instance.__dict__[self.name] = value def __delete__(self, instance): del instance.__dict__[self.name] A descriptor is a class that implements the three core attribute access operations (get, set, and delete) in the form of __get__(), __set__(), and __delete__() special methods. These methods work by receiving an instance as input. The underlying dictionary of the instance is then manipulated as appropriate. To use a descriptor, instances of the descriptor are placed into a class definition as class variables. For example: 1234567891011121314151617class Point: x = Integer('x') y = Integer('y') def __init__(self, x, y): self.x = x self.y = y&gt;&gt;&gt; p = Point(2, 3)&gt;&gt;&gt; p.x # Calls Point.x.__get__(p,Point)2&gt;&gt;&gt; p.y = 5 # Calls Point.y.__set__(p, 5)&gt;&gt;&gt; p.x = 2.3 # Calls Point.x.__set__(p, 2.3)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "descrip.py", line 12, in __set__ raise TypeError('Expected an int')TypeError: Expected an int&gt;&gt;&gt; As input, each method of a descriptor receives the instance being manipulated. To carry out the requested operation, the underlying instance dictionary (the __dict__ attribute) is manipulated as appropriate. The self.name attribute of the descriptor holds the dictionary key being used to store the actual data in the instance dictionary. The implementation of the __get__() method is trickier than it seems: 123456789# Descriptor attribute for an integer type-checked attributeclass Integer: ... def __get__(self, instance, cls): if instance is None: return self else: return instance.__dict__[self.name] ... The reason __get__() looks somewhat complicated is to account for the distinction between instance variables and class variables. If a descriptor is accessed as a class variable, the instance argument is set to None. In this case, it is standard practice to simply return the descriptor instance itself. 12345678910111213141516171819202122232425class Typed: def __init__(self, name, expected_type): self.name= name self.expected_type= expected_type def __get__(self, instance, cls): if instance is None: return self else: return instance.__dict__[self.name] def __set__(self, instance, value): if not isinstance(value, self.expected_type): raise TypeError('Expected '+ str(self.expected_type)) instance.__dict__[self.name]= valuedef typeassert(**kwargs): def decorate(cls): for name, expected_type in kwargs.items(): setattr(cls, name, Typed(name, expected_type)) return cls return decorate@typeassert(name= str, shares= int, price= float)class Stock: def __init__(self, name, shares, price): self.name= name self.shares= shares self.price= price Finally, it should be stressed that you would probably not write a descriptor if you simply want to customize the access of a single attribute of a specific class. For that, it’s easier to use a property instead. Descriptors are more useful in situations where there will be a lot of code reuse. Using Lazily Computed PropertiesYou’d like to define a read-only attribute as a property that only gets computed on access. However, once accessed, you’d like the value to be cached and not recomputed on each access. An efficient way to define a lazy attribute is through the use of a descriptor class. 1234567891011&gt;&gt;&gt; class lazyproperty:... def __init__(self, func):... self.func= func... def __get__(self, instance, cls):... if instance is None:... return self... else:... value= self.func(instance)... setattr(instance, self.func.__name__, value)... return value... To utilize this code, you would use it in a class such as the following: 12345678910111213&gt;&gt;&gt; import math&gt;&gt;&gt; class Circle:... def __init__(self, radius):... self.radius= radius... @lazyproperty... def area(self):... print('Computing area')... return math.pi * self.radius ** 2... @lazyproperty... def perimeter(self):... print('Computing perimeter')... return 2 * math.pi * self.radius... An interactive session that illustrates how it works: 1234567891011121314&gt;&gt;&gt; c = Circle(4.0)&gt;&gt;&gt; c.radius4.0&gt;&gt;&gt; c.areaComputing area50.26548245743669&gt;&gt;&gt; c.area50.26548245743669&gt;&gt;&gt; c.perimeterComputing perimeter25.132741228718345&gt;&gt;&gt; c.perimeter25.132741228718345&gt;&gt;&gt; When a descriptor is placed into a class definition, its __get__(), __set__(), and __delete__() methods get triggered on attribute access. However, if a descriptor only defines a __get__() method, it has a much weaker binding than usual. In particular, the__get__() method only fires if the attribute being accessed is not in the underlying instance dictionary. One possible downside to this recipe is that the computed value becomes mutable after it’s created. For example: 123456&gt;&gt;&gt; c.areaComputing area50.26548245743669&gt;&gt;&gt; c.area = 25&gt;&gt;&gt; c.area25 You can use a slightly less efficient implementation: 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def lazyproperty(func):... name= '_lazy_'+ func.__name__... @property... def lazy(self):... if hasattr(self, name):... return getattr(self, name)... else:... value= func(self)... setattr(self, name, value)... return value... return lazy... &gt;&gt;&gt; c= Circle(4)&gt;&gt;&gt; c.areaComputing area50.26548245743669&gt;&gt;&gt; c.area50.26548245743669&gt;&gt;&gt; c._lazy_area50.26548245743669&gt;&gt;&gt; c._lazy_area= 25&gt;&gt;&gt; c.area25&gt;&gt;&gt; c.area=45Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; Simplifying the Initialization of Data StructuresYou can often generalize the initialization of data structures into a single __init__() function defined in a common base class. 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; class Structure:... _fields= []... def __init__(self, *args):... if len(args) != len(self._fields):... raise TypeError('Expected &#123;&#125; arguments'.format(len(self._fields)))... for name, value in zip(self._fields, args):... setattr(self, name, value)... &gt;&gt;&gt; class Stock(Structure):... _fields= ['name', 'shares', 'price']... &gt;&gt;&gt; class Points(Structure):... _fields= ['x', 'y']... &gt;&gt;&gt; class Circle(Structure):... _fields=['radius']... def area(self):... return math.pi * self.radius ** 2... &gt;&gt;&gt; s= Stock('Acme', 80, 90)&gt;&gt;&gt; c=Circle(3)&gt;&gt;&gt; c.area()28.274333882308138&gt;&gt;&gt; s= Stock('Acme', 80)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 5, in __init__TypeError: Expected 3 arguments&gt;&gt;&gt; This technique of defining a general purpose __init__() method can be extremely useful if you’re ever writing a program built around a large number of small data structures. One subtle aspect of the implementation concerns the mechanism used to set value using the setattr() function. Instead of doing that, you might be inclined to directly access the instance dictionary. For example: 12345678class Structure: # Class variable that specifies expected fields _fields= [] def __init__(self, *args): if len(args) != len(self._fields): raise TypeError('Expected &#123;&#125; arguments'.format(len(self._fields))) # Set the arguments (alternate) self.__dict__.update(zip(self._fields,args)) Although this works, it’s often not safe to make assumptions about the implementation of a subclass. If a subclass decided to use __slots__ or wrap a specific attribute with a property (or descriptor), directly acccessing the instance dictionary would break. It should be noted that it is also possible to automatically initialize instance variables using a utility function and a so-called “frame hack.” 123456789101112&gt;&gt;&gt; def init_fromlocals(self):... import sys... locs= sys._getframe(1).f_locals... for k, v in locs.items():... if k != 'self':... setattr(self, k, v)... &gt;&gt;&gt; class Stock:... def __init__(self, name, shares, price):... init_fromlocals(self)... &gt;&gt;&gt; In this variation, the init_fromlocals() function uses sys._getframe() to peek at the local variables of the calling method. If used as the first step of an __init__() method, the local variables will be the same as the passed arguments and can be easily used to set attributes with the same names. Although this approach avoids the problem of getting the right calling signature in IDEs, it runs more than 50% slower than the solution provided in the recipe, requires more typing, and involves more sophisticated magic behind the scenes. Defining an Interface or Abstract Base ClassYou want to define a class that serves as an interface or abstract base class from which you can perform type checking and ensure that certain methods are implemented in subclasses. To define an abstract base class, use the abc module. For example: 12345678910111213&gt;&gt;&gt; from abc import ABCMeta, abstractmethod&gt;&gt;&gt; class IStream(metaclass= ABCMeta):... @abstractmethod... def read(self, maxbytes= -1):... pass... @abstractmethod... def write(self, data):... pass... &gt;&gt;&gt; a=IStream()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: Can't instantiate abstract class IStream with abstract methods read, write ABCs allow other classes to be registered as implementing the required interface. For example, you can do this: 123456import io# Register the built-in I/O classes as supporting our interfaceIStream.register(io.IOBase)# Open a normal file and type checkf = open('foo.txt')isinstance(f, IStream) # Returns True It should be noted that @abstractmethod can also be applied to static methods, class methods, and properties. You just need to make sure you apply it in the proper sequence where @abstractmethod appears immediately before the function definition. Predefined abstract base classes are found in various places in the standard library. The collections module defines a variety of ABCs related to containers and iterators (sequences, mappings, sets, etc.), the numbers library defines ABCs related to numeric objects (integers, floats, rationals, etc.), and the io library defines ABCs related to I/O handling.1234567891011121314import collections# Check if x is a sequenceif isinstance(x, collections.Sequence): ...# Check if x is iterableif isinstance(x, collections.Iterable): ...# Check if x has a sizeif isinstance(x, collections.Sized): ...# Check if x is a mappingif isinstance(x, collections.Mapping): ... It should be noted that, as of this writing, certain library modules don’t make use of these predefined ABCs as you might expect. For example: 1234from decimal import Decimalimport numbersx = Decimal('3.4')isinstance(x, numbers.Real) # Returns False Implementing a Data Model or Type SystemYou want to define various kinds of data structures, but want to enforce constraints on the values that are allowed to be assigned to certain attributes. The following code illustrates the use of descriptors to implement a system type and value checking framework: 123456789101112131415161718192021222324252627282930# Base class. Uses a descriptor to set a valueclass Descriptor: def __init__(self, name=None, **opts): self.name = name for key, value in opts.items(): setattr(self, key, value) def __set__(self, instance, value): instance.__dict__[self.name] = value# Descriptor for enforcing typesclass Typed(Descriptor): expected_type = type(None) def __set__(self, instance, value): if not isinstance(value, self.expected_type): raise TypeError('expected ' + str(self.expected_type)) super().__set__(instance, value)# Descriptor for enforcing valuesclass Unsigned(Descriptor): def __set__(self, instance, value): if value &lt; 0: raise ValueError('Expected &gt;= 0') super().__set__(instance, value)class MaxSized(Descriptor): def __init__(self, name=None, **opts): if 'size' not in opts: raise TypeError('missing size option') super().__init__(name, **opts) def __set__(self, instance, value): if len(value) &gt;= self.size: raise ValueError('size must be &lt; ' + str(self.size)) super().__set__(instance, value) These classes should be viewed as basic building blocks from which you construct a data model or type system. Continuing, here is some code that implements some different kinds of data: 123456789101112class Integer(Typed): expected_type = intclass UnsignedInteger(Integer, Unsigned): passclass Float(Typed): expected_type = floatclass UnsignedFloat(Float, Unsigned): passclass String(Typed): expected_type = strclass SizedString(String, MaxSized): pass Using these type objects, it is now possible to define a class such as this: 123456789class Stock: # Specify constraints name = SizedString('name',size=8) shares = UnsignedInteger('shares') price = UnsignedFloat('price') def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price With the constraints in place, you’ll find that assigning of attributes is now validated. For example: 123456789101112131415161718192021222324252627&gt;&gt;&gt; s = Stock('ACME', 50, 91.1)&gt;&gt;&gt; s.name'ACME'&gt;&gt;&gt; s.shares = 75&gt;&gt;&gt; s.shares = -10Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 17, in __set__ super().__set__(instance, value) File "example.py", line 23, in __set__ raise ValueError('Expected &gt;= 0')ValueError: Expected &gt;= 0&gt;&gt;&gt; s.price = 'a lot'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 16, in __set__ raise TypeError('expected ' + str(self.expected_type))TypeError: expected &lt;class 'float'&gt;&gt;&gt;&gt; s.name = 'ABRACADABRA'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 17, in __set__ super().__set__(instance, value) File "example.py", line 35, in __set__ raise ValueError('size must be &lt; ' + str(self.size))ValueError: size must be &lt; 8&gt;&gt;&gt; There are some techniques that can be used to simplify the specification of constraints in classes. One approach is to use a class decorator, like this:12345678910111213141516171819# Class decorator to apply constraintsdef check_attributes(**kwargs): def decorate(cls): for key, value in kwargs.items(): if isinstance(value, Descriptor): value.name = key setattr(cls, key, value) else: setattr(cls, key, value(key)) return cls return decorate# Example@check_attributes(name=SizedString(size=8), shares=UnsignedInteger, price=UnsignedFloat)class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price Another approach to simplify the specification of constraints is to use a metaclass. For example: 1234567891011121314151617# A metaclass that applies checkingclass checkedmeta(type): def __new__(cls, clsname, bases, methods): # Attach attribute names to the descriptors for key, value in methods.items(): if isinstance(value, Descriptor): value.name = key return type.__new__(cls, clsname, bases, methods)# Exampleclass Stock(metaclass=checkedmeta): name = SizedString(size=8) shares = UnsignedInteger() price = UnsignedFloat() def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price First, in the Descriptor base class, you will notice that there is a __set__() method, but no corresponding __get__(). If a descriptor will do nothing more than extract an identically named value from the underlying instance dictionary, defining __get__() is unnecessary. In fact, defining __get__() will just make it run slower. Thus, this recipe only focuses on the implementation of __set__(). The overall design of the various descriptor classes is based on mixin classes. For example, the Unsigned and MaxSized classes are meant to be mixed with the other descriptor classes derived from Typed. To handle a specific kind of data type, multiple inheritance is used to combine the desired functionality. The definitions of the various type classes such as Integer, Float, and String illustrate a useful technique of using class variables to customize an implementation. The Typed descriptor merely looks for an expected_type attribute that is provided by each of those subclasses. As a final twist, a class decorator approach can also be used as a replacement for mixin classes, multiple inheritance, and tricky use of the super() function. Here is an alternative formulation of this recipe that uses class decorators:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Base class. Uses a descriptor to set a valueclass Descriptor: def __init__(self, name=None, **opts): self.name = name for key, value in opts.items(): setattr(self, key, value) def __set__(self, instance, value): instance.__dict__[self.name] = value# Decorator for applying type checkingdef Typed(expected_type, cls=None): if cls is None: return lambda cls: Typed(expected_type, cls) super_set = cls.__set__ def __set__(self, instance, value): if not isinstance(value, expected_type): raise TypeError('expected ' + str(expected_type)) super_set(self, instance, value) cls.__set__ = __set__ return cls# Decorator for unsigned valuesdef Unsigned(cls): super_set = cls.__set__ def __set__(self, instance, value): if value &lt; 0: raise ValueError('Expected &gt;= 0') super_set(self, instance, value) cls.__set__ = __set__ return cls# Decorator for allowing sized valuesdef MaxSized(cls): super_init = cls.__init__ def __init__(self, name=None, **opts): if 'size' not in opts: raise TypeError('missing size option') super_init(self, name, **opts) cls.__init__ = __init__ super_set = cls.__set__ def __set__(self, instance, value): if len(value) &gt;= self.size: raise ValueError('size must be &lt; ' + str(self.size)) super_set(self, instance, value) cls.__set__ = __set__ return cls# Specialized descriptors@Typed(int)class Integer(Descriptor): pass@Unsignedclass UnsignedInteger(Integer): pass@Typed(float)class Float(Descriptor): pass@Unsignedclass UnsignedFloat(Float): pass@Typed(str)class String(Descriptor): pass@MaxSizedclass SizedString(String): pass Implementing Custom ContainersThe collections library defines a variety of abstract base classes that are extremely useful when implementing custom container classes. To illustrate, suppose you want your class to support iteration. To do that, simply start by having it inherit from collections.Iterable, as follows: 123import collectionsclass A(collections.Iterable): pass The special feature about inheriting from collections.Iterable is that it ensures you implement all of the required special methods. If you don’t, you’ll get an error upon instantiation: 12345&gt;&gt;&gt; a = A()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: Can't instantiate abstract class A with abstract methods __iter__&gt;&gt;&gt; Other notable classes defined in collections include Sequence, MutableSequence, Mapping, MutableMapping, Set, and MutableSet. Many of these classes form hierarchies with increasing levels of functionality (e.g., one such hierarchy is Container, Iterable, Sized, Sequence, and MutableSequence). Again, simply instantiate any of these classes to see what methods need to be implemented to make a custom container with that behavior: 1234567&gt;&gt;&gt; import collections&gt;&gt;&gt; collections.Sequence()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: Can't instantiate abstract class Sequence with abstract methods \__getitem__, __len__&gt;&gt;&gt; Here is a simple example of a class that implements the preceding methods to create a sequence where items are always stored in sorted order. 1234567891011&gt;&gt;&gt; import bisect&gt;&gt;&gt; class SortedItems(collections.Sequence):... def __init__(self, initial= None):... self._items= sorted(initial) if initial is not None else []... def __getitem__(self, index):... return self._items[index]... def __len__(self):... return len((self._items)... def add(self, item):... bisect.insort(self._items, item)... Here’s an example of using this class: 1234567891011121314151617181920212223242526&gt;&gt;&gt; items = SortedItems([5, 1, 3])&gt;&gt;&gt; list(items)[1, 3, 5]&gt;&gt;&gt; items[0]1&gt;&gt;&gt; items[-1]5&gt;&gt;&gt; items.add(2)&gt;&gt;&gt; list(items)[1, 2, 3, 5]&gt;&gt;&gt; items.add(-10)&gt;&gt;&gt; list(items)[-10, 1, 2, 3, 5]&gt;&gt;&gt; items[1:4][1, 2, 3]&gt;&gt;&gt; 3 in itemsTrue&gt;&gt;&gt; len(items)5&gt;&gt;&gt; for n in items:... print(n)...-10123 Inheriting from one of the abstract base classes in collections ensures that your custom container implements all of the required methods expected of the container. However, this inheritance also facilitates type checking.1234567891011121314&gt;&gt;&gt; items = SortedItems()&gt;&gt;&gt; import collections&gt;&gt;&gt; isinstance(items, collections.Iterable)True&gt;&gt;&gt; isinstance(items, collections.Sequence)True&gt;&gt;&gt; isinstance(items, collections.Container)True&gt;&gt;&gt; isinstance(items, collections.Sized)True&gt;&gt;&gt; isinstance(items, collections.Mapping)False&gt;&gt;&gt; Many of the abstract base classes in collections also provide default implementations of common container methods. To illustrate, suppose you have a class that inherits from collections.MutableSequence. 12345678910111213141516171819&gt;&gt;&gt; class Items(collections.MutableSequence):... def __init__(self, initial= None):... self._items= list(initial) if initial is not None else []... def __getitem__(self, index):... print('Gettinh:', index)... return self._items[index]... def __setitem__(self, index, value):... print('Setting:', index, value)... self._items[index]= value... def __delitem__(self, index):... print('Deleting:', index)... del self._items[index]... def insert(self, index, value):... print('Inserting:', index, value)... self._items.insert(index, value)... def __len__(self):... print('Len')... return len(self._items)... If you create an instance of Items, you’ll find that it supports almost all of the core list methods (e.g., append(), remove(), count(), etc.). 123456789101112131415161718192021222324&gt;&gt;&gt; a = Items([1, 2, 3])&gt;&gt;&gt; len(a)Len3&gt;&gt;&gt; a.append(4)LenInserting: 3 4&gt;&gt;&gt; a.append(2)LenInserting: 4 2&gt;&gt;&gt; a.count(2)Getting: 0Getting: 1Getting: 2Getting: 3Getting: 4Getting: 52&gt;&gt;&gt; a.remove(3)Getting: 0Getting: 1Getting: 2Deleting: 2&gt;&gt;&gt; Delegating Attribute AccessSimply stated, delegation is a programming pattern where the responsibility for implementing a particular operation is handed off (i.e., delegated) to a different object. 12345678910111213141516class A: def spam(self, x): pass def foo(self): passclass B: def __init__(self): self._a = A() def spam(self, x): # Delegate to the internal self._a instance return self._a.spam(x) def foo(self): # Delegate to the internal self._a instance return self._a.foo() def bar(self): pass However, if there are many methods to delegate, an alternative approach is to define the __getattr__() method, like this: 12345678910111213class A: def spam(self, x): pass def foo(self): passclass B: def __init__(self): self._a = A() def bar(self): pass # Expose all of the methods defined on class A def __getattr__(self, name): return getattr(self._a, name) Another example of delegation is in the implementation of proxies.123456789101112131415161718192021222324# A proxy class that wraps around another object, but# exposes its public attributesclass Proxy: def __init__(self, obj): self._obj = obj # Delegate attribute lookup to internal obj def __getattr__(self, name): print('getattr:', name) return getattr(self._obj, name) # Delegate attribute assignment def __setattr__(self, name, value): if name.startswith('_'): super().__setattr__(name, value) else: print('setattr:', name, value) setattr(self._obj, name, value) # Delegate attribute deletion def __delattr__(self, name): if name.startswith('_'): super().__delattr__(name) else: print('delattr:', name) delattr(self._obj, name) Delegation is sometimes used as an alternative to inheritance. For example, instead of writing code like this: 1234567891011class A: def spam(self, x): print('A.spam', x) def foo(self): print('A.foo')class B(A): def spam(self, x): print('B.spam') super().spam(x) def bar(self): print('B.bar') A solution involving delegation would be written as follows: 123456789101112131415class A: def spam(self, x): print('A.spam', x) def foo(self): print('A.foo')class B: def __init__(self): self._a = A() def spam(self, x): print('B.spam', x) self._a.spam(x) def bar(self): print('B.bar') def __getattr__(self, name): return getattr(self._a, name) It is also important to emphasize that the __getattr__() method usually does not apply to most special methods that start and end with double underscores. 12345class ListLike: def __init__(self): self._items = [] def __getattr__(self, name): return getattr(self._items, name) If you try to make a ListLike object, you’ll find that it supports the common list methods, such as append() and insert(). However, it does not support any of the operators like len(), item lookup, and so forth. 12345678910111213&gt;&gt;&gt; a = ListLike()&gt;&gt;&gt; a.append(2)&gt;&gt;&gt; a.insert(0, 1)&gt;&gt;&gt; a.sort()&gt;&gt;&gt; len(a)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: object of type 'ListLike' has no len()&gt;&gt;&gt; a[0]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'ListLike' object does not support indexing&gt;&gt;&gt; To support the different operators, you have to manually delegate the associated special methods yourself.123456789101112131415class ListLike: def __init__(self): self._items = [] def __getattr__(self, name): return getattr(self._items, name) # Added special methods to support certain list operations def __len__(self): return len(self._items) def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] Defining More Than One Constructor in a ClassTo define a class with more than one constructor, you should use a class method. 123456789101112131415161718&gt;&gt;&gt; import time&gt;&gt;&gt; class Date:... def __init__(self, year, month, day):... self.year= year... self.month= month... self.day= day... @classmethod... def today(cls):... t= time.localtime()... return cls(t.tm_year, t.tm_mon, t.tm_mday)... &gt;&gt;&gt; a= Date(2012,4,5)&gt;&gt;&gt; b=Date.today()&gt;&gt;&gt; b&lt;__main__.Date object at 0x05786D90&gt;&gt;&gt;&gt; b.year2018&gt;&gt;&gt; Instead of defining a separate class method, you might be inclined to implement the __init__() method in a way that allows for different calling conventions. For example: 123456class Date: def __init__(self, *args): if len(args) == 0: t = time.localtime() args = (t.tm_year, t.tm_mon, t.tm_mday) self.year, self.month, self.day = args Although this technique works in certain cases, it often leads to code that is hard to understand and difficult to maintain. For example, this implementation won’t show useful help strings (with argument names). In addition, code that creates Date instances will be less clear. Compare and contrast the following: 1234a = Date(2012, 12, 21) # Clear. A specific date.b = Date() # ??? What does this do?# Class method versionc = Date.today() # Clear. Today's date. Creating an Instance Without Invoking initA bare uninitialized instance can be created by directly calling the __new__() method of a class. 12345678910111213&gt;&gt;&gt; class Date:... def __init__(self, year, month, day):... self.year= year... self.month= month... self.day= day... &gt;&gt;&gt; d= Date.__new__(Date)&gt;&gt;&gt; d&lt;__main__.Date object at 0x05786E10&gt;&gt;&gt;&gt; d.yearTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Date' object has no attribute 'year' As you can see, the resulting instance is uninitialized. Thus, it is now your responsibility to set the appropriate instance variables. For example: 123456789&gt;&gt;&gt; data = &#123;'year':2012, 'month':8, 'day':29&#125;&gt;&gt;&gt; for key, value in data.items():... setattr(d, key, value)...&gt;&gt;&gt; d.year2012&gt;&gt;&gt; d.month8&gt;&gt;&gt; Extending Classes with MixinsTo illustrate, suppose you have an interest in adding various customizations (e.g., logging, set-once, type checking, etc.) to mapping objects. Here are a set of mixin classes that do that: 1234567891011121314151617181920212223242526&gt;&gt;&gt; class LoggedMappingMixin:... __slots__= ()... def __getitem__(self, key):... print('Getting', str(key))... return super().__getitem__(key)... def __setitem__(self, key, value):... print('Setting &#123;&#125;= &#123;!r&#125;'.format(key, value))... return super().__setitem__(key, value)... def __delitem__(self, key):... print('Deleting', str(key))... return super().__delitem__(key)... &gt;&gt;&gt; class SetOnceMappingMixin:... __slots__= ()... def __setitem__(self, key, value):... if key in self:... raise KeyError(str(key) + ' already set')... return super().__setitem__(key, value)... &gt;&gt;&gt; class StringKeysMappingMixin:... __slots__= ()... def __setitem__(self, key, value):... if not isinstance(key, str):... raise TypeError('keys must be strings')... return super().__setitem__(key, value)... These classes, by themselves, are useless. In fact, if you instantiate any one of them, it does nothing useful at all (other than generate exceptions). Instead, they are supposed to be mixed with other mapping classes through multiple inheritance. 12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; class loggedDict(LoggedMappingMixin, dict):... pass... &gt;&gt;&gt; d= loggedDict()&gt;&gt;&gt; d['x'] = 23Setting x= 23&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; d=SetOnceDefaultDict(list)&gt;&gt;&gt; d['x'][]&gt;&gt;&gt; d['x']= 23Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 5, in __setitem__KeyError: 'x already set'&gt;&gt;&gt; d['x'].append(1)&gt;&gt;&gt; d['x'].append(2)&gt;&gt;&gt; d['x'][1, 2]&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; class StringOrderedDict(StringKeysMappingMixin, SetOnceMappingMixin, OrderedDict):... pass... &gt;&gt;&gt; d = StringOrderedDict()&gt;&gt;&gt; d['x'] = 23&gt;&gt;&gt; d[42] = 10Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "mixin.py", line 45, in __setitem__ '''TypeError: keys must be strings&gt;&gt;&gt; d['x'] = 42Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "mixin.py", line 46, in __setitem__ __slots__ = () File "mixin.py", line 24, in __setitem__ if key in self:KeyError: 'x already set'&gt;&gt;&gt; Mixin classes appear in various places in the standard library, mostly as a means for extending the functionality of other classes similar to as shown. They are also one of the main uses of multiple inheritance. For instance, if you are writing network code, you can often use the ThreadingMixIn from the socketserver module to add thread support to other network-related classes. For example, here is a multithreaded XMLRPCserver: 1234from xmlrpc.server import SimpleXMLRPCServerfrom socketserver import ThreadingMixInclass ThreadedXMLRPCServer(ThreadingMixIn, SimpleXMLRPCServer): pass The ThreadingMixIn from the socketserver library has to be mixed with an appropriate server class—it can’t be used all by itself. mixin classes typically have no state of their own. This means there is no __init__() method and no instance variables. In this recipe, the specification of __slots__ = () is meant to serve as a strong hint that the mixin classes do not have their own instance data. If you are thinking about defining a mixin class that has an __init__() method and instance variables, be aware that there is significant peril associated with the fact that the class doesn’t know anything about the other classes it’s going to be mixed with. Thus, any instance variables created would have to be named in a way that avoids name clashes. In addition, the __init__() method would have to be programmed in a way that properly invokes the __init__() method of other classes that are mixed in. In general, thisis difficult to implement since you know nothing about the argument signatures of the other classes. At the very least, you would have to implement something very general using *arg, kwargs. If the __init__() of the mixin class took any arguments of its own, those arguments should be specified by keyword only and named in such a way to avoid name collisions with other arguments. Here is one possible implementation of a mixin defining an init()** and accepting a keyword argument: 12345678910111213141516171819202122232425&gt;&gt;&gt; class RestrictKeysMixin:... def __init__(self, *args, _restrict_key_type, **kwargs):... self.__restrict_key_type= _restrict_key_type... super().__init__(*args, **kwargs)... def __setitem__(self, key, value):... if not isinstance(key, self.__restrict_key_type):... raise TypeError('Keys must be', str(self.__restrict_key_type))... super().__setitem__(key, value)... &gt;&gt;&gt; class RDict(RestrictKeysMixin, dict):... pass... &gt;&gt;&gt; d= RDict(_restrict_key_type= str)&gt;&gt;&gt; d= RDict([('name','Dave'), ('n', 37)], _restrict_key_type= str)&gt;&gt;&gt; f= RDict(_restrict_key_type= str, name= 'Dave', n= 37)&gt;&gt;&gt; f&#123;'name': 'Dave', 'n': 37&#125;&gt;&gt;&gt; f[34]=5Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 7, in __setitem__TypeError: ('Keys must be', "&lt;class 'str'&gt;")&gt;&gt;&gt; f['m']=90&gt;&gt;&gt; f&#123;'name': 'Dave', 'm': 90, 'n': 37&#125; In this example, you’ll notice that initializing an RDict() still takes the arguments understood by dict(). However, there is an extra keyword argument restrict_key_type that is provided to the mixin class. Use of the super() function is an essential and critical part of writing mixin classes. In the solution, the classes redefine certain critical methods, such as __getitem__() and __setitem__(). However, they also need to call the original implementation of those methods. Using super() delegates to the next class on the method resolution order (MRO). This aspect of the recipe, however, is not obvious to novices, because super() is being used in classes that have no parent (at first glance, it might look like an error). However, in a class definition such as this: 12class LoggedDict(LoggedMappingMixin, dict): pass The use of super() in LoggedMappingMixin delegates to the next class over in the multiple inheritance list. That is, a call such as super().__getitem__() in LoggedMappingMixin actually steps over and invokes dict.__getitem__(). Without this behavior, the mixin class wouldn’t work at all. An alternative implementation of mixins involves the use of class decorators. For example, consider this code: 12345678910111213141516171819202122&gt;&gt;&gt; def LoggedMapping(cls):... cls_getitem= cls.__getitem__... cls_setitem= cls.__setitem__... cls_delitem= cls.__delitem__... def __getitem__(self, key):... print('Getting', str(key))... return cls_getitem(self, key)... def __setitem__(self, key, item):... print('Setting &#123;&#125;= &#123;!r&#125;'.format(key, value))... return cls_setitem(self, key, value)... def __delitem__(self, key):... print('Deleting', str(key))... return cls_delitem(self, key)... cls.__getitem__= __getitem__... cls.__setitem__= __setitem__... cls.__delitem__= __delitem__... return cls... &gt;&gt;&gt; @LoggedMapping... class LoggedDict(dict):... pass... Implementing Stateful Objects or State MachinesIn certain applications, you might have objects that operate differently according to some kind of internal state. For example, consider a simple class representing a connection: 12345678910111213141516171819class Connection: def __init__(self): self.state = 'CLOSED' def read(self): if self.state != 'OPEN': raise RuntimeError('Not open') print('reading') def write(self, data): if self.state != 'OPEN': raise RuntimeError('Not open') print('writing') def open(self): if self.state == 'OPEN': raise RuntimeError('Already open') self.state = 'OPEN' def close(self): if self.state == 'CLOSED': raise RuntimeError('Already closed') self.state = 'CLOSED' A more elegant approach is to encode each operational state as a separate class and arrange for the Connection class to delegate to the state class. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt;&gt;&gt; class Connection:... def __init__(self):... self.new_state(ClosedConnectionState)... def new_state(self,newstate):... self._state= newstate... def read(self):... return self._state.read(self)... def write(self, data):... return self._state.write(self, data)... def open(self):... return self._state.open(self)... def close(self):... return self._state.close(self)... &gt;&gt;&gt; class ConnectionState:... @staticmethod... def read(conn):... raise NotImplementedError()... @staticmethod... def write(conn, data):... raise NotImplementedError()... @staticmethod... def open(conn):... raise NotImplementedError()... @staticmethod... def close(conn):... raise NotImplementedError()... &gt;&gt;&gt; class ClosedConnectionState(ConnectionState):... @staticmethod... def read(conn):... raise RuntimeError('Not open')... @staticmethod... def write(conn, data):... raise RuntimeError('Not open')... @staticmethod... def open(conn):... conn.new_state(OpenConnectionState)... @staticmethod... def close(conn):... raise RuntimeError('Already closed')... &gt;&gt;&gt; class OpenConnectionState(ConnectionState):... @staticmethod... def read(conn):... print('reading')... @staticmethod... def write(conn, data):... print('writing')... @staticmethod... def open(conn):... raise RuntimeError('Already open')... @staticmethod... def close(conn):... conn.new_state(ClosedConnectionState)... Here is an interactive session that illustrates the use of these classes: 12345678910111213141516171819202122&gt;&gt;&gt; c = Connection()&gt;&gt;&gt; c._state&lt;class '__main__.ClosedConnectionState'&gt;&gt;&gt;&gt; c.read()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "example.py", line 10, in read return self._state.read(self) File "example.py", line 43, in read raise RuntimeError('Not open')RuntimeError: Not open&gt;&gt;&gt; c.open()&gt;&gt;&gt; c._state&lt;class '__main__.OpenConnectionState'&gt;&gt;&gt;&gt; c.read()reading&gt;&gt;&gt; c.write('hello')writing&gt;&gt;&gt; c.close()&gt;&gt;&gt; c._state&lt;class '__main__.ClosedConnectionState'&gt;&gt;&gt;&gt; It might look a little weird, but each state is implemented by a class with static methods, each of which take an instance of Connection as the first argument. This design is based on a decision to not store any instance data in the different state classes themselves. Instead, all instance data should be stored on the Connection instance. An alternative implementation technique concerns direct manipulation of the __class__ attribute of instances. 12345678910111213141516171819202122232425262728293031class Connection: def __init__(self): self.new_state(ClosedConnection) def new_state(self, newstate): self.__class__ = newstate def read(self): raise NotImplementedError() def write(self, data): raise NotImplementedError() def open(self): raise NotImplementedError() def close(self): raise NotImplementedError()class ClosedConnection(Connection): def read(self): raise RuntimeError('Not open') def write(self, data): raise RuntimeError('Not open') def open(self): self.new_state(OpenConnection) def close(self): raise RuntimeError('Already closed')class OpenConnection(Connection): def read(self): print('reading') def write(self, data): print('writing') def open(self): raise RuntimeError('Already open') def close(self): self.new_state(ClosedConnection) Finally, either technique is useful in implementing more complicated state machines— especially in code that might otherwise feature large if-elif-else blocks. 12345678910111213141516171819202122232425262728293031323334353637383940# Original implementationclass State: def __init__(self): self.state = 'A' def action(self, x): if state == 'A': # Action for A ... state = 'B' elif state == 'B': # Action for B ... state = 'C' elif state == 'C': # Action for C ... state = 'A'# Alternative implementationclass State: def __init__(self): self.new_state(State_A) def new_state(self, state): self.__class__ = state def action(self, x): raise NotImplementedError()class State_A(State): def action(self, x): # Action for A ... self.new_state(State_B)class State_B(State): def action(self, x): # Action for B ... self.new_state(State_C)class State_C(State): def action(self, x): # Action for C ... self.new_state(State_A) Calling a Method on an Object Given the Name As a StringYou have the name of a method that you want to call on an object stored in a string and you want to execute the method. For simple cases, you might use getattr(). 12345678910111213&gt;&gt;&gt; class Point:... def __init__(self, x, y):... self.x= x... self.y =y... def __repr__(self):... return 'Point(&#123;!r:&#125;, &#123;!r:&#125;)'.format(self.x, self.y)... def distance(self, x,y):... return math.hypot(self.x-x, self.y- y)... &gt;&gt;&gt; p= Point(2,3)&gt;&gt;&gt; d= getattr(p, 'distance')(0, 0)&gt;&gt;&gt; d3.6055512754639896 An alternative approach is to use operator.methodcaller().1234567891011121314151617&gt;&gt;&gt; import operator&gt;&gt;&gt; operator.methodcaller('distance', 0,0 )(p)3.6055512754639896&gt;&gt;&gt; points = [... Point(1, 2),... Point(3, 0),... Point(10, -3),... Point(-5, -7),... Point(-1, 8),... Point(3, 2)... ]... &gt;&gt;&gt; points.sort(key= operator.methodcaller('distance', 0, 0))&gt;&gt;&gt; points[Point(1, 2), Point(3, 0), Point(3, 2), Point(-1, 8), Point(-5, -7), Point(10, -3)]&gt;&gt;&gt; Calling a method is actually two separate steps involving an attribute lookup and a function call. Therefore, to call a method, you simply look up the attribute using getattr(), as for any other attribute. To invoke the result as a method, simply treat the result of the lookup as a function. operator.methodcaller() creates a callable object, but also fixes any arguments that are going to be supplied to the method. All that you need to do is provide the appropriate self argument. Implementing the Visitor PatternYou need to write code that processes or navigates through a complicated data structure consisting of many different kinds of objects, each of which needs to be handled in a different way. For example, walking through a tree structure and performing different actions depending on what kind of tree nodes are encountered. The problem addressed by this recipe is one that often arises in programs that build data structures consisting of a large number of different kinds of objects. To illustrate, suppose you are trying to write a program that represents mathematical expressions. To do that, the program might employ a number of classes, like this: 12345678910111213141516171819202122232425&gt;&gt;&gt; class Node: pass... &gt;&gt;&gt; class UnaryOperator(Node):... def __init__(self, operand):... self.operand= operand... &gt;&gt;&gt; class BinaryOperator(Node):... def __init__(self, left, right):... self.left= left... self.right= right... &gt;&gt;&gt; class Add(BinaryOperator): pass... &gt;&gt;&gt; class Sub(BinaryOperator): pass... &gt;&gt;&gt; class Mul(BinaryOperator): pass... &gt;&gt;&gt; class Div(BinaryOperator): pass... &gt;&gt;&gt; class Negate(UnaryOperator): pass... &gt;&gt;&gt; class Number(Node):... def __init__(self, value):... self.value= value... These classes would then be used to build up nested data structures, like this: 12345# Representation of 1 + 2 * (3 - 4) / 5t1 = Sub(Number(3), Number(4))t2 = Mul(Number(2), t1)t3 = Div(t2, Number(5))t4 = Add(Number(1), t3) For example, given such an expression, a program might want to do any number of things (e.g., produce output, generate instructions, perform translation, etc.). To enable general-purpose processing, a common solution is to implement the so-called “visitor pattern” using a class similar to this: 12345678910&gt;&gt;&gt; class NodeVisitor:... def visit(self, node):... methname= 'visit_'+ type(node).__name__... meth= getattr(self, methname, None)... if meth is None:... meth= self.generic_visit... return meth(node)... def generic_visit(self, node):... raise RuntimeError('No &#123;&#125; method'.format('visit_'+ type(node).__name__))... To use this class, a programmer inherits from it and implements various methods of the form visit_Name(), where Name is substituted with the node type. 1234567891011121314&gt;&gt;&gt; class Evaluator(NodeVisitor):... def visit_Number(self, node):... return node.value... def visit_Add(self, node):... return self.visit(node.left) + self.visit(node.right)... def visit_Sub(self, node):... return self.visit(node.left)- self.visit(node.right)... def visit_Mul(self, node):... return self.visit(node.left)* self.visit(node.right)... def visit_Div(self, node):... return self.visit(node.left)/ self.visit(node.right)... def visit_Negate(self, node):... return -node.operand... Here is an example of how you would use this class using the previously generated expression: 1234&gt;&gt;&gt; e = Evaluator()&gt;&gt;&gt; e.visit(t4)0.6&gt;&gt;&gt; As a completely different example, here is a class that translates an expression into operations on a simple stack machine: 12345678910111213141516171819202122232425&gt;&gt;&gt; class StackCode(NodeVisitor):... def generate_code(self, node):... self.instructions= []... self.visit(node)... return self.instructions... def visit_Number(self, node):... self.instructions.append(('PUSH', node.value))... def binop(self, node, instruction):... self.visit(node.left)... self.visit(node.right)... self.instructions.append((instruction,))... def visit_Add(self, node):... self.binop(node,'Add')... def visit_Sub(self, node):... self.binop(node, 'SUB')... def visit_Mul(self, node):... self.binop(node, 'MUL')... def visit_Div(self, node):... self.binop(self, 'DIV')... def unaryop(self, node, instruction):... self.visit(node.operand)... self.instrctions.append((instruction,))... def visit_Negate(self, node):... self.unaryop(node, 'NEG')... Here is an example of this class in action: 12345&gt;&gt;&gt; s = StackCode()&gt;&gt;&gt; s.generate_code(t4)[('PUSH', 1), ('PUSH', 2), ('PUSH', 3), ('PUSH', 4), ('SUB',), ('MUL',), ('PUSH', 5), ('DIV',), ('ADD',)]&gt;&gt;&gt; There are really two key ideas in this recipe. The first is a design strategy where code that manipulates a complicated data structure is decoupled from the data structure itself. That is, in this recipe, none of the various Node classes provide any implementation that does anything with the data. Instead, all of the data manipulation is carried out by specific implementations of the separate NodeVisitor class. This separation makes the code extremely general purpose. The second major idea of this recipe is in the implementation of the visitor class itself. In the visitor, you want to dispatch to a different handling method based on some value such as the node type. In a naive implementation, you might be inclined to write a huge if statement, like this: 12345678910class NodeVisitor: def visit(self, node): nodetype = type(node).__name__ if nodetype == 'Number': return self.visit_Number(node) elif nodetype == 'Add': return self.visit_Add(node) elif nodetype == 'Sub': return self.visit_Sub(node) ... It’s much better to play a little trick where you form the name of a method and go fetch it with the getattr() function, as shown. The generic_visit() method in the solution is a fallback should no matching handler method be found. In this recipe, it raises an exception to alert the programmer that an unexpected node type was encountered. Within each visitor class, it is common for calculations to be driven by recursive calls to the visit() method. For example: 1234class Evaluator(NodeVisitor): ... def visit_Add(self, node): return self.visit(node.left) + self.visit(node.right) Implementing the Visitor Pattern Without RecursionYou’re writing code that navigates through a deeply nested tree structure using the visitor pattern, but it blows up due to exceeding the recursion limit. You’d like to eliminate the recursion, but keep the programming style of the visitor pattern. Clever use of generators can sometimes be used to eliminate recursion from algorithms involving tree traversal or searching. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import typesclass Node: passclass NodeVisitor: def visit(self, node): stack= [node] last_result= None while stack: try: last= stack[-1] if isinstance(last, types.GeneratorType): stack.append(last.send(last_result)) last_result= None elif isinstance(last, Node): stack.append(self._visit(stack.pop())) else: last_result= stack.pop() except StopIteration: stack.pop() return last_result def _visit(self, node): methname= 'visit_' + type(node).__name__ meth=getattr(self, methname, None) if meth is None: meth= self.generic_visit return meth(node) def generic_visit(self, node): raise RuntimeError('No &#123;&#125; method'.format('visit_'+ type(node).__name__))class UnaryOperator(Node): def __init__(self, operand): self.operand= operandclass BinaryOperator(Node): def __init__(self, left, right): self.left= left self.right= rightclass Add(BinaryOperator): passclass Sub(BinaryOperator): passclass Mul(BinaryOperator): passclass Div(BinaryOperator): pass class Negate(UnaryOperator): pass class Number(Node): def __init__(self, value): self.value= valueclass Evaluator(NodeVisitor): def visit_Number(self, node): return node.value def visit_Add(self, node): return self.visit(node.left) + self.visit(node.right) def visit_Sub(self, node): return self.visit(node.left) - self.visit(node.right) def visit_Mul(self, node): return self.visit(node.left) * self.visit(node.right) def visit_Div(self, node): return self.visit(node.left) / self.visit(node.right) def visit_Negate(self, node): return - self.visit(node.operand)if __name__=='__main__': t1= Sub(Number(3), Number(4)) t2= Mul(Number(2), t1) t3= Div(t2, Number(5)) t4= Add(Number(1), t3) e= Evaluator() print(e.visit(t4)) # Outputs 0.6 The preceding code works for simple expressions. However, the implementation of Evaluator uses recursion and crashes if things get too nested. 1234567891011121314&gt;&gt;&gt; a = Number(0)&gt;&gt;&gt; for n in range(1, 100000):... a = Add(a, Number(n))...&gt;&gt;&gt; e = Evaluator()&gt;&gt;&gt; e.visit(a)Traceback (most recent call last):... File "visitor.py", line 29, in _visit return meth(node) File "visitor.py", line 67, in visit_Add return self.visit(node.left) + self.visit(node.right)RuntimeError: maximum recursion depth exceeded&gt;&gt;&gt; Now let’s change the Evaluator class ever so slightly to the following: 123456789101112131415161718192021222324class Evaluator(NodeVisitor): def visit_Number(self, node): return node.value def visit_Add(self, node): print('Add:', node) lhs= yield node.left print('left=', lhs) rhs= yield node.right print('right=', rhs) yield lhs + rhs #yield (yield node.left) + (yield node.right) def visit_Sub(self, node): yield (yield node.left) - (yield node.right) def visit_Mul(self, node): yield (yield node.left) * (yield node.right) def visit_Div(self, node): yield (yield node.left) / (yield node.right) def visit_Negate(self, node): yield - (yield node.operand) If you try the same recursive experiment, you’ll find that it suddenly works. 12345678&gt;&gt;&gt; a = Number(0)&gt;&gt;&gt; for n in range(1,100000):... a = Add(a, Number(n))...&gt;&gt;&gt; e = Evaluator()&gt;&gt;&gt; e.visit(a)4999950000&gt;&gt;&gt; First, in problems related to tree traversal, a common implementation strategy for avoiding recursion is to write algorithms involving a stack or queue. For example, depthfirst traversal can be implemented entirely by pushing nodes onto a stack when first encountered and then popping them off once processing has finished. The central core of the visit() method in the solution is built around this idea. The algorithm starts by pushing the initial node onto the stack list and runs until the stack is empty. During execution, the stack will grow according to the depth of the underlying tree structure. The second insight concerns the behavior of the yield statement in generators. When yield is encountered, the behavior of a generator is to emit a value and to suspend. This recipe uses this as a replacement for recursion. For example, instead of writing a recursive expression like this: value = self.visit(node.left) you replace it with the following: value = yield node.left Behind the scenes, this sends the node in question (node.left) back to the visit() method. The visit() method then carries out the execution of the appropriate visit_Name() method for that node. In some sense, this is almost the opposite of recursion. That is, instead of calling visit() recursively to move the algorithm forward, the yield statement is being used to temporarily back out of the computation in progress. Thus, the yield is essentially a signal that tells the algorithm that the yielded node needs to be processed first before further progress can be made. The final part of this recipe concerns propagation of results. When generator functions are used, you can no longer use return statements to emit values (doing so will cause a SyntaxError exception). Thus, the yield statement has to do double duty to cover the case. In this recipe, if the value produced by a yield statement is a non-Node type, it is assumed to be a value that will be propagated to the next step of the calculation. This is the purpose of the last_return variable in the code. Typically, this would hold the last value yielded by a visit method. That value would then be sent into the previously executing method, where it would show up as the return value from a yield statement. For example, in this code: value = yield node.left The value variable gets the value of last_return, which is the result returned by the visitor method invoked for node.left. The code works by simply looking at the top of the stack and deciding what to do next. If it’s a generator, then its send() method is invoked with the last result (if any) and the result appended onto the stack for further processing. The value returned by send() is the same value that was given to the yield statement. Thus, in a statement such as yield node.left, the Node instance node.left is returned by send() and placed on the top of the stack. If the top of the stack is a Node instance, then it is replaced by the result of calling the appropriate visit method for that node. This is where the underlying recursion is being eliminated. Instead of the various visit methods directly calling visit() recursively, it takes place here. As long as the methods use yield, it all works out. Finally, if the top of the stack is anything else, it’s assumed to be a return value of some kind. It just gets popped off the stack and placed into last_result. If the next item on the stack is a generator, then it gets sent in as a return value for the yield. It should be noted that the final return value of visit() is also set to last_result. This is what makes this recipe work with a traditional recursive implementation. If no generators are being used, this value simply holds the value given to any return statements used in the code. Making Classes Support Comparison OperationsPython classes can support comparison by implementing a special method for each comparison operator. For example, to support the &gt;= operator, you define a __ge__() method in the classes. Although defining a single method is usually no problem, it quickly gets tedious to create implementations of every possible comparison operator. The functools.total_ordering decorator can be used to simplify this process. To use it, you decorate a class with it, and define __eq__() and one other comparison method (__lt__, __le__, __gt__, or __ge__()). The decorator then fills in the other comparison methods for you. 1234567891011121314151617181920212223242526&gt;&gt;&gt; from functools import total_ordering&gt;&gt;&gt; class Room:... def __init__(self, name, length, width):... self.name= name... self.length= length... self.width= width... self.square_feet= self.length * self.width... &gt;&gt;&gt; @total_ordering... class House:... def __init__(self, name, style):... self.name= name... self.style= style... self.rooms=list()... @property... def living_space_footage(self):... return sum(r.square_feet for r in self.rooms)... def add_room(self, room):... self.rooms.append(room)... def __str__(self):... return '&#123;&#125;: &#123;&#125; square foot &#123;&#125;'.format(self.name, self.living_space_footage, self.style)... def __eq__(self, other):... return self.living_space_footage== other.living_space_footage... def __lt__(self, other):... return self.living_space_footage &lt; other.living_space_footage... Here, the House class has been decorated with @total_ordering. Definitions of__eq__() and __lt__ are provided to compare houses based on the total square footage of their rooms. This minimum definition is all that is required to make all of the other comparison operations work. 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; h1 = House('h1', 'Cape')&gt;&gt;&gt; h1.add_room(Room('Master Bedroom', 14, 21))&gt;&gt;&gt; h1.add_room(Room('Living Room', 18, 20))&gt;&gt;&gt; h1.add_room(Room('Kitchen', 12, 16))&gt;&gt;&gt; h1.add_room(Room('Office', 12, 12))&gt;&gt;&gt; &gt;&gt;&gt; h2 = House('h2', 'Ranch')&gt;&gt;&gt; h2.add_room(Room('Master Bedroom', 14, 21))&gt;&gt;&gt; h2.add_room(Room('Living Room', 18, 20))&gt;&gt;&gt; h2.add_room(Room('Kitchen', 12, 16))&gt;&gt;&gt; &gt;&gt;&gt; h3 = House('h3', 'Split')&gt;&gt;&gt; h3.add_room(Room('Master Bedroom', 14, 21))&gt;&gt;&gt; h3.add_room(Room('Living Room', 18, 20))&gt;&gt;&gt; h3.add_room(Room('Office', 12, 16))&gt;&gt;&gt; h3.add_room(Room('Kitchen', 15, 17))&gt;&gt;&gt; &gt;&gt;&gt; houses = [h1, h2, h3]&gt;&gt;&gt; &gt;&gt;&gt; print('Is h1 bigger than h2?', h1 &gt; h2)Is h1 bigger than h2? True&gt;&gt;&gt; print('Is h2 smaller than h3?', h2 &lt; h3) # prints TrueIs h2 smaller than h3? True&gt;&gt;&gt; print('Is h2 greater than or equal to h1?', h2 &gt;= h1) # Prints FalseIs h2 greater than or equal to h1? False&gt;&gt;&gt; print('Which one is biggest?', max(houses)) # Prints 'h3: 1101-square-foot Split'Which one is biggest? h3: 1101 square foot Split&gt;&gt;&gt; print('Which is smallest?', min(houses)) # Prints 'h2: 846-square-foot Ranch'Which is smallest? h2: 846 square foot Ranch&gt;&gt;&gt; f you’ve written the code to make a class support all of the basic comparison operators, then total_ordering probably doesn’t seem all that magical: it literally defines a mapping from each of the comparison-supporting methods to all of the other ones that would be required. So, if you defined__lt__ in your class as in the solution, it is used to build all of the other comparison operators. 12345678910class House: def __eq__(self, other): ... def __lt__(self, other): ... # Methods created by @total_ordering __le__ = lambda self, other: self &lt; other or self == other __gt__ = lambda self, other: not (self &lt; other or self == other) __ge__ = lambda self, other: not (self &lt; other) __ne__ = lambda self, other: not self == other Creating Cached InstancesWhen creating instances of a class, you want to return a cached reference to a previous instance created with the same arguments (if any). Practical examples include the behavior of libraries, such as the logging module, that only want to associate a single logger instance with a given name. 12345678&gt;&gt;&gt; import logging&gt;&gt;&gt; a = logging.getLogger(&apos;foo&apos;)&gt;&gt;&gt; b = logging.getLogger(&apos;bar&apos;)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; c = logging.getLogger(&apos;foo&apos;)&gt;&gt;&gt; a is cTrue To implement this behavior, you should make use of a factory function that’s separate from the class itself.123456789101112131415&gt;&gt;&gt; class Spam:... def __init__(self, name):... self.name= name... &gt;&gt;&gt; import weakref&gt;&gt;&gt; _spam_cache= weakref.WeakValueDictionary()&gt;&gt;&gt; def get_spam(name):... if name not in _spam_cache:... s=Spam(name)... _spam_cache[name] = s... else:... s= _spam_cache[name]... return s &gt;&gt;&gt; If you use this implementation, you’ll find that it behaves in the manner shown earlier: 12345678&gt;&gt;&gt; a = get_spam('foo')&gt;&gt;&gt; b = get_spam('bar')&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; c = get_spam('foo')&gt;&gt;&gt; a is cTrue&gt;&gt;&gt; Writing a special factory function is often a simple approach for altering the normal rules of instance creation. One question that often arises at this point is whether or not a more elegant approach could be taken. For example, you might consider a solution that redefines the __new__() method of a class as follows:1234567891011121314&gt;&gt;&gt; class Spam:... _spam_cache= weakref.WeakValueDictionary()... def __new__(cls, name):... if name in cls._spam_cache:... return cls._spam_cache[name]... else:... self= super().__new__(cls)... cls._spam_cache[name]= self... return self... def __init__(self, name):... print('Initializing Spam')... self.name= name... However, a major problem is that the __init__() method always gets called, regardless of whether the instance was cached or not. 123456&gt;&gt;&gt; s= Spam('Dave')Initializing Spam&gt;&gt;&gt; t= Spam('Dave')Initializing Spam&gt;&gt;&gt; s is tTrue That behavior is probably not what you want. So, to solve the problem of caching without reinitialization, you need to take a slightly different approach. When maintaining a cache of instances, you often only want to keep items in the cache as long as they’re actually being used somewhere in the program. A WeakValueDictionary instance only holds onto the referenced items as long as they exist somewhere else. Otherwise, the dictionary keys disappear when instances are no longer being used. 12345678910111213&gt;&gt;&gt; a = get_spam('foo')&gt;&gt;&gt; b = get_spam('bar')&gt;&gt;&gt; c = get_spam('foo')&gt;&gt;&gt; list(_spam_cache)['foo', 'bar']&gt;&gt;&gt; del a&gt;&gt;&gt; del c&gt;&gt;&gt; list(_spam_cache)['bar']&gt;&gt;&gt; del b&gt;&gt;&gt; list(_spam_cache)[]&gt;&gt;&gt; One immediate concern with this recipe might be its reliance on global variables and a factory function that’s decoupled from the original class definition. One way to clean this up is to put the caching code into a separate manager class and glue things together like this: 12345678910111213141516171819202122&gt;&gt;&gt; class CachedSpamManager:... def __init__(self):... self._cache= weakref.WeakValueDictionary()... def get_spam(self, name):... if name not in self._cache:... s= Spam(name)... self._cache[name] = s... else:... s= self._cache[name]... return s... def clear(self):... self._cache.clear()... &gt;&gt;&gt; class Spam:... manager= CachedSpamManager()... def __init__(self, name):... self.name=name... print('name:', name)... &gt;&gt;&gt; def get_spam(name):... return Spam.manager.get_spam(name)... Another design consideration is whether or not you want to leave the class definition exposed to the user. If you do nothing, a user can easily make instances, bypassing the caching mechanism: 12345&gt;&gt;&gt; a = Spam('foo')&gt;&gt;&gt; b = Spam('foo')&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; If preventing this is important, you can take certain steps to avoid it. For example, you might give the class a name starting with an underscore, such as _Spam, which at least gives the user a clue that they shouldn’t access it directly. Alternatively, if you want to give users a stronger hint that they shouldn’t instantiate Spam instances directly, you can make __init__() raise an exception and use a class method to make an alternate constructor like this: 12345678910111213141516171819202122&gt;&gt;&gt; class Spam:... def __init__(self, *args, **kwargs):... raise RuntimeError('Can not instantiate directly')... @classmethod... def _new(cls, name):... self= cls.__new__(cls)... self.name = name... &gt;&gt;&gt; class CachedSpamManager:... def __init__(self):... self._cache= weakref.WeakValueDictionary()... def get_spam(self, name):... if name not in self._cache:... s= Spam._new(name)... self._cache[name] = s... else:... s= self._cache[name]... return s... def clear(self):... self._cache.clear()... &gt;&gt;&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Functions]]></title>
    <url>%2F2018%2F12%2F15%2FC07-Functions%2F</url>
    <content type="text"><![CDATA[Writing Functions That Accept Any Number of ArgumentsTo accept any number of keyword arguments, use an argument that starts with **. 1234567891011121314&gt;&gt;&gt; import html&gt;&gt;&gt; make_element('item', 'Albatross', size='large', quantity=6)'&lt;itemsize= "large" quantity= "6"&gt;Albatross&lt;/item&gt;'&gt;&gt;&gt; def make_element(name, value, **attrs):... keyvals=['%s= "%s"' %item for item in attrs.items() ]... attr_str= ' '.join(keyvals)... element= '&lt;&#123;name&#125; &#123;attrs&#125;&gt;&#123;value&#125;&lt;/&#123;name&#125;&gt;'.format(... name= name,... value= html.escape(value),... attrs= attr_str)... return element... &gt;&gt;&gt; make_element('item', 'Albatross', size='large', quantity=6)'&lt;item size= "large" quantity= "6"&gt;Albatross&lt;/item&gt;' If you want a function that can accept both any number of positional and keyword-only arguments, use * and ** together.12345&gt;&gt;&gt; def anyargs(*args, **kwargs):... print(args)... print(kwargs)... A * argument can only appear as the last positional argument in a function definition. A ** argument can only appear as the last argument. A subtle aspect of function definitions is that arguments can still appear after a * argument. 12345&gt;&gt;&gt; def a(x, *args, y):pass... &gt;&gt;&gt; def a(x, *args, y, **kwargs):pass... &gt;&gt;&gt; Writing Functions That Only Accept Keyword ArgumentsYou want a function to only accept certain arguments by keyword. This feature is easy to implement if you place the keyword arguments after a * argument or a single unnamed *.12345678910&gt;&gt;&gt; def recv(maxsize,*,block):... 'Receives a message'... pass... &gt;&gt;&gt; recv(1024, True)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: recv() takes 1 positional argument but 2 were given&gt;&gt;&gt; This technique can also be used to specify keyword arguments for functions that accept a varying number of positional arguments. 12345678910&gt;&gt;&gt; def mininum(*values, clip= None):... m = min(values)... if clip is not None:... m= clip if clip &gt; m else m... return m... &gt;&gt;&gt; mininum(1,5,2,-5,10)-5&gt;&gt;&gt; mininum(1,5,2,-5,10, clip= 0)0 Attaching Informational Metadata to Function ArgumentsFunction argument annotations can be a useful way to give programmers hints about how a function is supposed to be used. 123456&gt;&gt;&gt; def add(x:int,y:int) -&gt; int:... return x+ y... &gt;&gt;&gt; help(add)Help on function add in module __main__:add(x:int, y:int) -&gt; int Function annotations are merely stored in a function’s __annotations__ attribute.1234&gt;&gt;&gt; add.__annotations__&#123;'y': &lt;class 'int'&gt;, 'return': &lt;class 'int'&gt;, 'x': &lt;class 'int'&gt;&#125;&gt;&gt;&gt; Defining Functions with Default ArgumentsOn the surface, defining a function with optional arguments is easy—simply assign values in the definition and make sure that default arguments appear last. 1234def spam(a, b=42): print(a, b)spam(1) # Ok. a=1, b=42spam(1, 2) # Ok. a=1, b=2 If the default value is supposed to be a mutable container, such as a list, set, or dictionary, use None as the default and write code like this: 12345# Using a list as a default valuedef spam(a, b=None): if b is None: b = [] ... If, instead of providing a default value, you want to write code that merely tests whether an optional argument was given an interesting value or not, use this idiom: 12345_no_value = object()def spam(a, b=_no_value): if b is _no_value: print('No b value supplied') ... Here’s how this function behaves: 12345&gt;&gt;&gt; spam(1)No b value supplied&gt;&gt;&gt; spam(1, 2) # b = 2&gt;&gt;&gt; spam(1, None) # b = None&gt;&gt;&gt; Carefully observe that there is a distinction between passing no value at all and passing a value of None. The tricky part here is that you can’t use a default value of None, 0, or False to test for the presence of a user-supplied argument (since all of these are perfectly valid values that a user might supply). Thus, you need something else to test against. To solve this problem, you can create a unique private instance of object, as shown in the solution (the _no_value variable). In the function, you then check the identity of the supplied argument against this special value to see if an argument was supplied or not. The thinking here is that it would be extremely unlikely for a user to pass the _no_value instance in as an input value. Therefore, it becomes a safe value to check against if you’re trying to determine whether an argument was supplied or not. The values assigned as a default are bound only once at the time of function definition.1234567891011&gt;&gt;&gt; x = 42&gt;&gt;&gt; def spam(a, b=x):... print(a, b)...&gt;&gt;&gt; spam(1)1 42&gt;&gt;&gt; x = 23 # Has no effect&gt;&gt;&gt; spam(1)1 42&gt;&gt;&gt; Notice how changing the variable x (which was used as a default value) has no effect whatsoever. This is because the default value was fixed at function definition time. Second, the values assigned as defaults should always be immutable objects, such as None, True, False, numbers, or strings. Specifically, never write code like this: 12def spam(a, b=[]): # NO! ... If you do this, you can run into all sorts of trouble if the default value ever escapes the function and gets modified. Such changes will permanently alter the default value across future function calls. 1234567891011121314&gt;&gt;&gt; def spam(a, b=[]):... print(b)... return b...&gt;&gt;&gt; x = spam(1)&gt;&gt;&gt; x[]&gt;&gt;&gt; x.append(99)&gt;&gt;&gt; x.append('Yow!')&gt;&gt;&gt; x[99, 'Yow!']&gt;&gt;&gt; spam(1) # Modified list gets returned![99, 'Yow!']&gt;&gt;&gt; Capturing Variables in Anonymous FunctionsYou’ve defined an anonymous function using lambda, but you also need to capture the values of certain variables at the time of definition. Consider the behavior of the following code: 1234&gt;&gt;&gt; x = 10&gt;&gt;&gt; a = lambda y: x + y&gt;&gt;&gt; x = 20&gt;&gt;&gt; b = lambda y: x + y Now ask yourself a question. What are the values of a(10) and b(10)? If you think the results might be 20 and 30, you would be wrong:12345&gt;&gt;&gt; a(10)30&gt;&gt;&gt; b(10)30 The problem here is that the value of x used in the lambda expression is a free variable that gets bound at runtime, not definition time. Thus, the value of x in the lambda expressions is whatever the value of the x variable happens to be at the time of execution. The problem addressed in this recipe is something that tends to come up in code that tries to be just a little bit too clever with the use of lambda functions. For example, creating a list of lambda expressions using a list comprehension or in a loop of some kind and expecting the lambda functions to remember the iteration variable at the time of definition. For example: 12345678910&gt;&gt;&gt; funcs = [lambda x: x+n for n in range(5)]&gt;&gt;&gt; for f in funcs:... print(f(0))...44444&gt;&gt;&gt; Notice how all functions think that n has the last value during iteration. Now compare to the following: 123456789101112&gt;&gt;&gt; funcs = [lambda x, n=n: x+n for n in range(5)]&gt;&gt;&gt; for f in funcs:... print(f(0))...01234&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Carrying Extra State with Callback FunctionsYou’re writing code that relies on the use of callback functions (e.g., event handlers, completion callbacks, etc.), but you want to have the callback function carry extra state for use inside the callback function. To illustrate and for the purposes of testing, define the following function, which invokes a callback: 1234567&gt;&gt;&gt; def apply_async(func, args, *, callback):... result = func(*args)... callback(result)... &gt;&gt;&gt; def add(x, y):... return x + y... One way to carry extra information in a callback is to use a bound-method instead of a simple function. For example, this class keeps an internal sequence number that is incremented every time a result is received: 12345678910111213&gt;&gt;&gt; class ResultHandler:... def __init__(self):... self.sequence= 0... def handler(self, result):... self.sequence+= 1... print('[&#123;&#125;] Got: &#123;&#125;'.format(self.sequence, result))... &gt;&gt;&gt; r= ResultHandler()&gt;&gt;&gt; apply_async(add, (2,3), callback= r.handler)[1] Got: 5&gt;&gt;&gt; apply_async(add, ('hello', 'world'), callback=r.handler)[2] Got: helloworld&gt;&gt;&gt; As an alternative to a class, you can also use a closure to capture state. 1234567891011121314&gt;&gt;&gt; def make_handler():... sequence= 0... def handler(result):... nonlocal sequence... sequence+= 1... print('[&#123;&#125;] Got: &#123;&#125;'.format(sequence, result))... return handler... &gt;&gt;&gt; handler= make_handler()&gt;&gt;&gt; apply_async(add, (2, 3), callback=handler)[1] Got: 5&gt;&gt;&gt; apply_async(add, ('hello', 'world'), callback=handler)[2] Got: helloworld&gt;&gt;&gt; As yet another variation on this theme, you can sometimes use a coroutine to accomplish the same thing: 12345678910111213&gt;&gt;&gt; def make_handler():... sequence= 0... while True:... result= yield... sequence += 1... print('[&#123;&#125;] Got: &#123;&#125;'.format(sequence, result))... &gt;&gt;&gt; handler= make_handler()&gt;&gt;&gt; next(handler)&gt;&gt;&gt; apply_async(add, (2,3), callback= handler.send)[1] Got: 5&gt;&gt;&gt; apply_async(add, (2,3), callback= handler.send)[2] Got: 5 You can also carry state into a callback using an extra argument and partial function application. For example:12345678910111213141516&gt;&gt;&gt; class SequenceNo:... def __init__(self):... self.sequence= 0... &gt;&gt;&gt; def handler(result, seq):... seq.sequence += 1... print('[&#123;&#125;] Got: &#123;&#125;'.format(seq.sequence, result))... &gt;&gt;&gt; seq= SequenceNo()&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; apply_async(add, (2,3), callback= partial(handler, seq= seq))[1] Got: 5&gt;&gt;&gt; apply_async(add, (2,3), callback= partial(handler, seq= seq))[2] Got: 5&gt;&gt;&gt; Inlining Callback FunctionsCallback functions can be inlined into a function using generators and coroutines. To illustrate, suppose you have a function that performs work and invokes a callback as follows: 123456789101112131415161718192021222324252627&gt;&gt;&gt; def apply_async(func, args, *, callback):... result= function(*args)... callback(result)... from queue import Queuefrom functools import wrapsclass Async: def __init__(self, func, args): self.func = func self.args = argsdef inlined_async(func): @wraps(func) def wrapper(*args): f = func(*args) result_queue = Queue() result_queue.put(None) while True: result = result_queue.get() try: a = f.send(result) apply_async(a.func, a.args, callback=result_queue.put) except StopIteration: breakreturn wrapper These two fragments of code will allow you to inline the callback steps using yield statements.1234567891011121314151617181920212223242526272829&gt;&gt;&gt; def add(x, y):... return x+ y... &gt;&gt;&gt; @inlined_async... def test():... r= yield Async(add, (2,3))... print(r)... r= yield Async(add, ('hello', 'world'))... print(r)... for n in range(10):... r= yield Async(add, (n,n))... print(r)... print('Goodbye')... &gt;&gt;&gt; test()5helloworld024681012141618Goodbye&gt;&gt;&gt; First, in code involving callbacks, the whole point is that the current calculation will suspend and resume at some later point in time (e.g., asynchronously). When the calculation resumes, the callback will get executed to continue the processing. The apply_async() function illustrates the essential parts of executing the callback, although in reality it might be much more complicated (involving threads, processes, event handlers, etc.). The idea that a calculation will suspend and resume naturally maps to the execution model of a generator function. Specifically, the yield operation makes a generator function emit a value and suspend. Subsequent calls to the __next__() or send() method of a generator will make it start again. With this in mind, the core of this recipe is found in the inline_async() decorator function. The key idea is that the decorator will step the generator function through all of its yield statements, one at a time. To do this, a result queue is created and initially populated with a value of None. A loop is then initiated in which a result is popped off the queue and sent into the generator. This advances to the next yield, at which point an instance of Async is received. The loop then looks at the function and arguments, and initiates the asynchronous calculation apply_async(). However, the sneakiest part of this calculation is that instead of using a normal callback function, the callback is set to the queue put() method. At this point, it is left somewhat open as to precisely what happens. The main loop immediately goes back to the top and simply executes a get() operation on the queue. If data is present, it must be the result placed there by the put() callback. If nothing is there, the operation blocks, waiting for a result to arrive at some future time. How that might happen depends on the precise implementation of the apply_async() function. Accessing Variables Defined Inside a ClosureNormally, the inner variables of a closure are completely hidden to the outside world. However, you can provide access by writing accessor functions and attaching them to the closure as function attributes. 123456789101112131415161718192021&gt;&gt;&gt; def sample():... n=0... def func():... print('n=', n)... def get_n():... return n... def set_n(value):... nonlocal n... n= value... func.get_n= get_n... func.set_n= set_n... return func... &gt;&gt;&gt; f=sample()&gt;&gt;&gt; f()n= 0&gt;&gt;&gt; f.set_n(10)&gt;&gt;&gt; f()n= 10&gt;&gt;&gt; f.get_n()10 A slight extension to this recipe can be made to have closures emulate instances of a class. All you need to do is copy the inner functions over to the dictionary of an instance and return it. 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import sys&gt;&gt;&gt; class ClosureInstance:... def __init__(self, locals= None):... if locals is None:... locals= sys._getframe(1).f_locals... print(locals)... self.__dict__.update((key, value) for key, value in locals.items() if callable(value))... def __len__(self):... return self.__dict__['__len__']()... &gt;&gt;&gt; def Stack():... items=[]... def push(item):... items.append(item)... def pop():... return items.pop()... def __len__():... return len(items)... return ClosureInstance()... &gt;&gt;&gt; s=Stack()&#123;'pop': &lt;function Stack.&lt;locals&gt;.pop at 0x010BB810&gt;, 'items': [], 'push': &lt;function Stack.&lt;locals&gt;.push at 0x010BB618&gt;, '__len__': &lt;function Stack.&lt;locals&gt;.__len__ at 0x010BB660&gt;&#125;&gt;&gt;&gt; s.push(1)&gt;&gt;&gt; s.push('he')&gt;&gt;&gt; len(s)2&gt;&gt;&gt; s.pop()'he'&gt;&gt;&gt; However, should you be inclined to do something like this in your code, be aware that it’s still a rather weird substitute for a real class. For example, major features such as inheritance, properties, descriptors, or class methods don’t work. You also have to play some tricks to get special methods to work (e.g., see the implementation of __len__() in ClosureInstance).]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_Data_Encoding_and_Processing]]></title>
    <url>%2F2018%2F12%2F15%2FC06-Data-Encoding-and-Processing%2F</url>
    <content type="text"><![CDATA[Reading and Writing CSV DataFor most kinds of CSV data, use the csv library. 1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... In the preceding code, row will be a tuple. Thus, to access certain fields, you will need to use indexing, such as row[0] (Symbol) and row[4] (Change). Since such indexing can often be confusing, this is one place where you might want to consider the use of named tuples. 123456789from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) #Process row ... This would allow you to use the column headers such as row.Symbol and row.Change instead of indices. It should be noted that this only works if the column headers are valid Python identifiers. Another alternative is to read the data as a sequence of dictionaries instead.1234567import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... In this version, you would access the elements of each row using the row headers. For example, row[‘Symbol’] or row[‘Change’]. To write CSV data, you also use the csv module but create a writer object. 1234567891011121314&gt;&gt;&gt; headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']&gt;&gt;&gt; rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',... 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;,... &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007',... 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;,... &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007',... 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;,... ]... &gt;&gt;&gt; with open('stocks.csv', 'w') as f:... f_csv= csv.DictWriter(f, headers)... f_csv.writeheader()... f_csv.writerows(rows)... Here is one example of performing extra type conversions on CSV data: 12345678910111213141516171819202122232425262728&gt;&gt;&gt; field_types = [ ('Price', float),... ('Change', float),... ('Volume', int) ]... &gt;&gt;&gt; with open('stock.csv') as f:... for row in csv.DictReader(f):... row.update((key, conversion(row[key])) for key, conversion in field_types)... print(row)... &#123;'Price': 39.48, 'Change': -0.18, 'Time': '9:36am', 'Volume': 181800, 'Symbol': 'AA', 'Date': '6/11/2007'&#125;&#123;'Price': 71.38, 'Change': -0.15, 'Time': '9:36am', 'Volume': 195500, 'Symbol': 'AIG', 'Date': '6/11/2007'&#125;&#123;'Price': 62.58, 'Change': -0.46, 'Time': '9:36am', 'Volume': 935000, 'Symbol': 'AXP', 'Date': '6/11/2007'&#125;&gt;&gt;&gt;``` ## Reading and Writing JSON DataJSON encoding supports the basic types of **None**, **bool**, **int**, **float**, and **str**, as well as **lists**, **tuples**, and **dictionaries** containing those types. The format of JSON encoding is almost identical to Python syntax except for a few minor changes. For instance, **True** is mapped to **true**, **False** is mapped to **false**, and **None** is mapped to **null**.```python&gt;&gt;&gt; d = &#123;'a': True,... 'b': 'Hello',... 'c': None&#125;&gt;&gt;&gt; json.dumps(d)'&#123;"b": "Hello", "c": null, "a": true&#125;'&gt;&gt;&gt; Normally, JSON decoding will create dicts or lists from the supplied data. If you want to create different kinds of objects, supply the object_pairs_hook or object_hook to json.loads(). 123456&gt;&gt;&gt; s = '&#123;"name": "ACME", "shares": 50, "price": 490.1&#125;'&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; data = json.loads(s, object_pairs_hook=OrderedDict)&gt;&gt;&gt; dataOrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])&gt;&gt;&gt; Turn a JSON dictionary into a Python object 1234567&gt;&gt;&gt; class JSONObject:... def __init__(self, d):... self.__dict__=d... &gt;&gt;&gt; data= json.loads(s, object_hook= JSONObject)&gt;&gt;&gt; data.name'ACME' The dictionary created by decoding the JSON data is passed as a single argument to __init__(). From there, you are free to use it as you will, such as using it directly as the instance dictionary of the object. If you would like the output to be nicely formatted, you can use the indent argument to json.dumps(). This causes the output to be pretty printed in a format similar to that with the pprint() function. 1234567&gt;&gt;&gt; print(json.dumps(data, indent=4))&#123; "price": 542.23, "name": "ACME", "shares": 100&#125;&gt;&gt;&gt; If you want the keys to be sorted on output, used the sort_keys argument: 123&gt;&gt;&gt; print(json.dumps(data, sort_keys=True))&#123;"name": "ACME", "price": 542.23, "shares": 100&#125;&gt;&gt;&gt; Instances are not normally serializable as JSON. For example: 12345678910111213141516171819&gt;&gt;&gt; class Point:... def __init__(self, x, y):... self.x = x... self.y = y...&gt;&gt;&gt; p = Point(2, 3)&gt;&gt;&gt; json.dumps(p)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/usr/local/lib/python3.3/json/__init__.py", line 226, in dumps return _default_encoder.encode(obj) File "/usr/local/lib/python3.3/json/encoder.py", line 187, in encode chunks = self.iterencode(o, _one_shot=True) File "/usr/local/lib/python3.3/json/encoder.py", line 245, in iterencode return _iterencode(o, 0) File "/usr/local/lib/python3.3/json/encoder.py", line 169, in default raise TypeError(repr(o) + " is not JSON serializable")TypeError: &lt;__main__.Point object at 0x1006f2650&gt; is not JSON serializable&gt;&gt;&gt; If you want to serialize instances, you can supply a function that takes an instance as input and returns a dictionary that can be serialized. 12345&gt;&gt;&gt; def serialize_instance(obj):... d=&#123;'__classname__': type(obj).__name__&#125;... d.update(vars(obj))... return d... If you want to get an instance back, you could write code like this: 12345678910111213141516171819202122&gt;&gt;&gt; classes=&#123;'Point': Point&#125;&gt;&gt;&gt; def unserialize_object(d):... clsname= d.pop('__classname__', None)... if clsname:... cls= classes[clsname]... obj= cls.__new__(cls)... for key, value in d.items():... setattr(obj, key, value)... return obj... else:... return d... &gt;&gt;&gt; p= Point(2,3)&gt;&gt;&gt; s= json.dumps(p, default=serialize_instance)&gt;&gt;&gt; s'&#123;"__classname__": "Point", "x": 2, "y": 3&#125;'&gt;&gt;&gt; a= json.loads(s, object_hook= unserialize_object)&gt;&gt;&gt; a&lt;__main__.Point object at 0x00D00D10&gt;&gt;&gt;&gt; a.x2&gt;&gt;&gt; Parsing Simple XML DataThe xml.etree.ElementTree module can be used to extract data from simple XML documents. 123456789101112131415161718192021from urllib.request import urlopenfrom xml.etree.ElementTree import parse# Download the RSS feed and parse itu = urlopen('http://planet.python.org/rss20.xml')doc = parse(u)# Extract and output tags of interestfor item in doc.iterfind('channel/item'): title = item.findtext('title') date = item.findtext('pubDate') link = item.findtext('link') print(title) print(date) print(link) print()Steve Holden: Python for Data AnalysisMon, 19 Nov 2012 02:13:51 +0000http://holdenweb.blogspot.com/2012/11/python-for-data-analysis.htmlVasudev Ram: The Python Data model (for v2 and v3)Sun, 18 Nov 2012 22:06:47 +0000http://jugad2.blogspot.com/2012/11/the-python-data-model.html The xml.etree.ElementTree.parse() function parses the entire XML document into a document object. From there, you use methods such as find(), iterfind(), and findtext() to search for specific XML elements. The arguments to these functions are the names of a specific tag, such as channel/item or title. Each element represented by the ElementTree module has a few essential attributes and methods that are useful when parsing. The tag attribute contains the name of the tag, the text attribute contains enclosed text, and the get() method can be used to extract attributes (if any). 123456789&gt;&gt;&gt; e=doc.find('channel/title')&gt;&gt;&gt; e&lt;Element 'title' at 0x0101F540&gt;&gt;&gt;&gt; e.tag'title'&gt;&gt;&gt; e.text'Planet Python'&gt;&gt;&gt; e.get('some_attribute')&gt;&gt;&gt; Parsing Huge XML Files IncrementallyYou need to extract data from a huge XML document using as little memory as possible, which are encoded like this: 123456789101112131415161718192021222324# potholes.xml &lt;response&gt; &lt;row&gt; &lt;row ...&gt; &lt;creation_date&gt;2012-11-18T00:00:00&lt;/creation_date&gt;&lt;status&gt;Completed&lt;/status&gt; &lt;completion_date&gt;2012-11-18T00:00:00&lt;/completion_date&gt; &lt;service_request_number&gt;12-01906549&lt;/service_request_number&gt; &lt;type_of_service_request&gt;Pot Hole in Street&lt;/type_of_service_request&gt; &lt;current_activity&gt;Final Outcome&lt;/current_activity&gt; &lt;most_recent_action&gt;CDOT Street Cut ... Outcome&lt;/most_recent_action&gt; &lt;street_address&gt;4714 S TALMAN AVE&lt;/street_address&gt; &lt;zip&gt;60632&lt;/zip&gt; &lt;x_coordinate&gt;1159494.68618856&lt;/x_coordinate&gt; &lt;y_coordinate&gt;1873313.83503384&lt;/y_coordinate&gt; &lt;ward&gt;14&lt;/ward&gt; &lt;police_district&gt;9&lt;/police_district&gt; &lt;community_area&gt;58&lt;/community_area&gt; &lt;latitude&gt;41.808090232127896&lt;/latitude&gt; &lt;longitude&gt;-87.69053684711305&lt;/longitude&gt; &lt;location latitude="41.808090232127896" longitude="-87.69053684711305" /&gt; &lt;/row&gt; &lt;row ...&gt; Suppose you want to write a script that ranks ZIP codes by the number of pothole reports. To do it, you could write code like this: 12345678from xml.etree.ElementTree import parsefrom collections import Counterpotholes_by_zip = Counter()doc = parse('potholes.xml')for pothole in doc.iterfind('row/row'): potholes_by_zip[pothole.findtext('zip')] += 1for zipcode, num in potholes_by_zip.most_common(): print(zipcode, num) The only problem with this script is that it reads and parses the entire XML file into memory. Here is a simple function that can be used to incrementally process huge XML files using a very small memory footprint: 12345678910111213141516171819202122&gt;&gt;&gt; from xml.etree.ElementTree import iterparse&gt;&gt;&gt; def parse_and_remove(filename, path):... path_parts= path.split('/')... doc= iterparse(filename,('start', 'end'))... next(doc)... ... tag_stack=[]... elem_stack=[]... for event, elem in doc:... if event=='start':... tag_stack.append(elem.tag)... elem_stack.append(elem)... elif event=='end':... if tag_stack== path_parts:... yield elem... elem_stack[-2].remove(elem)... try:... tag_stack.pop()... elem_stack.pop()... except IndexError:... pass... First, the iterparse() method allows incremental processing of XML documents. To use it, you supply the filename along with an event list consisting of one or more of the following: start, end, start-ns, and end-ns. The iterator created by iterparse() produces tuples of the form (event, elem), where event is one of the listed events and elem is the resulting XML element. 12345678910111213141516&gt;&gt;&gt; data = iterparse('potholes.xml',('start','end'))&gt;&gt;&gt; next(data)('start', &lt;Element 'response' at 0x100771d60&gt;)&gt;&gt;&gt; next(data)('start', &lt;Element 'row' at 0x100771e68&gt;)&gt;&gt;&gt; next(data)('start', &lt;Element 'row' at 0x100771fc8&gt;)&gt;&gt;&gt; next(data)('start', &lt;Element 'creation_date' at 0x100771f18&gt;)&gt;&gt;&gt; next(data)('end', &lt;Element 'creation_date' at 0x100771f18&gt;)&gt;&gt;&gt; next(data)('start', &lt;Element 'status' at 0x1006a7f18&gt;)&gt;&gt;&gt; next(data)('end', &lt;Element 'status' at 0x1006a7f18&gt;)&gt;&gt;&gt; The start and end events are used to manage stacks of elements and tags. The stacks represent the current hierarchical structure of the document as it’s being parsed, and are also used to determine if an element matches the requested path given to the parse_and_remove() function. If a match is made, yield is used to emit it back to the caller. The following statement after the yield is the core feature of ElementTree that makes this recipe save memory: elem_stack[-2].remove(elem) Turning a Dictionary into XMLYou want to take the data in a Python dictionary and turn it into XML. Although the xml.etree.ElementTree library is commonly used for parsing, it can also be used to create XML documents. 12345678910111213&gt;&gt;&gt; from xml.etree.cElementTree import Element&gt;&gt;&gt; def dict_to_xml(tag, d):... elem= Element(tag)... for key, val in d.items():... child= Element(key)... child.text= str(val)... elem.append(child)... return elem... &gt;&gt;&gt; s = &#123; 'name': 'GOOG', 'shares': 100, 'price':490.1 &#125;&gt;&gt;&gt; e=dict_to_xml('stock', s)&gt;&gt;&gt; e&lt;Element 'stock' at 0x03B07E40&gt; The result of this conversion is an Element instance. For I/O, it is easy to convert this to a byte string using the tostring() function in xml.etree.ElementTree. 123&gt;&gt;&gt; from xml.etree.ElementTree import tostring&gt;&gt;&gt; tostring(e)b'&lt;stock&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;/stock&gt;' If you want to attach attributes to an element, use its set() method. 123&gt;&gt;&gt; e.set('_id', '1234')&gt;&gt;&gt; tostring(e)b'&lt;stock _id="1234"&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;/stock&gt;' If the order of the elements matters, consider making an OrderedDict instead of a normal dictionary. Parsing, Modifying, and Rewriting XMLYou want to read an XML document, make changes to it, and then write it back out as XML. The xml.etree.ElementTree module makes it easy to perform such tasks. Essentially, you start out by parsing the document in the usual way. 1234567891011121314151617181920212223&lt;?xml version="1.0"?&gt;&lt;stop&gt; &lt;id&gt;14791&lt;/id&gt; &lt;nm&gt;Clark &amp;amp; Balmoral&lt;/nm&gt; &lt;sri&gt; &lt;rt&gt;22&lt;/rt&gt; &lt;d&gt;North Bound&lt;/d&gt; &lt;dd&gt;North Bound&lt;/dd&gt; &lt;/sri&gt; &lt;cr&gt;22&lt;/cr&gt; &lt;pre&gt; &lt;pt&gt;5 MIN&lt;/pt&gt; &lt;fd&gt;Howard&lt;/fd&gt; &lt;v&gt;1378&lt;/v&gt; &lt;rn&gt;22&lt;/rn&gt; &lt;/pre&gt; &lt;pre&gt; &lt;pt&gt;15 MIN&lt;/pt&gt; &lt;fd&gt;Howard&lt;/fd&gt; &lt;v&gt;1867&lt;/v&gt; &lt;rn&gt;22&lt;/rn&gt; &lt;/pre&gt;&lt;/stop&gt; Here is an example of using ElementTree to read it and make changes to the structure: 1234567891011121314&gt;&gt;&gt; from xml.etree.ElementTree import parse, Element&gt;&gt;&gt; doc= parse('pred.xml')&gt;&gt;&gt; root= doc.getroot()&gt;&gt;&gt; root&lt;Element 'stop' at 0x054DEE10&gt;&gt;&gt;&gt; root.remove(root.find('sri'))&gt;&gt;&gt; root.remove(root.find('cr'))&gt;&gt;&gt; root.getchildren().index(root.find('nm'))1&gt;&gt;&gt; e= Element('spam')&gt;&gt;&gt; e.text='This is a test'&gt;&gt;&gt; root.insert(2, e)&gt;&gt;&gt; doc.write('newpred.xml', xml_declaration= True)&gt;&gt;&gt; The result of these operations is a new XML file that looks like this:123456789101112131415161718&lt;?xml version='1.0' encoding='us-ascii'?&gt;&lt;stop&gt; &lt;id&gt;14791&lt;/id&gt; &lt;nm&gt;Clark &amp;amp; Balmoral&lt;/nm&gt; &lt;spam&gt;This is a test&lt;/spam&gt;&lt;pre&gt; &lt;pt&gt;5 MIN&lt;/pt&gt; &lt;fd&gt;Howard&lt;/fd&gt; &lt;v&gt;1378&lt;/v&gt; &lt;rn&gt;22&lt;/rn&gt; &lt;/pre&gt; &lt;pre&gt; &lt;pt&gt;15 MIN&lt;/pt&gt; &lt;fd&gt;Howard&lt;/fd&gt; &lt;v&gt;1867&lt;/v&gt; &lt;rn&gt;22&lt;/rn&gt; &lt;/pre&gt;&lt;/stop&gt; Parsing XML Documents with NamespacesConsider a document that uses namespaces like this:123456789101112131415&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;top&gt; &lt;author&gt;David Beazley&lt;/author&gt; &lt;content&gt; &lt;html xmlns="http://www.w3.org/1999/xhtml"&gt; &lt;head&gt; &lt;title&gt;Hello World&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello World!&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; &lt;/content&gt;&lt;/top&gt; If you parse this document and try to perform the usual queries, you’ll find that it doesn’t work so easily because everything becomes incredibly verbose: 123456789101112131415&gt;&gt;&gt; # Some queries that work&gt;&gt;&gt; doc.findtext('author')'David Beazley'&gt;&gt;&gt; # A query involving a namespace (doesn't work)&gt;&gt;&gt; doc.find('content/html')&gt;&gt;&gt; # Works if fully qualified&gt;&gt;&gt; doc.find('content/&#123;http://www.w3.org/1999/xhtml&#125;html')&lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;html' at 0x1007767e0&gt;&gt;&gt;&gt; # Doesn't work&gt;&gt;&gt; doc.findtext('content/&#123;http://www.w3.org/1999/xhtml&#125;html/head/title')&gt;&gt;&gt; # Fully qualified&gt;&gt;&gt; doc.findtext('content/&#123;http://www.w3.org/1999/xhtml&#125;html/'... '&#123;http://www.w3.org/1999/xhtml&#125;head/&#123;http://www.w3.org/1999/xhtml&#125;title')'Hello World'&gt;&gt;&gt; Unfortunately, there is no mechanism in the basic ElementTree parser to get further information about namespaces. However, you can get a bit more information about the scope of namespace processing if you’re willing to use the iterparse() function instead. 1234567891011121314151617&gt;&gt;&gt; from xml.etree.ElementTree import iterparse&gt;&gt;&gt; for evt, elem in iterparse('ns2.xml', ('end', 'start-ns', 'end-ns')):... print(evt, elem)...end &lt;Element 'author' at 0x10110de10&gt;start-ns ('', 'http://www.w3.org/1999/xhtml')end &lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;title' at 0x1011131b0&gt;end &lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;head' at 0x1011130a8&gt;end &lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;h1' at 0x101113310&gt;end &lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;body' at 0x101113260&gt;end &lt;Element '&#123;http://www.w3.org/1999/xhtml&#125;html' at 0x10110df70&gt;end-ns Noneend &lt;Element 'content' at 0x10110de68&gt;end &lt;Element 'top' at 0x10110dd60&gt;&gt;&gt;&gt; elem # This is the topmost element&lt;Element 'top' at 0x10110dd60&gt;&gt;&gt;&gt; Interacting with a Relational DatabaseYou need to select, insert, or delete rows in a relational database. A standard way to represent rows of data in Python is as a sequence of tuples. 123456stocks = [ ('GOOG', 100, 490.1), ('AAPL', 50, 545.75), ('FB', 150, 7.45), ('HPQ', 75, 33.2),] The first step is to connect to the database. Typically, you execute a connect() function, supplying parameters such as the name of the database, hostname, username, password, and other details as needed.12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import sqlite3&gt;&gt;&gt; db= sqlite3.connect('database.db')&gt;&gt;&gt; c= db.cursor()&gt;&gt;&gt; c.execute('create table portfolio (symbol text, shares integer, price real)')&lt;sqlite3.Cursor object at 0x054C97A0&gt;&gt;&gt;&gt; db.commit()&gt;&gt;&gt; stocks = [... ('GOOG', 100, 490.1),... ('AAPL', 50, 545.75),... ('FB', 150, 7.45),... ('HPQ', 75, 33.2),... ]... &gt;&gt;&gt; c.executemany('insert into portfolio values (?,?,?)', stocks)&lt;sqlite3.Cursor object at 0x054C97A0&gt;&gt;&gt;&gt; db.commit()&gt;&gt;&gt; for row in db.execute('select * from portfolio'):... print(row)... ('GOOG', 100, 490.1)('AAPL', 50, 545.75)('FB', 150, 7.45)('HPQ', 75, 33.2)&gt;&gt;&gt; min_price= 100&gt;&gt;&gt; for row in db.execute('select * from portfolio where price &gt;= ?',(min_price,)):... print(row)... ('GOOG', 100, 490.1)('AAPL', 50, 545.75)&gt;&gt;&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_Files_and_I/O]]></title>
    <url>%2F2018%2F12%2F15%2FC05-Files-and-I-O%2F</url>
    <content type="text"><![CDATA[Reading and Writing Binary DataA lesser-known aspect of binary I/O is that objects such as arrays and C structures can be used for writing without any kind of intermediate conversion to a bytes object. For example:1234import arraynums = array.array('i', [1, 2, 3, 4])with open('data.bin','wb') as f: f.write(nums) This applies to any object that implements the so-called “buffer interface,” which directly exposes an underlying memory buffer to operations that can work with it. Writing binary data is one such operation. Many objects also allow binary data to be directly read into their underlying memory using the readinto() method of files. For example: 123456789&gt;&gt;&gt; import array&gt;&gt;&gt; a = array.array('i', [0, 0, 0, 0, 0, 0, 0, 0])&gt;&gt;&gt; with open('data.bin', 'rb') as f:... f.readinto(a)...16&gt;&gt;&gt; aarray('i', [1, 2, 3, 4, 0, 0, 0, 0])&gt;&gt;&gt; Writing to a File That Doesn’t Already ExistYou want to write data to a file, but only if it doesn’t already exist on the filesystem. This problem is easily solved by using the little-known x mode to open() instead of the usual w mode. 12345678910&gt;&gt;&gt; with open('somefile', 'wt') as f:... f.write('Hello\n')...&gt;&gt;&gt; with open('somefile', 'xt') as f:... f.write('Hello\n')...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;FileExistsError: [Errno 17] File exists: 'somefile'&gt;&gt;&gt; If the file is binary mode, use mode xb instead of xt. Performing I/O Operations on a StringUse the io.StringIO() and io.BytesIO() classes to create file-like objects that operate on string data. 123456789101112&gt;&gt;&gt; import io&gt;&gt;&gt; s= io.StringIO()&gt;&gt;&gt; s.write('Hello World\n')12&gt;&gt;&gt; print('This is a test', file= s)&gt;&gt;&gt; s.getvalue()'Hello World\nThis is a test\n'&gt;&gt;&gt; s= io.StringIO('Hello\nWorld\n')&gt;&gt;&gt; s.read(4)'Hell'&gt;&gt;&gt; s.read()'o\nWorld\n' The io.StringIO class should only be used for text. If you are operating with binary data, use the io.BytesIO class instead. 123456&gt;&gt;&gt; s= io.BytesIO()&gt;&gt;&gt; s.write(b'binary data')11&gt;&gt;&gt; s.getvalue()b'binary data'&gt;&gt;&gt; The StringIO and BytesIO classes are most useful in scenarios where you need to mimic a normal file for some reason. Be aware that StringIO and BytesIO instances don’t have a proper integer filedescriptor. Thus, they do not work with code that requires the use of a real system-level file such as a file, pipe, or socket. Reading and Writing Compressed DatafilesYou need to read or write data in a file with gzip or bz2 compression. The gzip and bz2 modules make it easy to work with such files. Both modules provide an alternative implementation of open() that can be used for this purpose. Both gzip.open() and bz2.open() accept the same parameters as the built-in open() function, including encoding, errors, newline, and so forth. 12345678# gzip compressionimport gzipwith gzip.open('somefile.gz', 'rt') as f: text = f.read()# bz2 compressionimport bz2with bz2.open('somefile.bz2', 'rt') as f: text = f.read() Similarly, to write compressed data, do this: 12345678# gzip compressionimport gzipwith gzip.open('somefile.gz', 'wt') as f: f.write(text)# bz2 compressionimport bz2with bz2.open('somefile.bz2', 'wt') as f: f.write(text) When writing compressed data, the compression level can be optionally specified using the compresslevel keyword argument. For example: 12with gzip.open('somefile.gz', 'wt', compresslevel=5) as f: f.write(text) Finally, a little-known feature of gzip.open() and bz2.open() is that they can be layered on top of an existing file opened in binary mode. 1234import gzipf = open('somefile.gz', 'rb')with gzip.open(f, 'rt') as g: text = g.read() This allows the gzip and bz2 modules to work with various file-like objects such as sockets, pipes, and in-memory files. Iterating Over Fixed-Sized RecordsInstead of iterating over a file by lines, you want to iterate over a collection of fixedsized records or chunks. Use the iter() function and functools.partial() using this neat trick:1234567from functools import partialRECORD_SIZE = 32with open('somefile.data', 'rb') as f: records = iter(partial(f.read, RECORD_SIZE), b'') for r in records: ... The records object in this example is an iterable that will produce fixed-sized chunks until the end of the file is reached. A little-known feature of the iter() function is that it can create an iterator if you pass it a callable and a sentinel value. The resulting iterator simply calls the supplied callable over and over again until it returns the sentinel, at which point iteration stops. In the solution, the functools.partial is used to create a callable that reads a fixed number of bytes from a file each time it’s called. The sentinel of b’’ is what gets returned when a file is read but the end of file has been reached. Reading Binary Data into a Mutable BufferYou want to read binary data directly into a mutable buffer without any intermediate copying. Perhaps you want to mutate the data in-place and write it back out to a file. To read data into a mutable array, use the readinto() method of files. 12345678910111213141516171819202122&gt;&gt;&gt; import os.path&gt;&gt;&gt; def read_into_buffer(filename):... buf= bytearray(os.path.getsize(filename))... with open(filename, 'rb') as f:... f.readinto(buf)... return buf... &gt;&gt;&gt; # Write a sample file&gt;&gt;&gt; with open('sample.bin', 'wb') as f:... f.write(b'Hello World')...&gt;&gt;&gt; buf = read_into_buffer('sample.bin')&gt;&gt;&gt; bufbytearray(b'Hello World')&gt;&gt;&gt; buf[0:5] = b'Hallo'&gt;&gt;&gt; bufbytearray(b'Hallo World')&gt;&gt;&gt; with open('newsample.bin', 'wb') as f:... f.write(buf)...11&gt;&gt;&gt; The readinto() method of files can be used to fill any preallocated array with data. This even includes arrays created from the array module or libraries such as numpy. Unlike the normal read() method, readinto() fills the contents of an existing buffer rather than allocating new objects and returning them. Getting a Directory ListingFor filename matching, you may want to use the glob or fnmatch modules instead. 12345import globpyfiles = glob.glob('somedir/*.py')from fnmatch import fnmatchpyfiles = [name for name in os.listdir('somedir') if fnmatch(name, '*.py')] Getting a directory listing is easy, but it only gives you the names of entries in the directory. If you want to get additional metadata, such as file sizes, modification dates, and so forth, you either need to use additional functions in the os.path module or use the os.stat() function.12345678910111213import osimport os.pathimport globpyfiles = glob.glob('*.py')# Get file sizes and modification datesname_sz_date = [(name, os.path.getsize(name), os.path.getmtime(name)) for name in pyfiles]for name, size, mtime in name_sz_date: print(name, size, mtime)# Alternative: Get file metadatafile_metadata = [(name, os.stat(name)) for name in pyfiles]for name, meta in file_metadata: print(name, meta.st_size, meta.st_mtime) Serializing Python ObjectsThe most common approach for serializing data is to use the pickle module. To dump an object to a file, you do this: 1234import pickledata = ... # Some Python objectf = open('somefile', 'wb')pickle.dump(data, f) To dump an object to a string, use pickle.dumps(): s = pickle.dumps(data) To re-create an object from a byte stream, use either the pickle.load() or pickle.loads() functions. For example: 12345# Restore from a filef = open('somefile', 'rb')data = pickle.load(f)# Restore from a stringdata = pickle.loads(s) pickle is a Python-specific self-describing data encoding. By self-describing, the serialized data contains information related to the start and end of each object as well as information about its type. 1234567891011121314&gt;&gt;&gt; import pickle&gt;&gt;&gt; f = open('somedata', 'wb')&gt;&gt;&gt; pickle.dump([1, 2, 3, 4], f)&gt;&gt;&gt; pickle.dump('hello', f)&gt;&gt;&gt; pickle.dump(&#123;'Apple', 'Pear', 'Banana'&#125;, f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('somedata', 'rb')&gt;&gt;&gt; pickle.load(f)[1, 2, 3, 4]&gt;&gt;&gt; pickle.load(f)'hello'&gt;&gt;&gt; pickle.load(f)&#123;'Apple', 'Pear', 'Banana'&#125;&gt;&gt;&gt; Certain kinds of objects can’t be pickled. These are typically objects that involve some sort of external system state, such as open files, open network connections, threads, processes, stack frames, and so forth. User-defined classes can sometimes work around these limitations by providing __getstate__() and __setstate__() methods. If defined, pickle.dump() will call __getstate__() to get an object that can be pickled. Similarly, __setstate__() will be invoked on unpickling. 1234567891011121314151617import timeimport threadingclass Countdown: def __init__(self, n): self.n = n self.thr = threading.Thread(target=self.run) self.thr.daemon = True self.thr.start() def run(self): while self.n &gt; 0: print('T-minus', self.n) self.n -= 1 time.sleep(5) def __getstate__(self): return self.n def __setstate__(self, n): self.__init__(n) Try the following experiment involving pickling: 1234567891011&gt;&gt;&gt; import countdown&gt;&gt;&gt; c = countdown.Countdown(30)&gt;&gt;&gt; T-minus 30T-minus 29T-minus 28...&gt;&gt;&gt; # After a few moments&gt;&gt;&gt; f = open('cstate.p', 'wb')&gt;&gt;&gt; import pickle&gt;&gt;&gt; pickle.dump(c, f)&gt;&gt;&gt; f.close() Now quit Python and try this after restart: 123456&gt;&gt;&gt; f = open('cstate.p', 'rb')&gt;&gt;&gt; pickle.load(f)countdown.Countdown object at 0x10069e2d0&gt;T-minus 19T-minus 18...]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_Iterators_and_Generators]]></title>
    <url>%2F2018%2F12%2F15%2FC04-Iterators-and-Generators%2F</url>
    <content type="text"><![CDATA[Delegating IterationYou have built a custom container object that internally holds a list, tuple, or some other iterable. You would like to make iteration work with your new container.Typically, all you need to do is define an __iter__() method that delegates iteration to the internally held container 123456789101112131415161718class Node: def __init__(self, value): self._value=value self._children=[] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children)if __name__=='__main__': root=Node(0) child1=Node(1) child2=Node(2) root.add_child(child1) root.add_child(child2) for ch in root: print(ch) Python’s iterator protocol requires __iter__() to return a special iterator object that implements a __next__() method to carry out the actual iteration.If all you are doing is iterating over the contents of another container, you don’t really need to worry about the underlying details of how it works. Implementing the Iterator ProtocolPerhaps you want to implement an iterator that traverses nodes in a depth-first pattern. Here is how you could do it: 123456789101112131415161718192021222324252627282930313233class Node: def __init__(self, value): self._value=value self._children=[] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children) def depth_first(self): yield self for c in self: yield from c.depth_first()if __name__=='__main__': root=Node(0) child1=Node(1) child2=Node(2) root.add_child(child1) root.add_child(child2) child1.add_child(Node(3)) child1.add_child(Node(4)) child2.add_child(Node(5)) for ch in root.depth_first(): print(ch)Node(0)Node(1)Node(3)Node(4)Node(2)Node(5) Python’s iterator protocol requires __iter__() to return a special iterator object that implements a __next__() operation and uses a StopIteration exception to signal completion. However, implementing such objects can often be a messy affair. For example, the following code shows an alternative implementation of the depth_first() method using an associated iterator class: 1234567891011121314151617181920212223242526272829303132333435363738class Node: def __init__(self, value): self._value=value self._children=[] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children) def depth_first(self): return DepthFirstIterator(self)class DepthFirstIterator: def __init__(self, start_node) : self._node= start_node self._children_iter= None self._child_iter= None def __iter__(self): return self def __next__(self): if self._children_iter is None: self._children_iter= iter(self._node) return self._node elif self._child_iter: try: nextchild= next(self._child_iter) return nextchild except StopIteration: self._child_iter = None return next(self) else: self._child_iter= next(self._children_iter).depth_first() return next(self) The DepthFirstIterator class works in the same way as the generator version, but it’s a mess because the iterator has to maintain a lot of complex state about where it is in the iteration process. Iterating in ReverseMany programmers don’t realize that reversed iteration can be customized on userdefined classes if they implement the __reversed__() method. 123456789101112131415&gt;&gt;&gt; class Countdown:... def __init__(self,start):... self.start= start... def __iter__(self):... n= self.start... while n &gt; 0:... yield n... n-= 1... def __reversed__(self):... n= 1... while n &lt;= self.start:... yield n... n += 1... &gt;&gt;&gt; Defining Generator Functions with Extra StateIf you want a generator to expose extra state to the user, don’t forget that you can easily implement it as a class, putting the generator function code in the __iter__() method.123456789101112131415161718192021from collections import dequeclass linehistory: def __init__(self, lines, histlen= 3): self.lines= lines self.history= deque(maxlen= histlen) def __iter__(self): for lineno, line in enumerate(self.lines, 1): self.history.append((lineno, line)) yield line def clear(self): self.history.clear()with open('file.txt') as f: lines= linehistory(f) for line in lines: if 'python' in line: for lineno, hline in lines.history: print('&#123;&#125;: &#123;&#125;'.format(lineno, hline), end='') Defining your generator in the__iter__()method doesn’t change anything about how you write your algorithm. The fact that it’s part of a class makes it easy for you to provide attributes and methods for users to interact with. One potential subtlety with the method shown is that it might require an extra step of calling iter() if you are going to drive iteration using a technique other than a for loop. 12345678910111213&gt;&gt;&gt; f = open('somefile.txt')&gt;&gt;&gt; lines = linehistory(f)&gt;&gt;&gt; next(lines)Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'linehistory' object is not an iterator&gt;&gt;&gt; # Call iter() first, then start iterating&gt;&gt;&gt; it = iter(lines)&gt;&gt;&gt; next(it)'hello world\n'&gt;&gt;&gt; next(it)'this is a test\n'&gt;&gt;&gt; Taking a Slice of an IteratorThe itertools.islice() function is perfectly suited for taking slices of iterators and generators. 12345678910&gt;&gt;&gt; c= count(0)&gt;&gt;&gt; c[10:20]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'generator' object is not subscriptable&gt;&gt;&gt; import itertools&gt;&gt;&gt; for x in itertools.islice(c, 10, 20):... print(x)... 10 111213141516171819 The result of islice() is an iterator that produces the desired slice items, but it does this by consuming and discarding all of the items up to the starting slice index. Further items are then produced by the islice object until the ending index has been reached. It’s important to emphasize that islice() will consume data on the supplied iterator. Since iterators can’t be rewound, that is something to consider. If it’s important to go back, you should probably just turn the data into a list first. Skipping the First Part of an IterableSuppose you are reading a file that starts with a series of comment lines. 12345678910111213141516&gt;&gt;&gt; with open('/etc/passwd') as f:... for line in f:... print(line, end='')...### User Database## Note that this file is consulted directly only when the system is running# in single-user mode. At other times, this information is provided by# Open Directory....##nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/falseroot:*:0:0:System Administrator:/var/root:/bin/sh...&gt;&gt;&gt; If you want to skip all of the initial comment lines, here’s one way to do it: 123456789&gt;&gt;&gt; from itertools import dropwhile&gt;&gt;&gt; with open('/etc/passwd') as f:... for line in dropwhile(lambda line: line.startswith('#'), f):... print(line, end='')...nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/falseroot:*:0:0:System Administrator:/var/root:/bin/sh...&gt;&gt;&gt; The dropwhile() and islice() functions are mainly convenience functions that you can use to avoid writing rather messy code such as this: 1234567891011with open('/etc/passwd') as f: # Skip over initial comments while True: line = next(f, '') if not line.startswith('#'): break # Process remaining lines while line: # Replace with useful processing print(line, end='') line = next(f, None) Discarding the first part of an iterable is also slightly different than simply filtering all of it. For example, the first part of this recipe might be rewritten as follows: Iterating Over All Possible Combinations or PermutationsYou want to iterate over all of the possible combinations or permutations of a collection of items. The itertools module provides three functions for this task. 1234567891011121314151617181920&gt;&gt;&gt; items = ['a', 'b', 'c']&gt;&gt;&gt; from itertools import permutations&gt;&gt;&gt; for p in permutations(items):... print(p)... ('a', 'b', 'c')('a', 'c', 'b')('b', 'a', 'c')('b', 'c', 'a')('c', 'a', 'b')('c', 'b', 'a')&gt;&gt;&gt; for p in permutations(items,2):... print(p)... ('a', 'b')('a', 'c')('b', 'a')('b', 'c')('c', 'a')('c', 'b') Use itertools.combinations() to produce a sequence of combinations of items taken from the input. 1234567891011&gt;&gt;&gt; from itertools import combinations&gt;&gt;&gt; for c in combinations(items, 3):... print(c)... ('a', 'b', 'c')&gt;&gt;&gt; for c in combinations(items, 2):... print(c)... ('a', 'b')('a', 'c')('b', 'c') For combinations(), the actual order of the elements is not considered. That is, the combination (‘a’, ‘b’) is considered to be the same as (‘b’, ‘a’) (which is not produced). When producing combinations, chosen items are removed from the collection of possible candidates (i.e., if ‘a’ has already been chosen, then it is removed from consideration). The itertools.combinations_with_replacement() function relaxes this, and allows the same item to be chosen more than once. 123456789101112131415&gt;&gt;&gt; from itertools import combinations_with_replacement&gt;&gt;&gt; for c in combinations_with_replacement(items,3):... print(c)... ('a', 'a', 'a')('a', 'a', 'b')('a', 'a', 'c')('a', 'b', 'b')('a', 'b', 'c')('a', 'c', 'c')('b', 'b', 'b')('b', 'b', 'c')('b', 'c', 'c')('c', 'c', 'c')&gt;&gt;&gt; Iterating Over Multiple Sequences SimultaneouslyTo iterate over more than one sequence simultaneously, use the zip() function. zip(a, b) works by creating an iterator that produces tuples (x, y) where x is taken from a and y is taken from b. The length of the iteration is the same as the length of the shortest input.12345678910&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = ['w', 'x', 'y', 'z']&gt;&gt;&gt; for i in zip(a,b):... print(i)...(1, 'w')(2, 'x')(3, 'y')&gt;&gt;&gt; If this behavior is not desired, use itertools.zip_longest() instead.12345678910111213141516&gt;&gt;&gt; from itertools import zip_longest&gt;&gt;&gt; for i in zip_longest(a,b):... print(i)...(1, 'w')(2, 'x')(3, 'y')(None, 'z')&gt;&gt;&gt; for i in zip_longest(a, b, fillvalue=0):... print(i)...(1, 'w')(2, 'x')(3, 'y')(0, 'z')&gt;&gt;&gt; It’s less common, but zip() can be passed more than two sequences as input. 12345678910&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = [10, 11, 12]&gt;&gt;&gt; c = ['x','y','z']&gt;&gt;&gt; for i in zip(a, b, c):... print(i)...(1, 10, 'x')(2, 11, 'y')(3, 12, 'z')&gt;&gt;&gt; Iterating on Items in Separate Containersitertools.chain() accepts one or more iterables as arguments. It then works by creating an iterator that successively consumes and returns the items produced by each of the supplied iterables you provided. It’s a subtle distinction, but chain() is more efficient than first combining the sequences and iterating. 123456# Inefficentfor x in a + b: ...# Betterfor x in chain(a, b): ... In the first case, the operation a + b creates an entirely new sequence and additionally requires a and b to be of the same type. chain() performs no such operation, so it’s far more efficient with memory if the input sequences are large and it can be easily applied when the iterables in question are of different types. 12345678910111213&gt;&gt;&gt; from itertools import chain&gt;&gt;&gt; a=[1,2,3,4]&gt;&gt;&gt; b=['x','y','z']&gt;&gt;&gt; for x in chain(a,b):... print(x)... 1234xyz Creating Data Processing PipelinesGenerator functions are a good way to implement processing pipelines. 12345678910111213141516171819202122232425262728293031323334353637383940import osimport fnmatchimport gzipimport bz2import redef gen_find(filepat, top): ''' Find all filenames in a directory tree that match a shell wildcard pattern ''' for path, dirlist, filelist in os.walk(top): for name in fnmatch.filter(filelist, filepat): yield os.path.join(path,name)def gen_opener(filenames): ''' Open a sequence of filenames one at a time producing a file object. The file is closed immediately when proceeding to the next iteration. ''' for filename in filenames: if filename.endswith('.gz'): f = gzip.open(filename, 'rt') elif filename.endswith('.bz2'): f = bz2.open(filename, 'rt') else: f = open(filename, 'rt') yield f f.close()def gen_concatenate(iterators): ''' Chain a sequence of iterators together into a single sequence. ''' for it in iterators: yield from itdef gen_grep(pattern, lines): ''' Look for a regex pattern in a sequence of lines ''' pat = re.compile(pattern) for line in lines: if pat.search(line): yield line You can now easily stack these functions together to make a processing pipeline. For example, to find all log lines that contain the word python. 123456lognames = gen_find('access-log*', 'www')files = gen_opener(lognames)lines = gen_concatenate(files)pylines = gen_grep('(?i)python', lines)for line in pylines: print(line) There is a bit of extreme subtlety involving the gen_concatenate() function. The purpose of this function is to concatenate input sequences together into one long sequence of lines. The itertools.chain() function performs a similar function, but requires that all of the chained iterables be specified as arguments. In the case of this particular recipe, doing that would involve a statement such as lines = itertools.chain(*files), which would cause the gen_opener() generator to be fully consumed. Since that generator is producing a sequence of open files that are immediately closed in the next iteration step, chain() can’t be used. The solution shown avoids this issue. Flattening a Nested SequenceYou have a nested sequence that you want to flatten into a single list of values. This is easily solved by writing a recursive generator function involving a yield from statement 123456789101112131415161718192021&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; def flatten(items, ignore_types=(str, bytes)):... for x in items:... if isinstance(x, Iterable) and not isinstance(x, ignore_types):... yield from flatten(x)... else:... yield x... &gt;&gt;&gt; items = [1, 2, [3, 4, [5, 6], 7], 8]&gt;&gt;&gt; for x in flatten(items):... print(x)... 12345678&gt;&gt;&gt; In the code, the isinstance(x, Iterable) simply checks to see if an item is iterable. If so, yield from is used to emit all of its values as a kind of subroutine. The extra argument ignore_types and the check for not isinstance(x, ignore_types) is there to prevent strings and bytes from being interpreted as iterables and expanded as individual characters. The yield from statement is a nice shortcut to use if you ever want to write generators that call other generators as subroutines. If you don’t use it, you need to write code that uses an extra for loop.12345678def flatten(items, ignore_types=(str, bytes)): for x in items: if isinstance(x, Iterable) and not isinstance(x, ignore_types): for i in flatten(x): yield i else: yield x Iterating in Sorted Order Over Merged Sorted IterablesYou have a collection of sorted sequences and you want to iterate over a sorted sequence of them all merged together. The heapq.merge() function does exactly what you want. 1234567891011121314151617&gt;&gt;&gt; import heapq&gt;&gt;&gt; a = [1, 4, 7, 10]&gt;&gt;&gt; b = [2, 5, 6, 11]&gt;&gt;&gt; for c in heapq.merge(a,b):... print(c)... 1245671011&gt;&gt;&gt; heapq.merge(a,b)&lt;generator object merge at 0x050BF810&gt;&gt;&gt;&gt; The iterative nature of heapq.merge means that it never reads any of the supplied sequences all at once. This means that you can use it on very long sequences with very little overhead. It’s important to emphasize that heapq.merge() requires that all of the input sequences already be sorted. In particular, it does not first read all of the data into a heap or do any preliminary sorting. Nor does it perform any kind of validation of the inputs to check if they meet the ordering requirements. Instead, it simply examines the set of items from the front of each input sequence and emits the smallest one found. A new item from the chosen sequence is then read, and the process repeats itself until all input sequences have been fully consumed. Replacing Infinite while Loops with an IteratorYou have code that uses a while loop to iteratively process data because it involves a function or some kind of unusual test condition that doesn’t fall into the usual iteration pattern. 12345678&gt;&gt;&gt; CHUNKSIZE= 8192&gt;&gt;&gt; def reader(s):... while True:... data= s.recv(CHUNKSIZE)... if data == b'':... break... process_data(data)... Such code can often be replaced using iter() 12345678910111213141516&gt;&gt;&gt; def reader(s):... for chunk in iter(lambda : s.recv(CHUNKSIZE), b''):... process_data(data)... &gt;&gt;&gt; &gt;&gt;&gt; import sys&gt;&gt;&gt; f = open('/etc/passwd')&gt;&gt;&gt; for chunk in iter(lambda: f.read(10), ''):... n = sys.stdout.write(chunk)...nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/falseroot:*:0:0:System Administrator:/var/root:/bin/shdaemon:*:1:1:System Services:/var/root:/usr/bin/false_uucp:*:4:4:Unix to Unix Copy Protocol:/var/spool/uucp:/usr/sbin/uucico...&gt;&gt;&gt; A little-known feature of the built-in iter() function is that it optionally accepts a zeroargument callable and sentinel (terminating) value as inputs. When used in this way, it creates an iterator that repeatedly calls the supplied callable over and over again until it returns the value given as a sentinel.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Strings_and_Text]]></title>
    <url>%2F2018%2F12%2F15%2FC02-Strings-and-Text%2F</url>
    <content type="text"><![CDATA[Splitting Strings on Any of Multiple DelimitersThe split() method of string objects is really meant for very simple cases, and does not allow for multiple delimiters or account for possible whitespace around the delimiters. In cases when you need a bit more flexibility, use the re.split() method:1234&gt;&gt;&gt; line = 'asdf fjdk; afed, fjek,asdf, foo'&gt;&gt;&gt; import re&gt;&gt;&gt; re.split(r'[;,\s]', line)['asdf', 'fjdk', '', 'afed', '', 'fjek', 'asdf', '', 'foo'] When using re.split(), you need to be a bit careful should the regular expression pattern involve a capture group enclosed in parentheses. If capture groups are used, then the matched text is also included in the result. 123&gt;&gt;&gt; fields= re.split(r'(;|,|\s)\s*', line)&gt;&gt;&gt; fields['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo'] If you don’t want the separator characters in the result, but still need to use parentheses to group parts of the regular expression pattern, make sure you use a noncapture group, specified as (?:…). 123&gt;&gt;&gt; re.split(r'(?:,|;|\s)\s*', line)['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']&gt;&gt;&gt; Matching Text at the Start or End of a StringA simple way to check the beginning or end of a string is to use the str.startswith() or str.endswith() methods.If you need to check against multiple choices, simply provide a tuple of possibilities to startswith() or endswith():12345678910&gt;&gt;&gt; import os&gt;&gt;&gt; filenames = os.listdir('.')&gt;&gt;&gt; filenames[ 'Makefile', 'foo.c', 'bar.py', 'spam.c', 'spam.h' ]&gt;&gt;&gt; [name for name in filenames if name.endswith(('.c', '.h')) ]['foo.c', 'spam.c', 'spam.h'&gt;&gt;&gt; any(name.endswith('.py') for name in filenames)True&gt;&gt;&gt; Oddly, this is one part of Python where a tuple is actually required as input. If you happen to have the choices specified in a list or set, just make sure you convert them using tuple() first. 123456789&gt;&gt;&gt; choices = ['http:', 'ftp:']&gt;&gt;&gt; url = 'http://www.python.org'&gt;&gt;&gt; url.startswith(choices)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: startswith first arg must be str or a tuple of str, not list&gt;&gt;&gt; url.startswith(tuple(choices))True&gt;&gt;&gt; Matching Strings Using Shell Wildcard PatternsThe matching performed by fnmatch sits somewhere between the functionality of simple string methods and the full power of regular expressions. If you’re just trying to provide a simple mechanism for allowing wildcards in data processing operations, it’s often a reasonable solution.If you’re actually trying to write code that matches filenames, use the glob module instead. 12345678&gt;&gt;&gt; from fnmatch import fnmatch, fnmatchcase&gt;&gt;&gt; fnmatch('foo.txt','*.txt')True&gt;&gt;&gt; fnmatch('Dat45.csv','Dat[0-9]*')True&gt;&gt;&gt; fnmatchcase('foo.txt','*.TXT')False&gt;&gt;&gt; If this distinction matters, use fnmatchcase() instead. It matches exactly based on the lower and uppercase conventions that you supply: 123&gt;&gt;&gt; fnmatchcase('foo.txt', '*.TXT')False&gt;&gt;&gt; Matching and Searching for Text PatternsIf you’re going to perform a lot of matches using the same pattern, it usually pays to precompile the regular expression pattern into a pattern object first.12345678&gt;&gt;&gt; datepat = re.compile(r'\d+/\d+/\d+')&gt;&gt;&gt; if datepat.match(text1):... print('yes')... else:... print('no')...yes match() always tries to find the match at the start of a string. If you want to search text for all occurrences of a pattern, use the findall() method instead. 123&gt;&gt;&gt; text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; datepat.findall(text)['11/27/2012', '3/13/2013'] Capture groups often simplify subsequent processing of the matched text because the contents of each group can be extracted individually. 123456789101112&gt;&gt;&gt; datepat = re.compile(r'(\d+)/(\d+)/(\d+)')&gt;&gt;&gt; m=datepat.match('11/27/2012')&gt;&gt;&gt; m.group(0)'11/27/2012'&gt;&gt;&gt; m.group(1)'11'&gt;&gt;&gt; m.group(2)'27'&gt;&gt;&gt; m.group(3)'2012'&gt;&gt;&gt; m.groups()('11', '27', '2012') 12345&gt;&gt;&gt; # Find all matches (notice splitting into tuples)&gt;&gt;&gt; text'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; datepat.findall(text)[('11', '27', '2012'), ('3', '13', '2013')] The findall() method searches the text and finds all matches, returning them as a list. If you want to find matches iteratively, use the finditer() method instead. 1234567891011&gt;&gt;&gt; for m in datepat.finditer(text):... print(m.group(0))... 11/27/20123/13/2013&gt;&gt;&gt; for m in datepat.finditer(text):... print(m.groups())... ('11', '27', '2012')('3', '13', '2013')&gt;&gt;&gt; Searching and Replacing TextFor more complicated patterns, use the sub() functions/methods in the re module. 12345&gt;&gt;&gt; text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; import re&gt;&gt;&gt; re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)'Today is 2012-11-27. PyCon starts 2013-3-13.'&gt;&gt;&gt; The first argument to sub() is the pattern to match and the second argument is the replacement pattern. Backslashed digits such as \3 refer to capture group numbers in the pattern. If you’re going to perform repeated substitutions of the same pattern, consider compiling it first for better performance. For example: 12345&gt;&gt;&gt; import re&gt;&gt;&gt; datepat = re.compile(r'(\d+)/(\d+)/(\d+)')&gt;&gt;&gt; datepat.sub(r'\3-\1-\2', text)'Today is 2012-11-27. PyCon starts 2013-3-13.'&gt;&gt;&gt; For more complicated substitutions, it’s possible to specify a substitution callback function instead. For example: 12345678&gt;&gt;&gt; from calendar import month_abbr&gt;&gt;&gt; def change_date(m):... mon_name = month_abbr[int(m.group(1))]... return '&#123;&#125; &#123;&#125; &#123;&#125;'.format(m.group(2), mon_name, m.group(3))...&gt;&gt;&gt; datepat.sub(change_date, text)'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.'&gt;&gt;&gt; As input, the argument to the substitution callback is a match object, as returned by match() or find(). Use the .group() method to extract specific parts of the match. The function should return the replacement text. If you want to know how many substitutions were made in addition to getting the replacement text, use re.subn() instead.123456&gt;&gt;&gt; newtext, n = datepat.subn(r'\3-\1-\2', text)&gt;&gt;&gt; newtext'Today is 2012-11-27. PyCon starts 2013-3-13.'&gt;&gt;&gt; n2 Searching and Replacing Case-Insensitive TextTo perform case-insensitive text operations, you need to use the re module and supply the re.IGNORECASE flag to various operations. 123456&gt;&gt;&gt; text = 'UPPER PYTHON, lower python, Mixed Python'&gt;&gt;&gt; re.findall('python', text, flags=re.IGNORECASE)['PYTHON', 'python', 'Python']&gt;&gt;&gt; re.sub('python', 'snake', text, flags=re.IGNORECASE)'UPPER snake, lower snake, Mixed snake'&gt;&gt;&gt; The last example reveals a limitation that replacing text won’t match the case of the matched text. If you need to fix this, you might have to use a support function. 123456789101112131415&gt;&gt;&gt; def matchcase(word):... def replace(m):... text= m.group()... if text.isupper():... return word.upper()... elif text.islower():... return word.lower()... elif text[0].isupper():... return word.capitalize()... else:... return word ... return replace... &gt;&gt;&gt; re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)'UPPER SNAKE, lower snake, Mixed Snake' Specifying a Regular Expression for the Shortest MatchYou’re trying to match a text pattern using regular expressions, but it is identifying the longest possible matches of a pattern. Instead, you would like to change it to find the shortest possible match.This problem often arises in patterns that try to match text enclosed inside a pair of starting and ending delimiters (e.g., a quoted string). 1234567&gt;&gt;&gt; str_pat= re.compile(r'"(.*)"')&gt;&gt;&gt; text1= 'Computer says "no."'&gt;&gt;&gt; str_pat.findall(text1)['no.']&gt;&gt;&gt; text2 = 'Computer says "no." Phone says "yes."'&gt;&gt;&gt; str_pat.findall(text2)['no." Phone says "yes.'] In this example, the pattern r’”(.*)”‘ is attempting to match text enclosed inside quotes. However, the * operator in a regular expression is greedy, so matching is based on finding the longest possible match. To fix this, add the ? modifier after the * operator in the pattern 123&gt;&gt;&gt; str_pat= re.compile(r'"(.*?)"')&gt;&gt;&gt; str_pat.findall(text2)['no.', 'yes.'] In a pattern, the dot matches any character except a newline. However, if you bracket the dot with starting and ending text (such as a quote), matching will try to find the longest possible match to the pattern.This causes multiple occurrences of the starting or ending text to be skipped altogether and included in the results of the longer match. Adding the ? right after operators such as * or + forces the matching algorithm to look for the shortest possible match instead. Sanitizing and Cleaning Up TextSome bored script kiddie has entered the text “pýtĥöñ” into a form on your web page and you’d like to clean it up somehow.s. To do so, you can turn to the often overlooked str.translate() method. 123456789101112&gt;&gt;&gt; s = 'pýtĥöñ\fis\tawesome\r\n'&gt;&gt;&gt; s'pýtĥöñ\x0cis\tawesome\r\n'&gt;&gt;&gt; remap = &#123;... ord('\t') : ' ',... ord('\f') : ' ',... ord('\r') : None # Deleted... &#125;&gt;&gt;&gt; a = s.translate(remap)&gt;&gt;&gt; a'pýtĥöñ is awesome\n'&gt;&gt;&gt; You can take this remapping idea a step further and build much bigger tables. For example, let’s remove all combining characters: 1234567891011&gt;&gt;&gt; import unicodedata&gt;&gt;&gt; import sys&gt;&gt;&gt; cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode)... if unicodedata.combining(chr(c)))...&gt;&gt;&gt; b = unicodedata.normalize('NFD', a)&gt;&gt;&gt; b'pýtĥöñ is awesome\n'&gt;&gt;&gt; b.translate(cmb_chrs)'python is awesome\n'&gt;&gt;&gt; A dictionary mapping every Unicode combining character to None is created using the dict.fromkeys(). Aligning Text StringsFor basic alignment of strings, the ljust(), rjust(), and center() methods of strings can be used. For example: 1234567891011&gt;&gt;&gt; text = 'Hello World'&gt;&gt;&gt; text.ljust(20)'Hello World '&gt;&gt;&gt; text.rjust(20)' Hello World'&gt;&gt;&gt; text.center(20)' Hello World '&gt;&gt;&gt; text.rjust(20,'=')'=========Hello World'&gt;&gt;&gt; text.center(20,'*')'****Hello World*****' The format() function can also be used to easily align things. All you need to do is use the &lt;, &gt;, or ^ characters along with a desired width. 12345678&gt;&gt;&gt; format(text, '&gt;20')' Hello World'&gt;&gt;&gt; format(text, '&lt;20')'Hello World '&gt;&gt;&gt; format(text, '^20')' Hello World '&gt;&gt;&gt; format(text, '=&gt;20s')'=========Hello World' One benefit of format() is that it is not specific to strings. 123456&gt;&gt;&gt; x = 1.2345&gt;&gt;&gt; format(x, '&gt;10')' 1.2345'&gt;&gt;&gt; format(x, '^10.2f')' 1.23 '&gt;&gt;&gt; In older code, you will also see the % operator used to format text. For example: 12345&gt;&gt;&gt; '%-20s' % text'Hello World '&gt;&gt;&gt; '%20s' % text' Hello World'&gt;&gt;&gt; Combining and Concatenating StringsIf you’re writing code that is building output from lots of small strings, you might consider writing that code as a generator function, using yield to emit fragments. 12345678910&gt;&gt;&gt; def sample():... yield 'Is'... yield 'Chicago'... yield 'Not'... yield 'Chicago'... &gt;&gt;&gt; text=''.join(sample())&gt;&gt;&gt; text'IsChicagoNotChicago'&gt;&gt;&gt; Or you could come up with some kind of hybrid scheme that’s smart about combining I/O operations:123456789101112131415&gt;&gt;&gt; def combine(source, maxsize):... parts=[]... size=0... for part in source:... parts.append(part)... size+=len(part)... if size &gt; maxsize:... yield ''.join(parts)... parts=[]... size=0... yield ''.join(parts)... &gt;&gt;&gt; for part in combine(sample(), 32768):... f.write(part) Handling HTML and XML Entities in TextIf you are producing text, replacing special characters such as &lt; or &gt; is relatively easy if you use the html.escape() function. 123456&gt;&gt;&gt; s = 'Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".'&gt;&gt;&gt; import html&gt;&gt;&gt; print(html.escape(s))Elements are written as &amp;quot;&amp;lt;tag&amp;gt;text&amp;lt;/tag&amp;gt;&amp;quot;.&gt;&gt;&gt; print(html.escape(s, quote=False)) # Disable escaping of quotesElements are written as "&amp;lt;tag&amp;gt;text&amp;lt;/tag&amp;gt;". If you’re trying to emit text as ASCII and want to embed character code entities for nonASCII characters, you can use the errors=’xmlcharrefreplace’ argument to various I/O-related functions to do it. 123&gt;&gt;&gt; s = 'Spicy Jalapeño'&gt;&gt;&gt; s.encode('ascii', errors='xmlcharrefreplace')b'Spicy Jalape&amp;#241;o' 12345678910111213&gt;&gt;&gt; s = 'Spicy &amp;quot;Jalape&amp;#241;o&amp;quot.'&gt;&gt;&gt; from html.parser import HTMLParser&gt;&gt;&gt; p=HTMLParser()&gt;&gt;&gt; p.unescape&lt;bound method HTMLParser.unescape of &lt;html.parser.HTMLParser object at 0x0536FB90&gt;&gt;&gt;&gt;&gt; p.unescape(s)'Spicy "Jalapeño".'&gt;&gt;&gt; t = 'The prompt is &amp;gt;&amp;gt;&amp;gt;'&gt;&gt;&gt; from xml.sax.saxutils import unescape&gt;&gt;&gt; unescape(t)'The prompt is &gt;&gt;&gt;'&gt;&gt;&gt; Tokenizing TextYou have a string that you want to parse left to right into a stream of tokens.To tokenize the string, you need to do more than merely match patterns. You need to have some way to identify the kind of pattern as well. For instance, you might want to turn the string into a sequence of pairs like this: 12tokens = [('NAME', 'foo'), ('EQ','='), ('NUM', '23'), ('PLUS','+'), ('NUM', '42'), ('TIMES', '*'), ('NUM', 10')] To do this kind of splitting, the first step is to define all of the possible tokens, including whitespace, by regular expression patterns using named capture groups such as this: 12345678import reNAME = r'(?P&lt;NAME&gt;[a-zA-Z_][a-zA-Z_0-9]*)'NUM = r'(?P&lt;NUM&gt;\d+)'PLUS = r'(?P&lt;PLUS&gt;\+)'TIMES = r'(?P&lt;TIMES&gt;\*)'EQ = r'(?P&lt;EQ&gt;=)'WS = r'(?P&lt;WS&gt;\s+)'master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS])) In these re patterns, the ?P convention is used to assign a name to the pattern. This will be used later. Next, to tokenize, use the little-known scanner() method of pattern objects. This method creates a scanner object in which repeated calls to match() step through the supplied text one match at a time. 1234567891011121314151617181920212223&gt;&gt;&gt; scanner = master_pat.scanner('foo = 42')&gt;&gt;&gt; scanner.match()&lt;_sre.SRE_Match object at 0x100677738&gt;&gt;&gt;&gt; _.lastgroup, _.group()('NAME', 'foo')&gt;&gt;&gt; scanner.match()&lt;_sre.SRE_Match object at 0x100677738&gt;&gt;&gt;&gt; _.lastgroup, _.group()('WS', ' ')&gt;&gt;&gt; scanner.match()&lt;_sre.SRE_Match object at 0x100677738&gt;&gt;&gt;&gt; _.lastgroup, _.group()('EQ', '=')&gt;&gt;&gt; scanner.match()&lt;_sre.SRE_Match object at 0x100677738&gt;&gt;&gt;&gt; _.lastgroup, _.group()('WS', ' ')&gt;&gt;&gt; scanner.match()&lt;_sre.SRE_Match object at 0x100677738&gt;&gt;&gt;&gt; _.lastgroup, _.group()('NUM', '42')&gt;&gt;&gt; scanner.match()&gt;&gt;&gt; To take this technique and put it into code, it can be cleaned up and easily packaged into a generator like this: 123456789101112131415from collections import namedtupleToken = namedtuple('Token', ['type','value'])def generate_tokens(pat, text): scanner = pat.scanner(text) for m in iter(scanner.match, None): yield Token(m.lastgroup, m.group())# Example usefor tok in generate_tokens(master_pat, 'foo = 42'): print(tok)# Produces output# Token(type='NAME', value='foo')# Token(type='WS', value=' ')# Token(type='EQ', value='=')# Token(type='WS', value=' ')# Token(type='NUM', value='42') If you want to filter the token stream in some way, you can either define more generator functions or use a generator expression. For example, here is how you might filter out all whitespace tokens. 1234tokens = (tok for tok in generate_tokens(master_pat, text) if tok.type != 'WS')for tok in tokens: print(tok)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_Data_Structures_and_Algorithms]]></title>
    <url>%2F2018%2F12%2F15%2FC01-Data-Structures-and-Algorithms%2F</url>
    <content type="text"><![CDATA[Unpacking a Sequence into Separate VariablesUnpacking actually works with any object that happens to be iterable, not just tuples or lists. This includes strings, files, iterators, and generators. For example:123456789101112131415&gt;&gt;&gt; p=(4,5)&gt;&gt;&gt; x,y=p&gt;&gt;&gt; x4&gt;&gt;&gt; y5&gt;&gt;&gt; &gt;&gt;&gt; s='Hello'&gt;&gt;&gt; a,b,c,d,e=s&gt;&gt;&gt; a'H'&gt;&gt;&gt; b'e' Unpacking Elements from Iterables of Arbitrary LengthYou need to unpack N elements from an iterable, but the iterable may be longer than N elements, causing a “too many values to unpack” exception. 12345&gt;&gt;&gt; record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')&gt;&gt;&gt; name,email,*phone= record&gt;&gt;&gt; phone['773-555-1212', '847-555-1212']&gt;&gt;&gt; It’s worth noting that the phone variable will always be a list, regardless of how many phone numbers are unpacked (including none). The starred variable can also be the first one in the list. 123456&gt;&gt;&gt; *trailing, current=[10,9,5,4,5]&gt;&gt;&gt; trailing[10, 9, 5, 4]&gt;&gt;&gt; current5&gt;&gt;&gt; Keeping the Last N ItemsUsing deque(maxlen=N) creates a fixed-sized queue. When new items are added and the queue is full, the oldest item is automatically removed. 123456789&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; def search(lines, pattern, history=5):... previous_lines=deque(maxlen=history)... for line in lines:... if pattern in line:... yield license, previous_lines... previous_lines.append(line)... &gt;&gt;&gt; More generally, a deque can be used whenever you need a simple queue structure. If you don’t give it a maximum size, you get an unbounded queue that lets you append and pop items on either end. 123456789&gt;&gt;&gt; q=deque()&gt;&gt;&gt; q.append(1)&gt;&gt;&gt; q.append(2)&gt;&gt;&gt; qdeque([1, 2])&gt;&gt;&gt; q.appendleft(3)&gt;&gt;&gt; qdeque([3, 1, 2])&gt;&gt;&gt; Finding the Largest or Smallest N ItemsYou want to make a list of the largest or smallest N items in a collection. The heapq module has two functions—nlargest() and nsmallest()—that do exactly what you want. 123456789101112131415161718192021&gt;&gt;&gt; import heapq&gt;&gt;&gt; nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]&gt;&gt;&gt; heapq.nlargest(3, nums)&gt;&gt;&gt; heapq.nlargest(3, nums)[42, 37, 23]&gt;&gt;&gt; heapq.nsmallest(3, nums)[-4, 1, 2]&gt;&gt;&gt; portfolio = [... &#123;'name': 'IBM', 'shares': 100, 'price': 91.1&#125;,... &#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;,... &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;,... &#123;'name': 'HPQ', 'shares': 35, 'price': 31.75&#125;,... &#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;,... &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;... ]... &gt;&gt;&gt; cheap=heapq.nsmallest(3, portfolio, key=lambda s:s['price'])&gt;&gt;&gt; cheap[&#123;'name': 'YHOO', 'price': 16.35, 'shares': 45&#125;, &#123;'name': 'FB', 'price': 21.09, 'shares': 200&#125;, &#123;'name': 'HPQ', 'price': 31.```75, 'shares': 35&#125;]&gt;&gt;&gt; If you are looking for the N smallest or largest items and N is small compared to the overall size of the collection, these functions provide superior performance. Underneath the covers, they work by first converting the data into a list where items are ordered as a heap. 123456&gt;&gt;&gt; nums[1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]&gt;&gt;&gt; heap=list(nums)&gt;&gt;&gt; heapq.heapify(heap)&gt;&gt;&gt; heap[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8] The most important feature of a heap is that heap[0] is always the smallest item. Moreover, subsequent items can be easily found using the heapq.heappop() method. 1234&gt;&gt;&gt; heapq.heappop(heap)-4&gt;&gt;&gt; heapq.heappop(heap)1 The nlargest() and nsmallest() functions are most appropriate if you are trying to find a relatively small number of items. If you are simply trying to find the single smallest or largest item (N=1), it is faster to use min() and max(). Similarly, if N is about the same size as the collection itself, it is usually faster to sort it first and take a slice (i.e., use sorted(items)[:N] or sorted(items)[-N:]). Mapping Keys to Multiple Values in a DictionaryYou want to make a dictionary that maps keys to more than one value. 12345678910111213141516&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; d= defaultdict(list)&gt;&gt;&gt; ddefaultdict(&lt;class 'list'&gt;, &#123;&#125;)&gt;&gt;&gt; d['a'].append(1)&gt;&gt;&gt; d['b'].append(2)&gt;&gt;&gt; ddefaultdict(&lt;class 'list'&gt;, &#123;'b': [2], 'a': [1]&#125;)&gt;&gt;&gt; d= defaultdict(set)&gt;&gt;&gt; d['a']set()&gt;&gt;&gt; d['a'].add(1)&gt;&gt;&gt; d['a'].add(2)&gt;&gt;&gt; d['b'].add(4)&gt;&gt;&gt; ddefaultdict(&lt;class 'set'&gt;, &#123;'b': &#123;4&#125;, 'a': &#123;1, 2&#125;&#125;) One caution with defaultdict is that it will automatically create dictionary entries for keys accessed later on (even if they aren’t currently found in the dictionary). If you don’t want this behavior, you might use setdefault() on an ordinary dictionary instead. For example: 1234567&gt;&gt;&gt; d=&#123;&#125;&gt;&gt;&gt; d.setdefault('a',[]).append(1)&gt;&gt;&gt; d&#123;'a': [1]&#125;&gt;&gt;&gt; d.setdefault('b',[]).append(2)&gt;&gt;&gt; d&#123;'b': [2], 'a': [1]&#125; In principle, constructing a multivalued dictionary is simple. However, initialization of the first value can be messy if you try to do it yourself. For example, you might have code that looks like this: 123456&gt;&gt;&gt; d=&#123;&#125;&gt;&gt;&gt; for key , value in pairs:... if key not in pairs:... d[key]=[]... d[key].append(value)... Using a defaultdict simply leads to much cleaner code: 1234&gt;&gt;&gt; d=defaultdict(list)&gt;&gt;&gt; for key, value in pairs:... d[key].append(value)... Naming a SliceAs a general rule, writing code with a lot of hardcoded index values leads to a readability and maintenance mess. In general, the built-in slice() creates a slice object that can be used anywhere a slice is allowed. For example: 1234567891011121314151617&gt;&gt;&gt; items = [0, 1, 2, 3, 4, 5, 6]&gt;&gt;&gt; a=slice(2,4)&gt;&gt;&gt; items[a][2, 3]&gt;&gt;&gt; items[a]=[10,11]&gt;&gt;&gt; items[0, 1, 10, 11, 4, 5, 6]&gt;&gt;&gt; del items[a]&gt;&gt;&gt; items[0, 1, 4, 5, 6]&gt;&gt;&gt; a=slice(10, 50, 2)&gt;&gt;&gt; a.start, a.stop, a.step(10, 50, 2)&gt;&gt;&gt; s='HelloWorld'&gt;&gt;&gt; a.indices(len(s))(10, 10, 2)&gt;&gt;&gt; Determining the Most Frequently Occurring Items in a SequenceCounter objects are a tremendously useful tool for almost any kind of problem where you need to tabulate and count data.12345678910111213&gt;&gt;&gt; words = [... 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes',... 'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the',... 'eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into',... 'my', 'eyes', "you're", 'under'... ]... &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; word_counts= Counter(words)&gt;&gt;&gt; top_three= word_counts.most_common(3)&gt;&gt;&gt; top_three[('eyes', 8), ('the', 5), ('look', 4)] Counter objects can be fed any sequence of hashable input items. Under the covers, a Counter is a dictionary that maps the items to the number of occurrences. For example: 12&gt;&gt;&gt; word_counts['eyes']8 If you want to increment the count manually, simply use addition: 123456&gt;&gt;&gt; morewords = ['why','are','you','not','looking','in','my','eyes']&gt;&gt;&gt; for w in morewords:... word_counts[w]+=1... &gt;&gt;&gt; word_counts['eyes']9 Or, alternatively, you could use the update() method: 123&gt;&gt;&gt; word_counts.update(morewords) &gt;&gt;&gt; word_counts['eyes']10 A little-known feature of Counter instances is that they can be easily combined using various mathematical operations. For example: 1234567891011&gt;&gt;&gt; a = Counter(words)&gt;&gt;&gt; b = Counter(morewords)&gt;&gt;&gt; aCounter(&#123;'eyes': 8, 'the': 5, 'look': 4, 'into': 3, 'my': 3, 'around': 2, "don't": 1, 'under': 1, "you're": 1, 'not': 1&#125;)&gt;&gt;&gt; bCounter(&#123;'are': 1, 'looking': 1, 'in': 1, 'you': 1, 'eyes': 1, 'not': 1, 'my': 1, 'why': 1&#125;)&gt;&gt;&gt; a+bCounter(&#123;'eyes': 9, 'the': 5, 'my': 4, 'look': 4, 'into': 3, 'around': 2, 'not': 2, 'are': 1, 'you': 1, "don't": 1, 'under': 1, "you're": 1, 'why': 1, 'looking': 1, 'in': 1&#125;)&gt;&gt;&gt; a-bCounter(&#123;'eyes': 7, 'the': 5, 'look': 4, 'into': 3, 'around': 2, 'my': 2, "you're": 1, "don't": 1, 'under': 1&#125;)&gt;&gt;&gt; Sorting a List of Dictionaries by a Common KeyYou have a list of dictionaries and you would like to sort the entries according to one or more of the dictionary values. Sorting this type of structure is easy using the operator module’s itemgetter function. 123456789101112&gt;&gt;&gt; rows = [... &#123;'fname': 'Brian', 'lname': 'Jones', 'uid': 1003&#125;,... &#123;'fname': 'David', 'lname': 'Beazley', 'uid': 1002&#125;,... &#123;'fname': 'John', 'lname': 'Cleese', 'uid': 1001&#125;,... &#123;'fname': 'Big', 'lname': 'Jones', 'uid': 1004&#125;... ]... &gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; rows_by_fname=sorted(rows, key= itemgetter('fname'))&gt;&gt;&gt; rows_by_uid= sorted(rows,key= itemgetter('uid'))&gt;&gt;&gt; rows_by_fname[&#123;'uid': 1004, 'fname': 'Big', 'lname': 'Jones'&#125;, &#123;'uid': 1003, 'fname': 'Brian', 'lname': 'Jones'&#125;, &#123;'uid': 1002, 'fname': 'David', 'lname': 'Beazley'&#125;, &#123;'uid': 1001, 'fname': 'John', 'lname': 'Cleese'&#125;] The itemgetter() function can also accept multiple keys. 1234&gt;&gt;&gt; a=itemgetter('lname', 'fname')&gt;&gt;&gt; a(rows[1])('Beazley', 'David')&gt;&gt;&gt; rows_by_lfname = sorted(rows, key=itemgetter('lname','fname')) The operator.itemgetter() function takes as arguments the lookup indices used to extract the desired values from the records in rows. It can be a dictionary key name, a numeric list element, or any value that can be fed to an object’s __getitem__() method. If you give multiple indices to itemgetter(), the callable it produces will return a tuple with all of the elements in it, and sorted() will order the output according to the sorted order of the tuples. 1234&gt;&gt;&gt; min(rows, key=itemgetter('uid'))&#123;'fname': 'John', 'lname': 'Cleese', 'uid': 1001&#125;&gt;&gt;&gt; max(rows, key=itemgetter('uid'))&#123;'fname': 'Big', 'lname': 'Jones', 'uid': 1004&#125; Sorting Objects Without Native Comparison SupportYou want to sort objects of the same class, but they don’t natively support comparison operations.The built-in sorted() function takes a key argument that can be passed a callable that will return some value in the object that sorted will use to compare the objects. For example, if you have a sequence of User instances in your application, and you want to sort them by their user_id attribute, you would supply a callable that takes a User instance as input and returns the user_id. 123456789&gt;&gt; class User:... def __init__(self, user_id):... self.user_id= user_id... def __repr__(self):... return 'User (&#123;&#125;)'.format(self.user_id)... &gt;&gt;&gt; users=[User(1), User(3), User(99)]&gt;&gt;&gt; sorted(users, key= lambda u:u.user_id)[User (1), User (3), User (99)] Instead of using lambda, an alternative approach is to use operator.attrgetter(): 12345&gt;&gt;&gt; from operator import attrgetter&gt;&gt;&gt; sorted(users, key= attrgetter('user_id'))[User (1), User (3), User (99)]&gt;&gt;&gt; min(users, key= attrgetter('user_id'))User (1) The choice of whether or not to use lambda or attrgetter() may be one of personal preference. However, attrgetter() is often a tad bit faster and also has the added feature of allowing multiple fields to be extracted simultaneously. This is analogous to the use of operator.itemgetter() for dictionaries. Grouping Records Together Based on a FieldYou have a sequence of dictionaries or instances and you want to iterate over the data in groups based on the value of a particular field, such as date. 12345678910&gt;&gt;&gt; rows = [... &#123;'address': '5412 N CLARK', 'date': '07/01/2012'&#125;,... &#123;'address': '5148 N CLARK', 'date': '07/04/2012'&#125;,... &#123;'address': '5800 E 58TH', 'date': '07/02/2012'&#125;,... &#123;'address': '2122 N CLARK', 'date': '07/03/2012'&#125;,... &#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'&#125;,... &#123;'address': '1060 W ADDISON', 'date': '07/02/2012'&#125;,... &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012'&#125;,... &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012'&#125;,... ] To do it, first sort by the desired field (in this case, date) and then use itertools.groupby(): 1234567891011121314151617181920&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; from itertools import groupby&gt;&gt;&gt; rows.sort(key=itemgetter('date'))&gt;&gt;&gt; for date, items in groupby(rows,key= itemgetter('date')):... print(date)... for i in items:... print(' ', i)... 07/01/2012 &#123;'address': '5412 N CLARK', 'date': '07/01/2012'&#125; &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012'&#125;07/02/2012 &#123;'address': '5800 E 58TH', 'date': '07/02/2012'&#125; &#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'&#125; &#123;'address': '1060 W ADDISON', 'date': '07/02/2012'&#125;07/03/2012 &#123;'address': '2122 N CLARK', 'date': '07/03/2012'&#125;07/04/2012 &#123;'address': '5148 N CLARK', 'date': '07/04/2012'&#125; &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012'&#125; The groupby() function works by scanning a sequence and finding sequential “runs” of identical values (or values returned by the given key function). On each iteration, it returns the value along with an iterator that produces all of the items in a group with the same value. An important preliminary step is sorting the data according to the field of interest. Since groupby() only examines consecutive items, failing to sort first won’t group the records as you want. If your goal is to simply group the data together by dates into a large data structure that allows random access, you may have better luck using defaultdict() to build a multidict.123456&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; rows_by_date= defaultdict(list)&gt;&gt;&gt; for row in rows:... rows_by_date[row[&apos;date&apos;]].append(row)... Filtering Sequence ElementsThe easiest way to filter sequence data is often to use a list comprehension. For example: 123456&gt;&gt;&gt; mylist = [1, 4, -5, 10, -7, 2, 3, -1]&gt;&gt;&gt; [n for n in mylist if n &gt; 0][1, 4, 10, 2, 3]&gt;&gt;&gt; [n for n in mylist if n &lt; 0][-5, -7, -1]&gt;&gt;&gt; Sometimes, the filtering criteria cannot be easily expressed in a list comprehension or generator expression. For example, suppose that the filtering process involves exception handling or some other complicated detail. For this, put the filtering code into its own function and use the built-in filter() function. filter() creates an iterator. For example: 12345678910&gt;&gt;&gt; def is_int(val):... try:... x=int(val)... return True... except ValueError:... return False... &gt;&gt;&gt; ivals= list(filter(is_int, values))&gt;&gt;&gt; ivals['1', '2', '-3', '4', '5'] Another notable filtering tool is itertools.compress(), which takes an iterable and an accompanying Boolean selector sequence as input. As output, it gives you all of the items in the iterable where the corresponding element in the selector is True. This can be useful if you’re trying to apply the results of filtering one sequence to another related sequence. For example, suppose you have the following two columns of data: 12345678910111213141516171819&gt;&gt;&gt; addresses = [... '5412 N CLARK',... '5148 N CLARK',... '5800 E 58TH',... '2122 N CLARK'... '5645 N RAVENSWOOD',... '1060 W ADDISON',... '4801 N BROADWAY',... '1039 W GRANVILLE',... ]... &gt;&gt;&gt; counts = [ 0, 3, 10, 4, 1, 7, 6, 1]&gt;&gt;&gt; from itertools import compress&gt;&gt;&gt; more5=[n &gt; 5 for n in counts]&gt;&gt;&gt; list(compress(addresses, more5))['5800 E 58TH', '4801 N BROADWAY', '1039 W GRANVILLE']&gt;&gt;&gt; more5[False, False, True, False, False, True, True, False]&gt;&gt;&gt; Mapping Names to Sequence ElementsYou have code that accesses list or tuple elements by position, but this makes the code somewhat difficult to read at times. You’d also like to be less dependent on position in the structure, by accessing the elements by name. collections.namedtuple() provides these benefits.12345678910111213&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Subcriber= namedtuple('Subcriber', ['addr','joined'])&gt;&gt;&gt; sub= Subcriber('a@b.com', '2012-09-23')&gt;&gt;&gt; subSubcriber(addr='a@b.com', joined='2012-09-23')&gt;&gt;&gt; sub.addr'a@b.com'&gt;&gt;&gt; len(sub)2&gt;&gt;&gt; addr, joined= sub&gt;&gt;&gt; addr, joined('a@b.com', '2012-09-23') A major use case for named tuples is decoupling your code from the position of the elements it manipulates. 1234567&gt;&gt;&gt; Stock = namedtuple('Stock', ['name', 'shares', 'price'])&gt;&gt;&gt; def compute_cost(records):... total=0.0... for rec in records:... s=Stock(*rec)... total+= s.shares * s.price... return total One possible use of a namedtuple is as a replacement for a dictionary, which requires more space to store. Thus, if you are building large data structures involving dictionaries, use of a namedtuple will be more efficient. However, be aware that unlike a dictionary, a namedtuple is immutable.123456789&gt;&gt;&gt; s = Stock('ACME', 100, 123.45)&gt;&gt;&gt; sStock(name='ACME', shares=100, price=123.45)&gt;&gt;&gt; s.shares = 75Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; If you need to change any of the attributes, it can be done using the _replace() method of a namedtuple instance. 123&gt;&gt;&gt; s=s._replace(shares= 75)&gt;&gt;&gt; sStock(name='ACME', shares=75, price=123.45) A subtle use of the _replace() method is that it can be a convenient way to populate named tuples that have optional or missing fields. To do this, you make a prototype tuple containing the default values and then use _replace() to create new instances with values replaced. 123456789101112&gt;&gt;&gt; stock= namedtuple('Stock', ['name', 'shares','price','date','time'])&gt;&gt;&gt; stock_prototype= stock('',0,0,None, None)&gt;&gt;&gt; def dict_to_stock(s):... return stock_prototype._replace(**s)... &gt;&gt;&gt; a = &#123;'name': 'ACME', 'shares': 100, 'price': 123.45&#125;&gt;&gt;&gt; dict_to_stock(a)Stock(name='ACME', shares=100, price=123.45, date=None, time=None)&gt;&gt;&gt; b = &#123;'name': 'ACME', 'shares': 100, 'price': 123.45, 'date': '12/17/2012'&#125;&gt;&gt;&gt; dict_to_stock(b)Stock(name='ACME', shares=100, price=123.45, date='12/17/2012', time=None)&gt;&gt;&gt; Combining Multiple Mappings into a Single MappingSuppose you have two dictionaries: 12a = &#123;'x': 1, 'z': 3 &#125;b = &#123;'y': 2, 'z': 4 &#125; Now suppose you want to perform lookups where you have to check both dictionaries (e.g., first checking in a and then in b if not found). An easy way to do this is to use the ChainMap class from the collections module. For example: 12345from collections import ChainMapc = ChainMap(a,b)print(c['x']) # Outputs 1 (from a)print(c['y']) # Outputs 2 (from b)print(c['z']) # Outputs 3 (from a) If there are duplicate keys, the values from the first mapping get used. Thus, the entry c[‘z’] in the example would always refer to the value in dictionary a, not the value in dictionary b. Operations that mutate the mapping always affect the first mapping listed. For example: 1234567891011121314151617&gt;&gt;&gt; len(c)3&gt;&gt;&gt; list(c.keys())['x', 'y', 'z']&gt;&gt;&gt; list(c.values())[1, 2, 3]&gt;&gt;&gt;&gt;&gt;&gt; c['z'] = 10&gt;&gt;&gt; c['w'] = 40&gt;&gt;&gt; del c['x']&gt;&gt;&gt; a&#123;'w': 40, 'z': 10&#125;&gt;&gt;&gt; del c['y']Traceback (most recent call last):...KeyError: "Key not found in the first mapping: 'y'"&gt;&gt;&gt; 123456789&gt;&gt;&gt; a = &#123;'x': 1, 'z': 3 &#125;&gt;&gt;&gt; b = &#123;'y': 2, 'z': 4 &#125;&gt;&gt;&gt; merged = ChainMap(a, b)&gt;&gt;&gt; merged['x']1&gt;&gt;&gt; a['x'] = 42&gt;&gt;&gt; merged['x'] # Notice change to merged dicts42&gt;&gt;&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_time_datetime]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-time-datetime%2F</url>
    <content type="text"><![CDATA[timetime模块中时间表现的格式主要有三种：timestamp时间戳，时间戳表示的是从1970年1月1日00:00:00开始按秒计算的偏移量struct_time时间元组，共有九个元素组。format time 格式化时间，已格式化的结构使时间更具可读性。包括自定义格式和固定格式。1234567891011121314151617181920212223242526272829# 生成timestamptime.time()1477471508.05time.localtime() #生成struct_timetime.localtime(time.time()) # timestamp to struct_time 本地时间time.struct_time(tm_year=2016, tm_mon=10, tm_mday=26, tm_hour=16, tm_min=45, tm_sec=8, tm_wday=2, tm_yday=300, tm_isdst=0)time.gmtime()time.gmtime(time.time()) # timestamp to struct_time 格林威治时间time.struct_time(tm_year=2016, tm_mon=10, tm_mday=26, tm_hour=8, tm_min=45, tm_sec=8, tm_wday=2, tm_yday=300, tm_isdst=0)#format_time to struct_timetime.strptime('2011-05-05 16:37:06', '%Y-%m-%d %X')time.struct_time(tm_year=2011, tm_mon=5, tm_mday=5, tm_hour=16, tm_min=37, tm_sec=6, tm_wday=3, tm_yday=125, tm_isdst=-1)time.strftime("%Y-%m-%d %X")time.strftime("%Y-%m-%d %X",time.localtime()) # struct_time to format_time2016-10-26 16:48:41#生成固定格式的时间表示格式time.asctime(time.localtime())time.ctime(time.time())# Wed Oct 26 16:45:08 2016 struct_time元组元素结构 属性 值 tm_year（年） 比如2011 tm_mon（月） 1 - 12 tm_mday（日） 1 - 31 tm_hour（时） 0 - 23 tm_min（分） 0 - 59 tm_sec（秒） 0 - 61 tm_wday（weekday） 0 - 6（0表示周日） tm_yday（一年中的第几天） 1 - 366 tm_isdst（是否是夏令时） 默认为-1 format time结构化表示 格式 含义 %a 本地（locale）简化星期名称 %A 本地完整星期名称 %b 本地简化月份名称 %B 本地完整月份名称 %c 本地相应的日期和时间表示 %d 一个月中的第几天（01 - 31） %H 一天中的第几个小时（24小时制，00 - 23） %I 第几个小时（12小时制，01 - 12） %j 一年中的第几天（001 - 366） %m 月份（01 - 12） %M 分钟数（00 - 59） %p 本地am或者pm的相应符 %S 秒（01 - 61） %U 一年中的星期数。（00 - 53星期天是一个星期的开始。）第一个星期天之前的所有天数都放在第0周。 %w 一个星期中的第几天（0 - 6，0是星期天） %W 和%U基本相同，不同的是%W以星期一为一个星期的开始。 %x 本地相应日期 %X 本地相应时间 %y 去掉世纪的年份（00 - 99） %Y 完整的年份 %Z 时区的名字（如果不存在为空字符） %% ‘%’字符 datetimedatatime模块重新封装了time模块，提供更多接口，提供的类有：date,time,datetime,timedelta,tzinfo。 date类datetime.date(year, month, day) 12345678910111213141516171819202122232425&gt;&gt;&gt; from datetime import *&gt;&gt;&gt; date.max #所能表示的最大、最小日期datetime.date(9999, 12, 31)&gt;&gt;&gt; date.today() #返回一个表示当前本地日期的date对象datetime.date(2018, 7, 1)&gt;&gt;&gt; d1=date(2012,3,24) #date对象&gt;&gt;&gt; d1.year, d1.month, d1.day(2012, 3, 24)&gt;&gt;&gt; d1.replace(year=2011, month=2,day=4) #生成一个新的日期对象，用参数指定的年，月，日代替原有对象中的属性。datetime.date(2011, 2, 4)&gt;&gt;&gt; d1.timetuple() #返回日期对应的time.struct_time对象time.struct_time(tm_year=2012, tm_mon=3, tm_mday=24, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=84, tm_isdst=-1)&gt;&gt;&gt; d1.weekday() #返回weekday，如果是星期一，返回0；以此类推5&gt;&gt;&gt; d1.isoweekday() #：返回weekday，如果是星期一，返回1；以此类推6&gt;&gt;&gt; d1.isocalendar() # 返回格式如(year，month，day)的元组；(2012, 12, 6)&gt;&gt;&gt; d1.isoformat() #返回格式如'YYYY-MM-DD’的字符串；'2012-03-24'&gt;&gt;&gt; d1.strftime('%Y/%m/%d') # 和time模块format相同。'2012/03/24' time 类datetime.time(hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ) datetime类datetime相当于date和time结合起来。datetime.datetime (year, month, day[ , hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ] ) 12345678910111213141516171819&gt;&gt;&gt; datetime.today()datetime.datetime(2018, 7, 1, 23, 54, 38, 345372)&gt;&gt;&gt; datetime.now()datetime.datetime(2018, 7, 1, 23, 55, 26, 225742)&gt;&gt;&gt; import time&gt;&gt;&gt; datetime.fromtimestamp(time.time())datetime.datetime(2018, 7, 1, 23, 56, 8, 748363)&gt;&gt;&gt; dt= datetime.now()&gt;&gt;&gt; dt.year,dt.month,dt.day,dt.hour,dt.second,dt.microsecond,dt.tzinfo(2018, 7, 1, 23, 29, 199639, None)&gt;&gt;&gt; dt.time() #获取time对象；datetime.time(23, 57, 29, 199639)&gt;&gt;&gt; dt.date() # 获取date对象；datetime.date(2018, 7, 1)&gt;&gt;&gt; dt.timetuple()time.struct_time(tm_year=2018, tm_mon=7, tm_mday=1, tm_hour=23, tm_min=57, tm_sec=29, tm_wday=6, tm_yday=182, tm_isdst=-1)&gt;&gt;&gt; timedelta类，时间加减1234567&gt;&gt;&gt; dt= datetime.now()&gt;&gt;&gt; dt1= dt+timedelta(days=1)&gt;&gt;&gt; dt,dt1(datetime.datetime(2018, 7, 2, 0, 0, 4, 878766), datetime.datetime(2018, 7, 3, 0, 0, 4, 878766))&gt;&gt;&gt; dt2=dt+timedelta(hours=-1)&gt;&gt;&gt; dt2datetime.datetime(2018, 7, 1, 23, 0, 4, 878766) tzinfo时区类tzinfo是关于时区信息的类tzinfo是一个抽象类，所以不能直接被实例化 1234567891011121314151617&gt;&gt;&gt; class UTC(tzinfo):... """UTC"""... def __init__(self,offset = 0):... self._offset = offset... ... def utcoffset(self, dt):... return timedelta(hours=self._offset)... ... def tzname(self, dt):... return "UTC +%s" % self._offset... ... def dst(self, dt):... return timedelta(hours=self._offset)... &gt;&gt;&gt; bangkok=datetime(2011,11,11,0,0,0,tzinfo=UTC(7))&gt;&gt;&gt; bangkokdatetime.datetime(2011, 11, 11, 0, 0, tzinfo=&lt;__main__.UTC object at 0x03254AB0&gt;)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>time</tag>
        <tag>datetime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_shlex_subprocess.Popen]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-shlex-subprocess-Popen%2F</url>
    <content type="text"><![CDATA[shlex词法分析是编译中的一个很重要的步骤,Python中提供了shlex模块用来做词法分析,用于分析shell中的输入语法, shlex基本只提供一个分词的功能和识别引号的功能 解析一串字符串,以,分割,但引号内的,不用于分割 1234567import shlexstrin=shlex.shlex("ab,'cdsfd,sfsd',ewewq,5654",posix=True)strin.whitespace=','strin.whitesapce_split=Trueb=list(strin)print(b)['ab', 'cdsfd,sfsd', 'ewewq', '5654'] split方法可以在用多种空白的时候解析 12345import shlexstrin2=shlex.split("ab 'cdsfd sfsd' ewewq 5654",posix=True)b=list(strin)print(b)['ab', 'cdsfd sfsd', 'ewewq', '5654'] shlex.quote(s) Return a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list. 1234&gt;&gt;&gt; filename = 'somefile; rm -rf ~'&gt;&gt;&gt; command = 'ls -l &#123;&#125;'.format(filename)&gt;&gt;&gt; print(command) # executed by a shell: boom!ls -l somefile; rm -rf ~ quote() lets you plug the security hole: 12345678&gt;&gt;&gt; from shlex import quote&gt;&gt;&gt; command = 'ls -l &#123;&#125;'.format(quote(filename))&gt;&gt;&gt; print(command)ls -l 'somefile; rm -rf ~'&gt;&gt;&gt; remote_command = 'ssh home &#123;&#125;'.format(quote(command))&gt;&gt;&gt; print(remote_command)ssh home 'ls -l '"'"'somefile; rm -rf ~'"'"'' The quoting is compatible with UNIX shells and with split():1234567&gt;&gt;&gt; from shlex import split&gt;&gt;&gt; remote_command = split(remote_command)&gt;&gt;&gt; remote_command['ssh', 'home', "ls -l 'somefile; rm -rf ~'"]&gt;&gt;&gt; command = split(remote_command[-1])&gt;&gt;&gt; command['ls', '-l', 'somefile; rm -rf ~'] subprocess.Popen创建并返回一个子进程，并在这个进程中执行指定的程序。实例化 Popen 可以通过许多参数详细定制子进程的环境，但是只有一个参数是必须的，即位置参数 args subprocess.Popen(args, bufsize=0, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=False, shell=False, cwd=None, env=None, universal_newlines=False, startupinfo=None, creationflags=0) 参数介绍 args：要执行的命令或可执行文件的路径。一个由字符串组成的序列（通常是列表），列表的第一个元素是可执行程序的路径，剩下的是传给这个程序的参数，如果没有要传给这个程序的参数，args 参数可以仅仅是一个字符串。 bufsize：控制 stdin, stdout, stderr 等参数指定的文件的缓冲，和打开文件的 open()函数中的参数 bufsize 含义相同。 executable：如果这个参数不是 None，将替代参数 args 作为可执行程序； stdin：指定子进程的标准输入； stdout：指定子进程的标准输出； stderr：指定子进程的标准错误输出； 对于 stdin, stdout 和 stderr 而言，如果他们是 None（默认情况），那么子进程使用和父进程相同的标准流文件。父进程如果想要和子进程通过 communicate() 方法通信，对应的参数必须是 subprocess.PIPE当然 stdin, stdout 和 stderr 也可以是已经打开的 file 对象，前提是以合理的方式打开，比如 stdin 对应的文件必须要可读等。 preexec_fn：默认是None，否则必须是一个函数或者可调用对象，在子进程中首先执行这个函数，然后再去执行为子进程指定的程序或Shell。 close_fds：布尔型变量，为 True 时，在子进程执行前强制关闭所有除 stdin，stdout和stderr外的文件； shell：布尔型变量，明确要求使用shell运行程序，与参数 executable 一同指定子进程运行在什么 Shell 中——如果executable=None 而 shell=True，则使用 /bin/sh 来执行 args 指定的程序；也就是说，Python首先起一个shell，再用这个shell来解释指定运行的命令。 cwd：代表路径的字符串，指定子进程运行的工作目录，要求这个目录必须存在； env：字典，键和值都是为子进程定义环境变量的字符串； universal_newline：布尔型变量，为 True 时，stdout 和 stderr 以通用换行（universal newline）模式打开， startupinfo：见下一个参数； creationfalgs：最后这两个参数是Windows中才有的参数，传递给Win32的CreateProcess API调用。 同 Linux 中创建子进程类似，父进程创建完子进程后，并不会自动等待子进程执行，父进程在子进程之前推出将导致子进程成为孤儿进程，孤儿进程统一由 init 进程接管，负责其终止后的回收工作。 如果父进程在子进程之后终止，但子进程终止时父进程没有进行最后的回收工作，子进程残留的数据结构称为僵尸进程。大量僵尸进程将耗费系统资源，因此父进程及时等待和回收子进程是必要的，除非能够确认自己比子进程先终止，从而将回收工作过渡给 init 进程。 这个等待和回收子进程的操作就是wait()函数 1234567891011&gt;&gt;&gt; import subprocess&gt;&gt;&gt; p = subprocess.Popen('ls -l', shell=True)&gt;&gt;&gt; total 164-rw-r--r-- 1 root root 133 Jul 4 16:25 admin-openrc.sh-rw-r--r-- 1 root root 268 Jul 10 15:55 admin-openrc-v3.sh...&gt;&gt;&gt; p.returncode&gt;&gt;&gt; p.wait()0&gt;&gt;&gt; p.returncode0 Popen 对象的属性p.pid 子进程的PID。 p.returncode 该属性表示子进程的返回状态，returncode可能有多重情况： **None** —— 子进程尚未结束； **==0** —— 子进程正常退出； **&gt; 0**—— 子进程异常退出，**returncode**对应于出错码； **&lt; 0**—— 子进程被信号杀掉了。 p.stdin, p.stdout, p.stderr 子进程对应的一些初始文件，如果调用Popen()的时候对应的参数是subprocess.PIPE，则这里对应的属性是一个包裹了这个管道的 file 对象， Popen 对象的方法p.poll() 检查子进程 p 是否已经终止，返回 p.returncode 属性;p.wait() 等待子进程 p 终止，返回 p.returncode 属性； 注意： wait() 立即阻塞父进程，直到子进程结束！ p.communicate(input=None) 和子进程 p 交流，将参数 input （字符串）中的数据发送到子进程的 stdin，同时从子进程的 stdout 和 stderr 读取数据，直到EOF。 返回值： 二元组 (stdoutdata, stderrdata) 分别表示从标准出和标准错误中读出的数据。 父进程调用 p.communicate() 和子进程通信有以下限制： （1） 只能通过管道和子进程通信，也就是说，只有调用 Popen() 创建子进程的时候参数 stdin=subprocess.PIPE，才能通过 p.communicate(input) 向子进程的 stdin 发送数据；只有参数 stdout 和 stderr 也都为 subprocess.PIPE ，才能通过p.communicate() 从子进程接收数据，否则接收到的二元组中，对应的位置是None。 （2）父进程从子进程读到的数据缓存在内存中，因此commucate()不适合与子进程交换过大的数据。 注意： communicate() 立即阻塞父进程，直到子进程结束！ _p.send_signal(signal)_ 向子进程发送信号 signal； p.terminate() 终止子进程 p ，等于向子进程发送 SIGTERM 信号； p.kill() 杀死子进程 **p** ，等于向子进程发送 **SIGKILL** 信号；]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>shelx</tag>
        <tag>subprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_lxml_03]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-lxml-03%2F</url>
    <content type="text"><![CDATA[Using XPath to find textAnother way to extract the text content of a tree is XPath, which also allows you to extract the separate text chunks into a list: 12345678&gt;&gt;&gt; print(html.xpath("string()")) # lxml.etree only!TEXTTAIL&gt;&gt;&gt; print(html.xpath("//text()")) # lxml.etree only!['TEXT', 'TAIL']# If you want to use this more often, you can wrap it in a function:&gt;&gt;&gt; build_text_list = etree.XPath("//text()") # lxml.etree only!&gt;&gt;&gt; print(build_text_list(html))['TEXT', 'TAIL'] Note that a string result returned by XPath is a special ‘smart’ object that knows about its origins. You can ask it where it came from through its getparent() method, just as you would with Elements: 1234567891011121314151617&gt;&gt;&gt; texts = build_text_list(html)&gt;&gt;&gt; print(texts[0])TEXT&gt;&gt;&gt; parent = texts[0].getparent()&gt;&gt;&gt; print(parent.tag)body&gt;&gt;&gt; print(texts[1])TAIL&gt;&gt;&gt; print(texts[1].getparent().tag)brYou can also find out if it's normal text content or tail text:&gt;&gt;&gt; print(texts[0].is_text)True&gt;&gt;&gt; print(texts[1].is_text)False&gt;&gt;&gt; print(texts[1].is_tail)True While this works for the results of the text() function, lxml will not tell you the origin of a string value that was constructed by the XPath functions string() or concat(): 12345&gt;&gt;&gt; stringify = etree.XPath("string()")&gt;&gt;&gt; print(stringify(html))TEXTTAIL&gt;&gt;&gt; print(stringify(html).getparent())None]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_lxml_02]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-lxml-02%2F</url>
    <content type="text"><![CDATA[The Element classAn Element is the main container object for the ElementTree API. Most of the XML tree functionality is accessed through this class. Elements are easily created through the Element factory.123456789&gt;&gt;&gt; root= etree.Element('root')&gt;&gt;&gt; root.tag'root'&gt;&gt;&gt; root.append(etree.Element('child1'))&gt;&gt;&gt; child2= etree.SubElement(root, 'child2')&gt;&gt;&gt; child3= etree.SubElement(root, 'child3')&gt;&gt;&gt; etree.tostring(root, pretty_print= True)b'&lt;root&gt;\n &lt;child1/&gt;\n &lt;child2/&gt;\n &lt;child3/&gt;\n&lt;/root&gt;\n' Elements are listsTo make the access to these subelements easy and straight forward, elements mimic the behaviour of normal Python lists as closely as possible.12345678910111213141516171819202122232425&gt;&gt;&gt; child= root[0]&gt;&gt;&gt; child.tag'child1'&gt;&gt;&gt; root[1]&lt;Element child2 at 0x3b6bb20&gt;&gt;&gt;&gt; for child in root: print(child.tag)... child1child2child3&gt;&gt;&gt; root.insert(0, etree.Element('child0'))&gt;&gt;&gt; root[:1][&lt;Element child0 at 0x3b70d28&gt;]&gt;&gt;&gt; root[-1:][&lt;Element child3 at 0x3b6ba80&gt;]&gt;&gt;&gt; len(root)4&gt;&gt;&gt; for child in root:... print(child.tag)... child0child1child2child3 12345678&gt;&gt;&gt; root[0]= root[3]&gt;&gt;&gt; for child in root:print (child.tag)... child3child1child2 In above example, the last element is moved to a different position, instead of being copied, i.e. it is automatically removed from its previous position when it is put in a different place. In lists, objects can appear in multiple positions at the same time, and the above assignment would just copy the item reference into the first position, so that both contain the exact same item. 1234567&gt;&gt;&gt; from copy import deepcopy&gt;&gt;&gt; element= etree.Element('neu')&gt;&gt;&gt; element.append(deepcopy(root[1]))&gt;&gt;&gt; element[0].tag'child1'&gt;&gt;&gt; [c.tag for c in root]['child0', 'child1', 'child2', 'child3'] If you want to copy an element to a different position in lxml.etree, consider creating an independent deep copy using the copy module from Python’s standard library 1234567&gt;&gt;&gt; root is root[0].getparent() # lxml.etree only!True&gt;&gt;&gt; root[0] is root[1].getprevious()True&gt;&gt;&gt; root[1] is root[0].getnext()True&gt;&gt;&gt; The siblings (or neighbours) of an element are accessed as next and previous elements. Elements carry attributes as a dictXML elements support attributes. You can create them directly in the Element factory:1234&gt;&gt;&gt; root = etree.Element("root", interesting="totally")&gt;&gt;&gt; etree.tostring(root)b'&lt;root interesting="totally"/&gt;' Attributes are just unordered name-value pairs, so a very convenient way of dealing with them is through the dictionary-like interface of Elements: 123456789101112131415&gt;&gt;&gt; print(root.get("interesting"))totally&gt;&gt;&gt; print(root.get("hello"))None&gt;&gt;&gt; root.set("hello", "Huhu")&gt;&gt;&gt; print(root.get("hello"))Huhu&gt;&gt;&gt; etree.tostring(root)b'&lt;root interesting="totally" hello="Huhu"/&gt;'&gt;&gt;&gt; sorted(root.keys())['hello', 'interesting']&gt;&gt;&gt; for name, value in sorted(root.items()):... print('%s = %r' % (name, value))hello = 'Huhu'interesting = 'totally' For the cases where you want to do item lookup or have other reasons for getting a ‘real’ dictionary-like object, e.g. for passing it around, you can use the attrib property: 12345678910&gt;&gt;&gt; attributes = root.attrib&gt;&gt;&gt; print(attributes["interesting"])totally&gt;&gt;&gt; print(attributes.get("no-such-attribute"))None&gt;&gt;&gt; attributes["hello"] = "Guten Tag"&gt;&gt;&gt; print(attributes["hello"])Guten Tag&gt;&gt;&gt; print(root.get("hello"))Guten Tag Note that attrib is a dict-like object backed by the Element itself. This means that any changes to the Element are reflected in attrib and vice versa. It also means that the XML tree stays alive in memory as long as the attrib of one of its Elements is in use. To get an independent snapshot of the attributes that does not depend on the XML tree, copy it into a dict: 123&gt;&gt;&gt; d = dict(root.attrib)&gt;&gt;&gt; sorted(d.items())[('hello', 'Guten Tag'), ('interesting', 'totally')] Elements contain textElements can contain text: 123456&gt;&gt;&gt; root = etree.Element("root")&gt;&gt;&gt; root.text = "TEXT"&gt;&gt;&gt; print(root.text)TEXT&gt;&gt;&gt; etree.tostring(root)b'&lt;root&gt;TEXT&lt;/root&gt;' In many XML documents (data-centric documents), this is the only place where text can be found. It is encapsulated by a leaf tag at the very bottom of the tree hierarchy.However, if XML is used for tagged text documents such as (X)HTML, text can also appear between different elements, right in the middle of the tree: &lt;html&gt;&lt;body&gt;Hello&lt;br/&gt;World&lt;/body&gt;&lt;/html&gt; Here, the tag is surrounded by text. This is often referred to as document-style or mixed-content XML. Elements support this through their tail property. It contains the text that directly follows the element, up to the next element in the XML tree: 1234567891011121314&gt;&gt;&gt; html = etree.Element("html")&gt;&gt;&gt; body = etree.SubElement(html, "body")&gt;&gt;&gt; body.text = "TEXT"&gt;&gt;&gt; etree.tostring(html)b'&lt;html&gt;&lt;body&gt;TEXT&lt;/body&gt;&lt;/html&gt;'&gt;&gt;&gt; br = etree.SubElement(body, "br")&gt;&gt;&gt; etree.tostring(html)b'&lt;html&gt;&lt;body&gt;TEXT&lt;br/&gt;&lt;/body&gt;&lt;/html&gt;'&gt;&gt;&gt; br.tail = "TAIL"&gt;&gt;&gt; etree.tostring(html)b'&lt;html&gt;&lt;body&gt;TEXT&lt;br/&gt;TAIL&lt;/body&gt;&lt;/html&gt;' The two properties .text and .tail are enough to represent any text content in an XML document. This way, the ElementTree API does not require any special text nodes in addition to the Element class, that tend to get in the way fairly often (as you might know from classic DOM APIs).However, there are cases where the tail text also gets in the way. For example, when you serialise an Element from within the tree, you do not always want its tail text in the result (although you would still want the tail text of its children). For this purpose, the tostring() function accepts the keyword argument with_tail: 1234&gt;&gt;&gt; etree.tostring(br)b'&lt;br/&gt;TAIL'&gt;&gt;&gt; etree.tostring(br, with_tail=False) # lxml.etree only!b'&lt;br/&gt;' If you want to read only the text, i.e. without any intermediate tags, you have to recursively concatenate all text and tail attributes in the correct order. Again, the tostring() function comes to the rescue, this time using the method keyword:123&gt;&gt;&gt; etree.tostring(html, method="text")b'TEXTTAIL']]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_lxml_01]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-lxml-01%2F</url>
    <content type="text"><![CDATA[etree.parseetree.HTML 文本获取12345678910111213141516171819202122232425262728from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = etree.tostring(html)print(result.decode('utf-8'))# 輸出：&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 经过处理之后 li 节点标签被补全，并且还自动添加了 body、html 节点12345result = html.xpath('//li[@class="item-0"]/text()')print(result)# 运行结果如下：['\n '] XPath 中 text() 前面是 /，而此 / 的含义是选取直接子节点，而此处很明显 li 的直接子节点都是 a 节点，文本都是在 a 节点内部的，所以这里匹配到的结果就是被修正的 li 节点内部的换行符，因为自动修正的li节点的尾标签换行了。即选中的是这两个节点： 123&lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; 其中一个节点因为自动修正，li 节点的尾标签添加的时候换行了，所以提取文本得到的唯一结果就是 li 节点的尾标签和 a 节点的尾标签之间的换行符。因此，如果我们想获取 li 节点内部的文本就有两种方式，一种是选取到 a 节点再获取文本，另一种就是使用 //，我们来看下二者的区别是什么。首先我们选取到 a 节点再获取文本，代码如下： 1234567from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]/a/text()')print(result)# 运行结果：['first item', 'fifth item'] 用另一种方式 // 选取的结果，代码如下：12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]//text()')print(result)# 运行结果：['first item', 'fifth item', '\n '] 属性获取我们知道了用 text() 可以获取节点内部文本，那么节点属性该怎样获取呢？其实还是用 @ 符号就可以，例如我们想获取所有 li 节点下所有 a 节点的 href 属性，代码如下：12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li/a/@href')print(result)# 运行结果：['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html'] 通过 @href 即可获取节点的 href 属性，注意此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如 [@href=”link1.html”]，而此处的 @href 指的是获取节点的某个属性，二者需要做好区分。 属性多值匹配123456789from lxml import etreetext = '''&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li")]/a/text()')print(result)# 运行结果：['first item'] 这样我们通过 contains() 方法，第一个参数传入属性名称，第二个参数传入属性值，这样只要此属性包含所传入的属性值就可以完成匹配了。 多属性匹配另外我们可能还遇到一种情况，我们可能需要根据多个属性才能确定一个节点，这是就需要同时匹配多个属性才可以，那么这里可以使用运算符 and 来连接，示例如下：123456789from lxml import etreetext = '''&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li") and @name="item"]/a/text()')print(result)['first item'] 在这里 HTML 文本的 li 节点又增加了一个属性 name，这时候我们需要同时根据 class 和 name 属性来选择，就可以 and 运算符连接两个条件，两个条件都被中括号包围， 按序选择123456789101112131415161718192021&gt;&gt;&gt; text = '''... &lt;div&gt;... &lt;ul&gt;... &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;... &lt;/ul&gt;... &lt;/div&gt;... '''... &gt;&gt;&gt; html=etree.HTML(text)&gt;&gt;&gt; html.xpath('//li[1]/a/text()')['first item']&gt;&gt;&gt; html.xpath('//li[last()]/a/text()')['fifth item']&gt;&gt;&gt; html.xpath('//li[position()&lt;3]/a/text()')['first item', 'second item']&gt;&gt;&gt; html.xpath('//li[last()-2]/a/text()')['third item'] 节点轴选择XPath 提供了很多节点轴选择方法，英文叫做 XPath Axes，包括获取子元素、兄弟元素、父元素、祖先元素等等，在一定情况下使用它可以方便地完成节点的选择。 123456789101112131415161718192021222324252627282930313233343536373839404142434445text = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[1]/ancestor::*')print(result)result = html.xpath('//li[1]/ancestor::div')print(result)result = html.xpath('//li[1]/attribute::*')print(result)result = html.xpath('//li[1]/child::a[@href="link1.html"]')print(result)result = html.xpath('//li[1]/descendant::span')print(result)result = html.xpath('//li[1]/following::*[2]')print(result)result = html.xpath('//li[1]/following-sibling::*')print(result)# 运行结果：[&lt;Element html at 0x107941808&gt;, &lt;Element body at 0x1079418c8&gt;, &lt;Element div at 0x107941908&gt;, &lt;Element ul at 0x107941948&gt;][&lt;Element div at 0x107941908&gt;]['item-0'][&lt;Element a at 0x1079418c8&gt;][&lt;Element span at 0x107941948&gt;][&lt;Element a at 0x1079418c8&gt;][&lt;Element li at 0x107941948&gt;, &lt;Element li at 0x107941988&gt;, &lt;Element li at 0x1079419c8&gt;, &lt;Element li at 0x10794&gt;&gt;&gt; html.xpath('//ul/child::[@class="item-0"]')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "src\lxml\etree.pyx", line 1589, in lxml.etree._Element.xpath (src\lxml\etree.c:61209) File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__ (src\lxml\etree.c:178748) File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result (src\lxml\etree.c:177700)lxml.etree.XPathEvalError: Invalid expression&gt;&gt;&gt; html.xpath('//ul/child::li[@class="item-0"]')[&lt;Element li at 0x3b6bb20&gt;, &lt;Element li at 0x3b6bc88&gt;] 第一次选择我们调用了 ancestor 轴，可以获取所有祖先节点，其后需要跟两个冒号，然后是节点的选择器，这里我们直接使用了 ，表示匹配所有节点，因此返回结果是第一个 li 节点的所有祖先节点，包括 html，body，div，ul。第二次选择我们又加了限定条件，这次在冒号后面加了 div，这样得到的结果就只有 div 这个祖先节点了。第三次选择我们调用了 attribute 轴，可以获取所有属性值，其后跟的选择器还是 ，这代表获取节点的所有属性，返回值就是 li 节点的所有属性值。第四次选择我们调用了 child 轴，可以获取所有直接子节点，在这里我们又加了限定条件选取 href 属性为 link1.html 的 a 节点。第五次选择我们调用了 descendant 轴，可以获取所有子孙节点，这里我们又加了限定条件获取 span 节点，所以返回的就是只包含 span 节点而没有 a 节点。第六次选择我们调用了 following 轴，可以获取当前节点之后的所有节点，这里我们虽然使用的是 匹配，但又加了索引选择，所以只获取了第二个后续节点。第七次选择我们调用了 following-sibling 轴，可以获取当前节点之后的所有同级节点，这里我们使用的是 匹配，所以获取了所有后续同级节点。]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_unitteste]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-unittest%2F</url>
    <content type="text"><![CDATA[unittest库提供了test cases, test suites, test fixtures，test runner： test case ：通过继承TestCase类，我们可以创建一个test，或者一组tests，包括测试前准备环境的搭建(setUp)，执行测试代码(run)，以及测试后环境的还原(tearDown)。 test suites ： 测试套件,多个测试用例集合在一起,TestSuite也可以嵌套TestSuite。 test fixtures ： setup + test case + teardown结构 TestLoader:用来加载TestCase到TestSuite中，其中的方法从各个地方寻找TestCase，创建它们的实例，然后add到TestSuite中，返回一个TestSuite实例。 test runner：执行测试用例，其中的run()会执行TestSuite/TestCase。 TextTestResult：测试的结果会保存到TextTestResult实例中，包括运行用例数，成功数，失败数等 写好TestCase，然后由TestLoader加载TestCase到TestSuite，然后由TextTestRunner来运行TestSuite，运行的结果保存在TextTestResult中，整个过程集成在unittest.main模块中。所有的测试函数以test开头，test_XXX。 123456789101112131415161718import unittestclass TestStringMethods(unittest.TestCase): def test_upper(self): self.assertEqual('foo'.upper(),'FOO') def test_split(self): s='hello world' self.assertEqual(s.split(),['hello', 'world']) #with self.assertEqualRaise(TypeError): s.split(2)if __name__=='__main__': unittest.main()----------------------------------------------------------------------Ran 2 tests in 0.001sOK 命令行1234python -m unittest test_module1 test_module2 #同时测试多个modulepython -m unittest test_module.TestClasspython -m unittest test_module.TestClass.test_method TestCaseTestCase的实例是最小的可测试单元。 testcase 是由unittest的TestCase类的实例表示的。要编写自己的测试用例必须继承TestCase类,或者使用FunctionTestCase。且Testcase类提供了各种assert的方法来检测预期结果和实际结果。 12345678910111213import unittestclass DefaultWidgetSizeTestCase(unittest.TestCase): #unittest.TestCase表示某个测试函数 def runTest(self): widget = Widget('The widget') self.assertEqual(widget.size(), (50, 50), 'incorrect default size')# 建立这样一个测试用例的一个实例,使用该类的构造函数，且不带参数(这样会执行所有的测试方法)testCase = DefaultWidgetSizeTestCase()#建了两个**WidgetTestCase**的实例，每个实例只运行**WidgetTestCase**类中的一个测试方法(通过参数传入)defaultSizeTestCase = WidgetTestCase('test_default_size')resizeTestCase = WidgetTestCase('test_resize') Test fixtures方法固定装置： 如果要对一个模块中的每一个测试函数都做同样的初始化操作和结尾清除等操作，那么创建n个测试用例就得写n遍一样的代码，为了减少重复的代码，可以使用下面两个函数： setUp(): 每次执行测试用例之前调用。无参数，无返回值。该方法抛出的异常都视为error，而不是测试不通过。没有默认的实现。 tearDown(): 每次执行测试用例之后调用。无参数，无返回值。测试方法抛出异常，该方法也正常调用，该方法抛出的异常都视为error，而不是测试不通过。只用setUp()调用成功，该方法才会被调用。没有默认的实现。通过setup 和 tearDown组装一个module成为一个固定的测试装置。 Class固定装置： 必须为类实现setUpClass():一个类方法在单个类测试之前运行。setUpClass作为唯一的参数被调用时,必须使用classmethod()作为装饰器。tearDownClass():一个类方法在单个类测试之后运行。setUpClass作为唯一的参数被调用时,必须使用classmethod()作为装饰器。123456789import unittestclass Test(unittest.TestCase): @classmethod def setUpClass(cls): #这里的cls是当前类的对象 cls._connection = createExpensiveConnectionObject() @classmethod def tearDownClass(cls): cls._connection.destroy() Module固定装置： 必须为方法实现 1234def setUpModule(): createConnection()def tearDownModule(): closeConnection() 特殊方法: setup():每个测试函数运行前运行 teardown():每个测试函数运行完后执行 setUpClass():必须使用@classmethod 装饰器,所有test运行前运行一次 tearDownClass():必须使用@classmethod装饰器,所有test运行完后运行一次 setUpModule() 和 tearDownModule()：在整个文件级别上只调用一次]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>unittest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logging_module_three_ways_to_configure]]></title>
    <url>%2F2018%2F12%2F15%2Flogging-module-three-ways-to-configure%2F</url>
    <content type="text"><![CDATA[Configure logging in three ways: Creating loggers, handlers, and formatters explicitly using Python code that calls the configuration methods listed above. Creating a logging config file and reading it using the fileConfig() function. Creating a dictionary of configuration information and passing it to the dictConfig() function. 1 12345678910111213141516171819202122232425import logging# create loggerlogger = logging.getLogger('simple_example')logger.setLevel(logging.DEBUG)# create console handler and set level to debugch = logging.StreamHandler()ch.setLevel(logging.DEBUG)# create formatterformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')# add formatter to chch.setFormatter(formatter)# add ch to loggerlogger.addHandler(ch)# 'application' codelogger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') Running this module from the command line produces the following output: 123456$ python simple_logging_module.py2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message2005-03-19 15:10:26,620 - simple_example - INFO - info message2005-03-19 15:10:26,695 - simple_example - WARNING - warn message2005-03-19 15:10:26,697 - simple_example - ERROR - error message2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message 2 fileConfig123456789101112131415import loggingimport logging.configlogging.config.fileConfig('logging.conf')# create loggerlogger = logging.getLogger('simpleExample')# 'application' codelogger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') Here is the logging.conf file:1234567891011121314151617181920212223242526272829[loggers]keys=root,simpleExample[handlers]keys=consoleHandler[formatters]keys=simpleFormatter[logger_root]level=DEBUGhandlers=consoleHandler[logger_simpleExample]level=DEBUGhandlers=consoleHandlerqualname=simpleExamplepropagate=0[handler_consoleHandler]class=StreamHandlerlevel=DEBUGformatter=simpleFormatterargs=(sys.stdout,)[formatter_simpleFormatter]format=%(asctime)s - %(name)s - %(levelname)s - %(message)sdatefmt= The output is nearly identical to that of the non-config-file-based example: The fileConfig() function takes a default parameter, disable_existing_loggers, which defaults to True for reasons of backward compatibility. This may or may not be what you want, since it will cause any loggers existing before the fileConfig() call to be disabled unless they (or an ancestor) are explicitly named in the configuration. Please refer to the reference documentation for more information, and specify False for this parameter if you wish. The dictionary passed to dictConfig() can also specify a Boolean value with key disable_existing_loggers, which if not specified explicitly in the dictionary also defaults to being interpreted as True. This leads to the logger-disabling behaviour described above, which may not be what you want - in which case, provide the key explicitly with a value of False. using dictionaries to hold configuration informationIn Python 3.2, a new means of configuring logging has been introduced, using dictionaries to hold configuration information. This provides a superset of the functionality of the config-file-based approach outlined above, and is the recommended configuration method for new applications and deployments. Because a Python dictionary is used to hold configuration information, and since you can populate that dictionary using different means, you have more options for configuration. For example, you can use a configuration file in JSON format, or, if you have access to YAML processing functionality, a file in YAML format, to populate the configuration dictionary. Or, of course, you can construct the dictionary in Python code, receive it in pickled form over a socket, or use whatever approach makes sense for your application.Here’s an example of the same configuration as above, in YAML format for the new dictionary-based approach: 123456789101112131415161718version: 1formatters: simple: format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple stream: ext://sys.stdoutloggers: simpleExample: level: DEBUG handlers: [console] propagate: noroot: level: DEBUG handlers: [console]]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_logging_module_advanced]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-logging-module-advanced%2F</url>
    <content type="text"><![CDATA[The logging library takes a modular approach and offers several categories of components: loggers, handlers, filters, and formatters. Loggers expose the interface that application code directly uses. Handlers send the log records (created by loggers) to the appropriate destination. Filters provide a finer grained facility for determining which log records to output. Formatters specify the layout of log records in the final output. Logging is performed by calling methods on instances of the Logger class (hereafter called loggers). Each instance has a name, and they are conceptually arranged in a namespace hierarchy using dots (periods) as separators. For example, a logger named ‘scan’ is the parent of loggers ‘scan.text’, ‘scan.html’ and ‘scan.pdf’. Logger names can be anything you want, and indicate the area of an application in which a logged message originates. A good convention to use when naming loggers is to use a module-level logger, in each module which uses logging, named as follows: logger = logging.getLogger(__name__) This means that logger names track the package/module hierarchy, and it’s intuitively obvious where events are logged just from the logger name. It is, of course, possible to log messages to different destinations. Support is included in the package for writing log messages to files, HTTP GET/POST locations, email via SMTP, generic sockets, queues, or OS-specific logging mechanisms such as syslog or the Windows NT event log. Destinations are served by handler classes. You can create your own log destination class if you have special requirements not met by any of the built-in handler classes. By default, no destination is set for any logging messages. You can specify a destination (such as console or file) by using basicConfig() as in the tutorial examples. If you call the functions debug(), info(), warning(), error() and critical(), they will check to see if no destination is set; and if one is not set, they will set a destination of the console (sys.stderr) and a default format for the displayed message before delegating to the root logger to do the actual message output. The default format set by basicConfig() for messages is: severity:logger name:message loggersLogger objects have a threefold job. First, they expose several methods to application code so that applications can log messages at runtime. Second, logger objects determine which log messages to act upon based upon severity (the default filtering facility) or filter objects. Third, logger objects pass along relevant log messages to all interested log handlers. The most widely used methods on logger objects fall into two categories: configuration and message sending. These are the most common configuration methods: Logger.setLevel() specifies the lowest-severity log message a logger will handle, where debug is the lowest built-in severity level and critical is the highest built-in severity. For example, if the severity level is INFO, the logger will handle only INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG messages.Logger.addHandler() and Logger.removeHandler() add and remove handler objects from the logger object.Logger.addFilter() and Logger.removeFilter() add and remove filter objects from the logger object. You don’t need to always call these methods on every logger you create. With the logger object configured, the following methods create log messages: Logger.debug(), Logger.info(), Logger.warning(), Logger.error(), and Logger.critical() all create log records with a message and a level that corresponds to their respective method names. The message is actually a format string, which may contain the standard string substitution syntax of %s, %d, %f, and so on. The rest of their arguments is a list of objects that correspond with the substitution fields in the message. With regard to kwargs, the logging methods care only about a keyword of exc_info** and use it to determine whether to log exception information. Logger.exception() creates a log message similar to Logger.error(). The difference is that Logger.exception() dumps a stack trace along with it. Call this method only from an exception handler. Logger.log() takes a log level as an explicit argument. This is a little more verbose for logging messages than using the log level convenience methods listed above, but this is how to log at custom log levels. getLogger() returns a reference to a logger instance with the specified name if it is provided, or root if not. The names are period-separated hierarchical structures. Multiple calls to getLogger() with the same name will return a reference to the same logger object. Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. Loggers have a concept of effective level. If a level is not explicitly set on a logger, the level of its parent is used instead as its effective level. If the parent has no explicit level set, its parent is examined, and so on - all ancestors are searched until an explicitly set level is found. The root logger always has an explicit level set (WARNING by default). When deciding whether to process an event, the effective level of the logger is used to determine whether the event is passed to the logger’s handlers. Child loggers propagate messages up to the handlers associated with their ancestor loggers. Because of this, it is unnecessary to define and configure handlers for all the loggers an application uses. It is sufficient to configure handlers for a top-level logger and create child loggers as needed. (You can, however, turn off propagation by setting the propagate attribute of a logger to False.) HandlersHandler objects are responsible for dispatching the appropriate log messages (based on the log messages’ severity) to the handler’s specified destination. Logger objects can add zero or more handler objects to themselves with an addHandler() method. As an example scenario, an application may want to send all log messages to a log file, all log messages of error or higher to stdout, and all messages of critical to an email address. This scenario requires three individual handlers where each handler is responsible for sending messages of a specific severity to a specific location. The standard library includes quite a few handler types (see Useful Handlers); the tutorials use mainly StreamHandler and FileHandler in its examples. There are very few methods in a handler for application developers to concern themselves with. The only handler methods that seem relevant for application developers who are using the built-in handler objects (that is, not creating custom handlers) are the following configuration methods: The setLevel() method, just as in logger objects, specifies the lowest severity that will be dispatched to the appropriate destination. Why are there two setLevel() methods? The level set in the logger determines which severity of messages it will pass to its handlers. The level set in each handler determines which messages that handler will send on. setFormatter() selects a Formatter object for this handler to use. addFilter() and removeFilter() respectively configure and deconfigure filter objects on handlers. Application code should not directly instantiate and use instances of Handler. Instead, the Handler class is a base class that defines the interface that all handlers should have and establishes some default behavior that child classes can use (or override). FormattersFormatter objects configure the final order, structure, and contents of the log message. Unlike the base logging.Handler class, application code may instantiate formatter classes, although you could likely subclass the formatter if your application needs special behavior. The constructor takes three optional arguments – a message format string, a date format string and a style indicator. logging.Formatter.__init__(fmt=None, datefmt=None, style=&#39;%&#39;) If there is no message format string, the default is to use the raw message. If there is no date format string, the default date format is: %Y-%m-%d %H:%M:%S with the milliseconds tacked on at the end. The style is one of %, ‘{‘ or ‘$’. If one of these is not specified, then ‘%’ will be used. If the style is ‘%’, the message format string uses %()s styled string substitution; the possible keys are documented in LogRecord attributes. If the style is ‘{‘, the message format string is assumed to be compatible with str.format() (using keyword arguments), while if the style is ‘$’ then the message format string should conform to what is expected by string.Template.substitute().]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_logging_module_simple]]></title>
    <url>%2F2018%2F12%2F15%2Fpython-logging-module-simple%2F</url>
    <content type="text"><![CDATA[1234&gt;&gt;&gt; import logging&gt;&gt;&gt; logging.warn('warm')WARNING:root:warm日志级别：Logger实例名称: 日志消息内容 logging.basicConfig(filename=’logger.log’, level= logging.INFO)logging.debug(‘debug message’) 发现当前工作目录下生成了logger.log,因为通过level=logging.INFO设置日志级别为INFO，所以所有的日志信息均输出出来了。 几个重要的概念 Logger 记录器，暴露了应用程序代码能直接使用的接口。 Handler 处理器，将（记录器产生的）日志记录发送至合适的目的地。 Filter 过滤器，提供了更好的粒度控制，它可以决定输出哪些日志记录。 Formatter 格式化器，指明了最终输出中日志记录的布局。 ###logging模块使用过程 第一次导入logging模块或使用reload函数重新导入logging模块，logging模块中的代码将被执行，这个过程中将产生logging日志系统的默认配置。 自定义配置(可选)。logging标准模块支持三种配置方式: dictConfig，fileConfig，listen。其中，dictConfig是通过一个字典进行配置Logger，Handler，Filter，Formatter；fileConfig则是通过一个文件进行配置；而listen则监听一个网络端口，通过接收网络数据来进行配置。当然，除了以上集体化配置外，也可以直接调用Logger，Handler等对象中的方法在代码中来显式配置。 使用logging模块的全局作用域中的getLogger函数来得到一个Logger对象实例(其参数即是一个字符串，表示Logger对象实例的名字，即通过该名字来得到相应的Logger对象实例)。 使用Logger对象中的debug，info，error，warn，critical等方法记录日志信息。 logging to a file123456 import logging logging.basicConfig(filename='example.log', level= logging.DEBUG) # because we set the threshold to DEBUG, all of the messages were printed.logging.debug('This message should go to the log file')logging.info('So should this')logging.warning('And this, too') If you run the above script several times, the messages from successive runs are appended to the file example.log.If you want each run to start afresh, not remembering the messages from earlier runs, you can specify the filemode argument, by changing the call in the above example to:logging.basicConfig(filename=&#39;example.log&#39;, filemode=&#39;w&#39;, level=logging.DEBUG) Logging from multiple modulesmyapp.py 1234567891011import loggingimport mylibdef main(): logging.basicConfig(filename='myapp.log', level=logging.INFO) logging.info('Started') mylib.do_something() logging.info('Finished')if __name__ == '__main__': main() mylib.py 1234import loggingdef do_something(): logging.info('Doing something') If you run myapp.py, you should see this in myapp.log: INFO:root:StartedINFO:root:Doing somethingINFO:root:Finished Changing the format of displayed messagesTo change the format which is used to display messages, you need to specify the format you want to use: 1234import logginglogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)logging.debug('This message should appear on the console')logging.info('So should this') which would print: DEBUG:This message should appear on the console Notice that the ‘root’ which appeared in earlier examples has disappeared.]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests_HTTP_for_humans_Advanced]]></title>
    <url>%2F2018%2F12%2F15%2Frequests-HTTP-for-humans-Advanced%2F</url>
    <content type="text"><![CDATA[Session ObjectsThe Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance, and will use urllib3’s connection pooling. So if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase (see HTTP persistent connection). A Session object has all the methods of the main Requests API. Let’s persist some cookies across requests: s = requests.Session()s.get(‘http://httpbin.org/cookies/set/sessioncookie/123456789&#39;)r = s.get(‘http://httpbin.org/cookies&#39;)print(r.text) ‘{“cookies”: {“sessioncookie”: “123456789”}}’Sessions can also be used to provide default data to the request methods. This is done by providing data to the properties on a Session object: s = requests.Session()s.auth = (‘user’, ‘pass’)s.headers.update({‘x-test’: ‘true’}) both ‘x-test’ and ‘x-test2’ are sents.get(‘http://httpbin.org/headers&#39;, headers={‘x-test2’: ‘true’}) Any dictionaries that you pass to a request method will be merged with the session-level values that are set. The method-level parameters override session parameters. Note, however, that method-level parameters will not be persisted across requests, even if using a session. This example will only send the cookies with the first request, but not the second: s = requests.Session()r = s.get(‘http://httpbin.org/cookies&#39;, cookies={‘from-my’: ‘browser’})print(r.text) ‘{“cookies”: {“from-my”: “browser”}}’r = s.get(‘http://httpbin.org/cookies&#39;)print(r.text) ‘{“cookies”: {}}’If you want to manually add cookies to your session, use the Cookie utility functions to manipulate Session.cookies. Sessions can also be used as context managers: with requests.Session() as s: s.get(‘http://httpbin.org/cookies/set/sessioncookie/123456789&#39;) This will make sure the session is closed as soon as the with block is exited, even if unhandled exceptions occurred. Remove a Value From a Dict ParameterSometimes you’ll want to omit session-level keys from a dict parameter. To do this, you simply set that key’s value to None in the method-level parameter. It will automatically be omitted.All values that are contained within a session are directly available to you.]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests_HTTP_for_humans_Authentication]]></title>
    <url>%2F2018%2F12%2F15%2Frequests-HTTP-for-humans-Authentication%2F</url>
    <content type="text"><![CDATA[Basic AuthenticationMany web services that require authentication accept HTTP Basic Auth. This is the simplest kind, and Requests supports it straight out of the box. Making requests with HTTP Basic Auth is very simple: 123&gt;&gt;&gt; from requests.auth import HTTPBasicAuth&gt;&gt;&gt; requests.get('https://api.github.com/user', auth=HTTPBasicAuth('user', 'pass'))&lt;Response [200]&gt; In fact, HTTP Basic Auth is so common that Requests provides a handy shorthand for using it: 12&gt;&gt;&gt; requests.get('https://api.github.com/user', auth=('user', 'pass'))&lt;Response [200]&gt; Providing the credentials in a tuple like this is exactly the same as the HTTPBasicAuth example above. netrc AuthenticationIf no authentication method is given with the auth argument, Requests will attempt to get the authentication credentials for the URL’s hostname from the user’s netrc file. The netrc file overrides raw HTTP authentication headers set with headers=. If credentials for the hostname are found, the request is sent with HTTP Basic Auth. Digest AuthenticationAnother very popular form of HTTP Authentication is Digest Authentication, and Requests supports this out of the box as well: 1234&gt;&gt;&gt; from requests.auth import HTTPDigestAuth&gt;&gt;&gt; url = 'http://httpbin.org/digest-auth/auth/user/pass'&gt;&gt;&gt; requests.get(url, auth=HTTPDigestAuth('user', 'pass'))&lt;Response [200]&gt; OAuth 1 AuthenticationA common form of authentication for several web APIs is OAuth. The requests-oauthlib library allows Requests users to easily make OAuth 1 authenticated requests: 1234567&gt;&gt;&gt; import requests&gt;&gt;&gt; from requests_oauthlib import OAuth1&gt;&gt;&gt; url = 'https://api.twitter.com/1.1/account/verify_credentials.json'&gt;&gt;&gt; auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET',... 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')&gt;&gt;&gt; requests.get(url, auth=auth)&lt;Response [200]&gt;]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests_HTTP_for_humans_01]]></title>
    <url>%2F2018%2F12%2F15%2Frequests-HTTP-for-humans-01%2F</url>
    <content type="text"><![CDATA[Make a Request12&gt;&gt;&gt; r= requests.get('https://api.github.com/events')&gt;&gt;&gt; r=requests.post('http://httpbin.org/post', data=&#123;'key':'value'&#125;) Now, we have a Response object called r. We can get all the information we need from this object.What about the other HTTP request types: PUT, DELETE, HEAD and OPTIONS?1234&gt;&gt;&gt; r = requests.put('http://httpbin.org/put', data = &#123;'key':'value'&#125;)&gt;&gt;&gt; r = requests.delete('http://httpbin.org/delete')&gt;&gt;&gt; r = requests.head('http://httpbin.org/get')&gt;&gt;&gt; r = requests.options('http://httpbin.org/get') Passing Parameters In URLsYou often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. httpbin.org/get?key=val. Requests allows you to provide these arguments as a dictionary of strings, using the params keyword argument. 1234&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': ['value2', 'value3']&#125;&gt;&gt;&gt; r = requests.get('http://httpbin.org/get', params=payload)&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3 Response ContentWe can read the content of the server’s response. 123&gt;&gt;&gt; r = requests.get('https://api.github.com/events')&gt;&gt;&gt; r.textu'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. The text encoding guessed by Requests is used when you access r.text. You can find out what encoding Requests is using, and change it, using the r.encoding property: 123&gt;&gt;&gt; r.encoding'utf-8'&gt;&gt;&gt; r.encoding = 'ISO-8859-1' If you change the encoding, Requests will use the new value of r.encoding whenever you call r.text. You might want to do this in any situation where you can apply special logic to work out what the encoding of the content will be. For example, HTML and XML have the ability to specify their encoding in their body. In situations like this, you should use r.content to find the encoding, and then set r.encoding. This will let you use r.text with the correct encoding. Binary Response Content12&gt;&gt;&gt; r.contentb'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... For example, to create an image from binary data returned by a request, you can use the following code: 123&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; i= Image.open(BytesIO(r.content)) JSON Response ContentThere’s also a builtin JSON decoder. 123&gt;&gt;&gt; r = requests.get('https://api.github.com/events')&gt;&gt;&gt; r.json()[&#123;u'repository': &#123;u'open_issues': 0, u'url': 'https://github.com/... In case the JSON decoding fails, r.json() raises an exception. For example, if the response gets a 204 (No Content), or if the response contains invalid JSON, attempting r.json() raises ValueError: No JSON object could be decoded. It should be noted that the success of the call to r.json() does not indicate the success of the response. Some servers may return a JSON object in a failed response (e.g. error details with HTTP 500). Such JSON will be decoded and returned. To check that a request is successful, use r.raise_for_status() or check r.status_code is what you expect. Raw Response ContentIn the rare case that you’d like to get the raw socket response from the server, you can access r.raw. If you want to do this, make sure you set stream=True in your initial request. 12345&gt;&gt;&gt; r = requests.get('https://api.github.com/events', stream=True)&gt;&gt;&gt; r.raw&lt;urllib3.response.HTTPResponse object at 0x101194810&gt;&gt;&gt;&gt; r.raw.read(10)'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03' In general, however, you should use a pattern like this to save what is being streamed to a file:1234with open(filename, 'wb') as fd: for chunk in r.iter_content(chunk_size=128): fd.write(chunk) An important note about using Response.iter_content versus Response.raw. Response.iter_content will automatically decode the gzip and deflate transfer-encodings. Response.raw is a raw stream of bytes – it does not transform the response content. If you really need access to the bytes as they were returned, use Response.raw. Custom HeadersIf you’d like to add HTTP headers to a request, simply pass in a dict to the headers parameter. 123url = 'https://api.github.com/some/endpoint'headers=&#123;'user-agent':'my-app/0.0.1'&#125;r=requests.get(url, headers= headeres) Custom headers are given less precedence than more specific sources of information. For instance: Authorization headers set with headers= will be overridden if credentials are specified in .netrc, which in turn will be overridden by the auth= parameter. Authorization headers will be removed if you get redirected off-host. Proxy-Authorization headers will be overridden by proxy credentials provided in the URL. Content-Length headers will be overridden when we can determine the length of the content. More complicated POST requestsTypically, you want to send some form-encoded data — much like an HTML form. To do this, simply pass a dictionary to the data argument. Your dictionary of data will automatically be form-encoded when the request is made: 1234&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;&gt;&gt;&gt; r= requests.post('http://httpbin.org/post', data= payload)&gt;&gt;&gt; r.text'&#123;"args":&#123;&#125;,"data":"","files":&#123;&#125;,"form":&#123;"key1":"value1","key2":"value2"&#125;,...... You can also pass a list of tuples to the data argument. This is particularly useful when the form has multiple elements that use the same key: 1234&gt;&gt;&gt; payload = (('key1', 'value1'), ('key1', 'value2'))&gt;&gt;&gt; r= requests.post('http://httpbin.org/post', data= payload)&gt;&gt;&gt; print(r.text)&#123;"args":&#123;&#125;,"data":"","files":&#123;&#125;,"form":&#123;"key1":["value1","value2"]&#125;,"headers":&#123;...... There are times that you may want to send data that is not form-encoded. If you pass in a string instead of a dict, that data will be posted directly. For example, the GitHub API v3 accepts JSON-Encoded POST/PATCH data: 123&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r= requests.post(url, data= json.dumps(payload)) Instead of encoding the dict yourself, you can also pass it directly using the json parameter (added in version 2.4.2) and it will be encoded automatically: 123&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r= requests.post(url, json= payload) Note, the json parameter is ignored if either data or files is passed. Using the json parameter in the request will change the Content-Type in the header to application/json. POST a Multipart-Encoded File1234567891011&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files=&#123;'file': open('report.xls', 'rb')&#125;&gt;&gt;&gt; r=requests.post(url, files= files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "&lt;censored...binary...data&gt;" &#125;, ...&#125; You can set the filename, content_type and headers explicitly: 1234567891011&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files = &#123;'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', &#123;'Expires': '0'&#125;)&#125;&gt;&gt;&gt; r = requests.post(url, files=files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "&lt;censored...binary...data&gt;" &#125;, ...&#125; you can send strings to be received as files: 1234567891011&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files = &#123;'file': ('report.csv', 'some,data,to,send\nanother,row,to,send\n')&#125;&gt;&gt;&gt; r = requests.post(url, files=files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "some,data,to,send\\nanother,row,to,send\\n" &#125;, ...&#125; In the event you are posting a very large file as a multipart/form-data request, you may want to stream the request. By default, requests does not support this, but there is a separate package which does - requests-toolbelt. Response Status CodesWe can check the response status code: 123&gt;&gt;&gt; r = requests.get('http://httpbin.org/get')&gt;&gt;&gt; r.status_code200 Requests also comes with a built-in status code lookup object for easy reference: 12&gt;&gt;&gt; r.status_code == requests.codes.okTrue If we made a bad request (a 4XX client error or 5XX server error response), we can raise it with Response.raise_for_status(): 12345678&gt;&gt;&gt; bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt; bad_r.status_code404&gt;&gt;&gt; bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error But, since our status_code for r was 200, when we call raise_for_status() we get: 12&gt;&gt;&gt; r.raise_for_status()None Response HeadersWe can view the server’s response headers using a Python dictionary:1234567891011&gt;&gt;&gt; r.headers&#123; 'content-encoding': 'gzip', 'transfer-encoding': 'chunked', 'connection': 'close', 'server': 'nginx/1.0.4', 'x-runtime': '148ms', 'etag': '"e1ca502697e5c9317743dc078f67693f"', 'content-type': 'application/json'&#125; The dictionary is special, though: it’s made just for HTTP headers. According to RFC 7230, HTTP Header names are case-insensitive. So, we can access the headers using any capitalization we want: 1234&gt;&gt;&gt; r.headers['Content-Type']'application/json'&gt;&gt;&gt; r.headers.get('content-type')'application/json' It is also special in that the server could have sent the same header multiple times with different values, but requests combines them so they can be represented in the dictionary within a single mapping, as per RFC 7230: CookiesIf a response contains some Cookies, you can quickly access them: 1234&gt;&gt;&gt; url = 'http://example.com/some/cookie/setting/url'&gt;&gt;&gt; r = requests.get(url)&gt;&gt;&gt; r.cookies['example_cookie_name']'example_cookie_value' To send your own cookies to the server, you can use the cookies parameter: 12345&gt;&gt;&gt; url = 'http://httpbin.org/cookies'&gt;&gt;&gt; cookies = dict(cookies_are='working')&gt;&gt;&gt; r = requests.get(url, cookies=cookies)&gt;&gt;&gt; r.text'&#123;"cookies": &#123;"cookies_are": "working"&#125;&#125;' Cookies are returned in a RequestsCookieJar, which acts like a dict but also offers a more complete interface, suitable for use over multiple domains or paths. Cookie jars can also be passed in to requests: 1234567&gt;&gt;&gt; jar = requests.cookies.RequestsCookieJar()&gt;&gt;&gt; jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')&gt;&gt;&gt; jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')&gt;&gt;&gt; url = 'http://httpbin.org/cookies'&gt;&gt;&gt; r = requests.get(url, cookies=jar)&gt;&gt;&gt; r.text'&#123;"cookies": &#123;"tasty_cookie": "yum"&#125;&#125;' Redirection and HistoryBy default Requests will perform location redirection for all verbs except HEAD. We can use the history property of the Response object to track redirection. The Response.history list contains the Response objects that were created in order to complete the request. The list is sorted from the oldest to the most recent response. For example, GitHub redirects all HTTP requests to HTTPS: 1234567&gt;&gt;&gt; r = requests.get('http://github.com')&gt;&gt;&gt; r.url'https://github.com/'&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] If you’re using GET, OPTIONS, POST, PUT, PATCH or DELETE, you can disable redirection handling with the allow_redirects parameter: 12345&gt;&gt;&gt; r = requests.get('http://github.com', allow_redirects=False)&gt;&gt;&gt; r.status_code301&gt;&gt;&gt; r.history[] If you’re using HEAD, you can enable redirection as well: 12345&gt;&gt;&gt; r = requests.head('http://github.com', allow_redirects=True)&gt;&gt;&gt; r.url'https://github.com/'&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] TimeoutsYou can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely: 1234&gt;&gt;&gt; requests.get('http://github.com', timeout=0.001)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001) timeout is not a time limit on the entire response download; rather, an exception is raised if the server has not issued a response for timeout seconds (more precisely, if no bytes have been received on the underlying socket for timeout seconds). If no timeout is specified explicitly, requests do not time out. Errors and ExceptionsIn the event of a network problem (e.g. DNS failure, refused connection, etc), Requests will raise a ConnectionError exception. Response.raise_for_status() will raise an HTTPError if the HTTP request returned an unsuccessful status code. If a request times out, a Timeout exception is raised. If a request exceeds the configured number of maximum redirections, a TooManyRedirects exception is raised. All exceptions that Requests explicitly raises inherit from requests.exceptions.RequestException.]]></content>
      <categories>
        <category>python module</category>
      </categories>
      <tags>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_Scripting_C10_Animation]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-Scripting-C10-Animation%2F</url>
    <content type="text"><![CDATA[PositionAn element’s position in the browser window is presentational information. As such, it is usually addedusing CSS. Here’s an example that sets an element’s position on the page: 12345element &#123; position: absolute; top: 50px; left: 100px;&#125; That will position the element 100 pixels from the left of the browser window and 50 pixels from the top. Here’s the DOM equivalent of the same information: 123element.style.position = "absolute";element.style.left = "100px";element.style.top = "50px"; Valid values for the position property are “static”, “fixed”, “relative”, and “absolute”. Elements have a position value of “static” by default, which simply means that they appear one after the other in the same sequence as they occur in the markup. The “relative” value is similar. The difference is that relatively positioned elements can be taken out of the regular flow of the document by applying the float property. By applying a value of “absolute” to an element’s position, you can place the element wherever you want in relation to its container. The container is either the document itself or a parent element with a position of “fixed” or “absolute”. It doesn’t matter where the element appears in the original markup, because its position will be determined by properties like top, left, right, and bottom. You can set any of those properties using pixels or percentages. TimeThe JavaScript function setTimeout allows you to execute a function after a specified amount of time has elapsed. It takes two arguments. The first argument is a string containing the function you want to execute. The second argument is the number of milliseconds that will elapse before the first argument is executed. setTimeout(&quot;function&quot;,interval) It’s a good idea to always assign the result of this function to a variable: variable = setTimeout(&quot;function&quot;,interval) You’ll need to do this if you want to cancel the action that has been queued up. You can cancel a pending action using a function called clearTimeout. This function takes one argument, which is a variable that has been assigned to the result of a setTimeout function: clearTimeout(variable) Let’s update the positionMessage function so that it calls moveMessage after 5 seconds (5,000milliseconds): 12345678910111213141516function moveMessage() &#123; if (!document.getElementById) return false; if (!document.getElementById("message")) return false; var elem = document.getElementById("message"); elem.style.left = "200px";&#125; function positionMessage() &#123; if (!document.getElementById) return false; if (!document.getElementById("message")) return false; var elem = document.getElementById("message"); elem.style.position = "absolute"; elem.style.left = "50px"; elem.style.top = "100px"; movement = setTimeout("moveMessage()",5000);&#125; The movement variable refers to the setTimeout function defined in positionMessage. It’s a global variable; it wasn’t declared with the var keyword. This means the action can be canceled outside the positionMessage function. Incremental movement12345678910111213141516171819202122232425function moveMessage() &#123; if (!document.getElementById) return false; if (!document.getElementById("message")) return false; var elem = document.getElementById("message"); var xpos = parseInt(elem.style.left); var ypos = parseInt(elem.style.top); if (xpos == 200 &amp;&amp; ypos == 100) &#123; return true; &#125; if (xpos &lt; 200) &#123; xpos++; &#125; if (xpos &gt; 200) &#123; xpos--; &#125; if (ypos &lt; 100) &#123; ypos++; &#125; if (ypos &gt; 100) &#123; ypos--; &#125; elem.style.left = xpos + "px"; elem.style.top = ypos + "px"; movement = setTimeout("moveMessage()",10);&#125; The message moves across the screen, one pixel at a time. Once the top property is “100px” and the left property is “200px”, the function stops. The solutionThe image is called topics.gif. It is 400 pixels wide and 100 pixels tall. Right now, the entire image is visible. We want only a 100-by-100 pixel portion to be visible at any one time. We can’t do that with JavaScript, but we can do it with CSS. 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="utf-8" /&gt; &lt;title&gt;Web Design&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Web Design&lt;/h1&gt; &lt;p&gt;These are the things you should know.&lt;/p&gt; &lt;ol id="linklist"&gt; &lt;li&gt; &lt;a href="structure.html"&gt;Structure&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="presentation.html"&gt;Presentation&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="behavior.html"&gt;Behavior&lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;img src="images/topics.gif" alt="building blocks of web design" id="preview" /&gt;&lt;/body&gt;&lt;/html&gt; CSSThe CSS overflow property dictates how content within an element should be displayed when the content is larger than its container element. When an element contains content that is larger than itself, there is an overflow. In that situation, you can clip the content so that only a portion of it is visible. You can also specify whether or not the web browser should display scrollbars, allowing the user to see the rest of the content. There are four possible values for the overflow property: “visible”: If the overflow of an element is set to “visible”, then no clipping occurs. The content overflows and is rendered outside the element. “hidden”: A value of “hidden” will cause the excess content to be clipped. Only a portion of the content will be visible. “scroll”: The “scroll” value is similar to “hidden”. The content will be clipped, but the web browser will display scrollbars so that the rest of the content can be viewed. “auto”: A value of “auto” is just like “scroll”, except that the scrollbars will be displayed only if the content overflows its container element. If there is no overflow, no scrollbars appear. Of these four values, “hidden” sounds like the most promising for our purposes. We want to display just a 100-by-100 pixel portion of an image that is 400-by-100 pixels in size. First, let’s wrap the image in a container element. We’ll put it in a div element with an id of “slideshow”: 123&lt;div id="slideshow"&gt; &lt;img src="images/topics.gif" alt="building blocks of web design" id="preview" /&gt;&lt;/div&gt; In layout.css, we can set the size of the “slideshow” div: 123456#slideshow &#123; width: 100px; height: 100px; position: relative; overflow: hidden;&#125; Setting the position to relative is important because we want to use an absolute position for the child image. By using relative, the 0,0 position for children elements will be the upper-left corner of the slideshow div. The next part of the plan revolves around the actions of the user. We want to display a different portion of topics.gif in the “slideshow” div depending on which link the user hovers the mouse over. This is a behavioral change and definitely a job for JavaScript and the DOM. JavaScriptWe’ll use the moveElement function to move the topics.gif image around. We’ll move the image to the left or to the right, depending on which link the user is currently hovering over. We need to attach that behavior (calling the moveElement function) to the onmouseover event of each link in the link list. 1234567891011121314151617181920212223242526function prepareSlideshow() &#123;// Make sure the browser understands the DOM methods if (!document.getElementsByTagName) return false; if (!document.getElementById) return false;// Make sure the elements exist if (!document.getElementById("linklist")) return false; if (!document.getElementById("preview")) return false;// Apply styles to the preview image var preview = document.getElementById("preview"); preview.style.position = "absolute"; preview.style.left = "0px"; preview.style.top = "0px";// Get all the links in the list var list = document.getElementById("linklist"); var links = list.getElementsByTagName("a");// Attach the animation behavior to the mouseover event links[0].onmouseover = function() &#123; moveElement("preview",-100,0,10); &#125; links[1].onmouseover = function() &#123; moveElement("preview",-200,0,10); &#125; links[2].onmouseover = function() &#123; moveElement("preview",-300,0,10); &#125;&#125; This doesn’t mean that the topics.gif image will appear in the top-left corner of the screen. Instead, it will appear in the top-left corner of its container element, the “slideshow” div. That’s because the CSS position value of the div is “relative”. Any absolutely positioned elements contained by a relatively positioned element will be placed in relation to that container element. In other words, the “preview” image will appear zero pixels to the left and zero pixels from the top of the “slideshow” element. But something is not quite right. If you move quickly from link to the link, the animation becomes confused. There’s something wrong with the moveElement function. A question of scopeThe animation problem is being caused by a global variable. When we abstracted the moveMessagefunction and turned it into the moveElement function, we left the variable movement as it was: 1234567891011121314151617181920212223242526function moveElement(elementID,final_x,final_y,interval) &#123; if (!document.getElementById) return false; if (!document.getElementById(elementID)) return false; var elem = document.getElementById(elementID); var xpos = parseInt(elem.style.left); var ypos = parseInt(elem.style.top); if (xpos == final_x &amp;&amp; ypos == final_y) &#123; return true; &#125; if (xpos &lt; final_x) &#123; xpos++; &#125; if (xpos &gt; final_x) &#123; xpos--; &#125; if (ypos &lt; final_y) &#123; ypos++; &#125; if (ypos &gt; final_y) &#123; ypos--; &#125; elem.style.left = xpos + "px"; elem.style.top = ypos + "px"; var repeat = "moveElement('"+elementID+"',"+final_x+","+final_y+","+interval+")"; movement = setTimeout(repeat,interval);&#125; This is causing a problem now that the moveElement function is being called whenever the user hovers over a link. Regardless of whether or not the previous call to the function has finished moving the image, the function is being asked to move the same element somewhere else. In other words, the moveElement function is attempting to move the same element to two different places at once, and the movement variable has become the rope in a tug of war. As the user quickly moves from link to link, there is a backlog of events building up in the setTimeoutqueue. We can flush out this backlog by using clearTimeout: clearTimeout(movement); But if this statement is executed before movement has been set, we’ll get an error. We can’t use a local variable: var movement = setTimeout(repeat,interval); If we do that, the clearTimeout statement won’t work; the movement variable will no longer exist. We can’t use a global variable. We can’t use a local variable. We need something in between. We need a variable that applies just to the element being moved. Element-specific variables do exist. In fact, we’ve been using them all the time. What I’ve just described is a property. Let’s change movement from being a global variable to a property of the element being moved, elem. That way, we can test for its existence and, if it exists, use clearTimeout. 1234567891011121314151617181920212223242526272829function moveElement(elementID,final_x,final_y,interval) &#123; if (!document.getElementById) return false; if (!document.getElementById(elementID)) return false; var elem = document.getElementById(elementID); if (elem.movement) &#123; clearTimeout(elem.movement); &#125; var xpos = parseInt(elem.style.left); var ypos = parseInt(elem.style.top); if (xpos == final_x &amp;&amp; ypos == final_y) &#123; return true; &#125; if (xpos &lt; final_x) &#123; xpos++; &#125; if (xpos &gt; final_x) &#123; xpos--; &#125; if (ypos &lt; final_y) &#123; ypos++; &#125; if (ypos &gt; final_y) &#123; ypos--; &#125; elem.style.left = xpos + "px"; elem.style.top = ypos + "px"; var repeat = "moveElement('"+elementID+"',"+final_x+","+final_y+","+interval+")"; elem.movement = setTimeout(repeat,interval);&#125; Whichever element is currently being moved by the moveElement function is assigned a property called movement. If the element already has this property at the start of the function, it is reset using clearTimeout. This means that even if the same element is being told to move in different directions, there is only ever one setTimeout statement. Refining the animationUsing the ceil property of the Math object, you can round up the value of the variable dist. The ceil property has the following syntax: Math.ceil(number) This will round up any floating-point number to the nearest integer. There is a corresponding floor property that will round any floating-point number down to the nearest integer. The round property will round any floating-point number to whichever whole number is closest: 12Math.floor(number)Math.round(number) For the moveElement function, we’ll round upward. If we used floor or round, the element might never reach its final destination. 12dist = Match.ceil((final_x - xpos)/10);xpos = xpos + dist; The updated moveElement function looks like this:1234567891011121314151617181920212223242526272829303132333435function moveElement(elementID,final_x,final_y,interval) &#123; if (!document.getElementById) return false; if (!document.getElementById(elementID)) return false; var elem = document.getElementById(elementID); if (elem.movement) &#123; clearTimeout(elem.movement); &#125; var xpos = parseInt(elem.style.left); var ypos = parseInt(elem.style.top); var dist = 0; if (xpos == final_x &amp;&amp; ypos == final_y) &#123; return true; &#125; if (xpos &lt; final_x) &#123; dist = Math.ceil((final_x - xpos)/10); xpos = xpos + dist; &#125; if (xpos &gt; final_x) &#123; dist = Math.ceil((xpos - final_x)/10); xpos = xpos - dist; &#125; if (ypos &lt; final_y) &#123; dist = Math.ceil((final_y - ypos)/10); ypos = ypos + dist; &#125; if (ypos &gt; final_y) &#123; dist = Math.ceil((ypos - final_y)/10); ypos = ypos - dist; &#125; elem.style.left = xpos + "px"; elem.style.top = ypos + "px"; var repeat = "moveElement('"+elementID+"',"+final_x+","+final_y+","+interval+")"; elem.movement = setTimeout(repeat,interval);&#125;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_Scripting_C09_CSS-DOM]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-Scripting-C09-CSS-DOM%2F</url>
    <content type="text"><![CDATA[The style propertyEvery element in a document is an object, and every one of these objects has a whole collection of properties. Some properties contain information about the element’s position in the node tree. Properties like parentNode, nextSibling, previousSibling, childNodes, firstChild, and lastChild all supply information about related nodes in the document.Other properties, like nodeType and nodeName, contain information about the element itself. Querying the nodeName property of an element will return a string like “p”. There’s another property called style. Every element node has this property. It contains information about the styles attached to the element. Querying this property doesn’t return a simple string; it returns an object. The style property is in fact an object. You can use the typeof keyword to retrieve this information. Compare the results of applying typeof to a property like nodeName and applying typeof to the style property. Style information is stored as properties of this style object: element.style.property Here’s an example of a paragraph with some inline styling attached: 123&lt;p id="example" style="color: grey; font-family: 'Arial',sans-serif;"&gt;An example of a paragraph&lt;/p&gt; Not only is every element an object, every element has a property called style, which is also an object. 12345window.onload = function() &#123; var para = document.getElementById("example"); alert(typeof para.nodeName); //string alert(typeof para.style); //object&#125; Getting stylesThe value of a style property is always a string. &lt;p id=&quot;example&quot; style=&quot;color: grey; font-family: &#39;Arial&#39;,sans-serif;&quot;&gt; You can retrieve the inline styles attached to para.The camel-casing convention applies to just about any CSS property that contains one or more dashes. The CSS property background-color becomes the DOM property backgroundColor. The CSS property font-weight is fontWeight in the DOM. The DOM property marginTopWidth is equivalent to margin-top-width in CSS. 12element.style.color // greyelement.style.fontFamily // 'Arial',sans-serif In the example paragraph, the CSS color property has been set with the word “grey”. The DOM color property returns a value of “grey”. Edit the paragraph so that the CSS color property is set with the hexadecimal value #999999: &lt;p id=&quot;example&quot; style=&quot;color: #999999; font-family: &#39;Arial&#39;,sans-serif&quot;&gt; para.style.color // rgb(153,153,153) This is an exceptional circumstance. Usually values are returned in the same units with which they are set. If you set the CSS font-size property in ems, the corresponding DOM fontSize property will also be in ems. If an element’s CSS font-size property has a value of “1em”, the DOM fontSize property will return a value of “1em”. If “12px” is applied with CSS, the DOM will return “12px”. Inline onlyThe style property only returns inline style information. If you apply styling information by inserting style attributes into your markup, you can query that information using the DOM style property: &lt;p id=&quot;example&quot; style=&quot;color: grey; font: 12px &#39;Arial&#39;,sans-serif;&quot;&gt; Add a link element to the head of example.html, pointing to the styles.css file: &lt;link rel=&quot;stylesheet&quot; media=&quot;screen&quot; href=&quot;styles/styles.css&quot; /&gt; The DOM style property doesn’t retrieve styles declared externally.You’ll see the same result (or lack thereof) if you add style information between \&lt;style> tags in the head of example.html.The DOM style property won’t pick up that information. 123456&lt;style&gt; p#example &#123; color: grey; font: 12px 'Arial', sans-serif; &#125;&lt;/style&gt; The style object doesn’t include stylistic information that has been declared in an external stylesheet. It also doesn’t include stylistic information that has been declared in the head of a document. The style object does pick up stylistic information that has been declared inline using the style attribute. But this is of little practical use, because styles should be applied separately from the markup. Setting stylesMany DOM properties are read-only. That means you can use them to retrieve information, but you can’t use them to set or update information. Properties like previousSibling, nextSibling, parentNode, firstChild, and lastChild are invaluable for gathering information about an element’s position in the document’s node tree, but they can’t be used to update information. The properties of the style object, on the other hand, are read/write. That means you can use an element’s style property to retrieve information, and you can also use it to update information. Styling elements in the node treeCSS2 introduced a number of position-based selectors such as :first-child and :last-child, while CSS3 includes position selectors such as :nth-child() and :nth-of-type(), but it’s still sometimes difficult to apply styles to an element based on its position in the document’s node tree. For instance, with CSS3 you could say, “Apply the following styles to the next sibling of all h1 elements” by using the h1~ * selector. The problem, however, is that many browsers don’t support all the nice CSS3 position selectors. The DOM, on the other hand, makes it quite easy to target elements based on their familial relationships with other elements. Using the DOM, you could quite easily imitate the CSS3 sibling selector and find all the h1 elements in a document and then find out what element immediately follows each h1 and apply styles specifically to those elements. But in this case you actually want to find not just the next node, but specifically the next element node. This is easily done using a function called getNextElement: 123456789function getNextElement(node) &#123; if(node.nodeType == 1) &#123; return node; &#125; if (node.nextSibling) &#123; return getNextElement(node.nextSibling); &#125; return null;&#125; Repetitive stylingA common technique for making table rows more readable is to alternate the background colors. The resulting striped effect helps to separate individual rows. This can be done by applying styles to every second row. If the browser supports CSS3, it’s as easy as this: 12tr:nth-child(odd) &#123; background-color:# ffc; &#125;tr:nth-child(even) &#123; background-color:#fff; &#125; To get this same effect when :nth-child() isn’t available, you’ll need a different technique. This could be easily done by assigning a class attribute to each odd or even row.However, this isn’t very convenient, especially for larger tables—if a row is added to or removed from the middle of the table, you would have to painstakingly update the class attributes by hand. JavaScript is very good at handling repetitive tasks. You can easily loop through a long list using a while or for loop. 1234567891011121314151617function stripeTables() &#123; if (!document.getElementsByTagName) return false; var tables = document.getElementsByTagName("table"); var odd, rows; for (var i=0; i&lt;tables.length; i++) &#123; odd = false; rows = tables[i].getElementsByTagName("tr"); for (var j=0; j&lt;rows.length; j++) &#123; if (odd == true) &#123; rows[j].style.backgroundColor = "#ffc"; odd = false; &#125; else &#123; odd = true; &#125; &#125; &#125;&#125; Responding to eventsIt’s not always easy to know when to use CSS and when to use DOM scripting to set styles. The biggest grey area concerns the changing of styles based on events. CSS provides pseudo-classes like :hover that allow you to change the styles of elements based on their state. The DOM also responds to changes of state using event handlers like onmouseover. It’s difficult to know when to use :hover and when to use onmouseover. The simplest solution is to follow the path of least resistance. If you simply want to change the color of your links when they’re moused over, then you should definitely use CSS: 123a:hover &#123; color: #c60;&#125; The :hover pseudo-class is widely supported, at least when it is used for styling links. If you want to style any other elements when they are moused over, browser support isn’t quite so widespread. Take the example of the table in itinerary.html. If you wanted to highlight a row when it is moused over, you could use CSS: 123tr:hover &#123; font-weight: bold;&#125; In theory, this should bold the text in any table row whenever it is moused over. In practice, this will only work in certain browsers. In situations like that, the DOM can be used to level the playing field. While support for CSS pseudoclasses remains patchy, the DOM is well supported in most modern browsers. Until CSS support improves, it makes sense to use the DOM to apply styles based on events. Here’s a short function called highlightRows that will bold text whenever a table row is moused over: 123456789101112function highlightRows() &#123; if(!document.getElementsByTagName) return false; var rows = document.getElementsByTagName("tr"); for (var i=0; i&lt;rows.length; i++) &#123; rows[i].onmouseover = function() &#123; this.style.fontWeight = "bold"; &#125; rows[i].onmouseout = function() &#123; this.style.fontWeight = "normal"; &#125; &#125;&#125; classNameIn the examples you’ve seen so far in this chapter, the DOM has been used to explicitly set stylistic information. This is less than ideal because the behavior layer is doing the work of the presentation layer. If you change your mind about the styles being set, you’ll need to dig down into your JavaScript functions and update the relevant statements. It would be better if you could make those kinds of changes in your style sheets. There’s a simple solution. Instead of changing the presentation of an element directly with the DOM, use JavaScript to update the class attribute attached to that element. Take a look at how the styleHeaderSiblings function is adding stylistic information: 12345678910function styleHeaderSiblings() &#123; if (!document.getElementsByTagName) return false; var headers = document.getElementsByTagName("h1"); var elem; for (var i=0; i&lt;headers.length; i++) &#123; elem = getNextElement(headers[i].nextSibling); elem.style.fontWeight = "bold"; elem.style.fontSize = "1.2em"; &#125;&#125; If you ever decided that the elements following level one headings should have a CSS font-size value of “1.4em” instead of “1.2em”, you would have to update the styleHeaderSiblings function. It would be better if you were to include a style sheet with a declaration for a class named something like intro: 1234.intro &#123; font-weight: bold; font-size: 1.2em;&#125; You can use the assignment operator to set the class of an element. You can do this using the setAttribute method. elem.setAttribute(&quot;class&quot;,&quot;intro&quot;); An easier solution is to update a property called className. This is a read/write property of any element node. You can use className to get the class of an element. element.className You can use the assignment operator to set the class of an element. element.className = value This is how the styleHeaderSiblings function looks when you use className instead of setting styles directly with the style property: 123456789function styleHeaderSiblings() &#123; if (!document.getElementsByTagName) return false; var headers = document.getElementsByTagName("h1"); var elem; for (var i=0; i&lt;headers.length; i++) &#123; elem = getNextElement(headers[i].nextSibling); elem.className = "intro"; &#125;&#125; There’s just one drawback to this technique. If you assign a class using the className property, you will overwrite any classes that are already attached to that element. You can do this by concatenating a space and the name of the new class to the className property: elem.className += &quot; intro&quot;; But you really only want to do this when there are existing classes. If there are no existing classes, you can just use the assignment operator with className. 12345678910function addClass(element,value) &#123; if (!element.className) &#123; element.className = value; &#125; else &#123; newClassName = element.className; newClassName+= " "; newClassName+= value; element.className = newClassName; &#125;&#125; Call the addClass function from the styleHeaderSiblings function: 123456789function styleHeaderSiblings() &#123; if (!document.getElementsByTagName) return false; var headers = document.getElementsByTagName("h1"); var elem; for (var i=0; i&lt;headers.length; i++) &#123; elem = getNextElement(headers[i].nextSibling); addClass(elem,"intro"); &#125;&#125;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_scripting_C05_Best_Practices]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-scripting-C05-Best-Practices%2F</url>
    <content type="text"><![CDATA[JavaScript uses the open() method of the window object to create new browser windows. Thismethod takes three arguments:window.open(url,name,features) Here’s an example of a typical function that uses window.open(): 1234function popUp(winURL) &#123; window.open(winURL,"popup","width=320,height=480");&#125; //This will open up a new window (called "popup") that’s 320 pixels wide by 480 pixels tall 12345678910&lt;a href="#" onclick="popUp('http://example.com');return false;"&gt;Example&lt;/a&gt;&lt;a href="http://www.example.com/"onclick="popUp('http://www.example.com'; return false;"&gt;Example&lt;/a&gt; &lt;a href="http://www.example.com/"onclick="popUp(this.getAttribute('href'); return false;"&gt;Example&lt;/a&gt; &lt;a href="http://www.example.com/"onclick="popUp(this.href; return false;"&gt;Example&lt;/a&gt; The document loads within the browser window. The document object is a property of the window object. When the onload event is triggered by the window object, the document object then exists. 123456789101112window.onload= prepareLinks;function prepareLinks()&#123; var links= document.getElementsByTagName('a'); for(var i=0; i&lt; links.length; i++)&#123; if(links[i].getAttribute('class')=='popUp')&#123; links[i].onclick= function()&#123; popUp(this.getAttribute('href')); return false; &#125; &#125; &#125;&#125; DOM Core and HTML-DOMSo far, I’ve been using a small set of methods to accomplish everything I want to do, including• getElementById• getElementsByTagName• getAttribute• setAttribute These methods are all part of the DOM Core. They aren’t specific to JavaScript, and they can be used by any programming language with DOM support. They aren’t just for web pages, either. These methods can be used on documents written in any markup language (XML, for instance). When you are using JavaScript and the DOM with HTML files, you have many more properties atyour disposal. For example, I’ve actually used one of these properties, onclick, for event management in the image gallery. These properties belong to the HTML-DOM, which has been around longer than the DOM Core. For instance, the HTML-DOM provides a forms object. That means that instead of writingdocument.getElementsByTagName(“form”) you can usedocument.forms Similarly, the HTML-DOM provides properties to represent attributes of elements. Images, for instance, have a src property. Instead of writing:element.getAttribute(&quot;src&quot;) you can writeelement.src These methods and properties are interchangeable. It doesn’t really matter whether you decide to use the DOM Core exclusively or use the HTML-DOM. As you can see, the HTML-DOM is generally shorter. However, it’s worth remembering that it is specific to web documents, so you’ll need to bear that in mind if you ever find yourself using the DOM with other kinds of documents]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_Scripting_C07_adding_markup_to_a_document_Ajax]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-Scripting-C07-adding-markup-to-a-document-Ajax%2F</url>
    <content type="text"><![CDATA[innerHTMLinnerHTML can be used to read and write the HTML in an element. The DOM provides a very detailed picture of the markup. Using DOM methods and properties, you can access any of those nodes individually. The innerHTML property takes a much simpler view. 1234&lt;div id="testdiv"&gt; &lt;p&gt;This is &lt;em&gt;my&lt;/em&gt; content.&lt;/p&gt;&lt;/div&gt; Like document.write, innerHTML is HTML-specific. You won’t be able to use it on any other kind of markup document. That means if you are serving up XHTML with the correct MIME type, innerHTML won’t work. In any case, the standardized DOM can be used instead of innerHTML. It may take slightly more code to achieve the same results, but as you’ll see, it offers more precision and power. DOM methodsThe DOM is a representation of the document. The information contained in the DOM reflects the information in the document. The DOM is a two-way street. You can query the contents of a document and you can also update the contents of a document. According to the DOM, a document is a tree of nodes. If you want to add to this tree, you need to insert new nodes. If you want to add markup to a document, you need to insert element nodes. createElementvar para = document.createElement(&quot;p&quot;); The variable para now contains a reference to the p element you’ve just created. Right now, this newly created paragraph element is floating in JavaScript limbo. The element exists, but it isn’t part of the DOM node tree. This is called a document fragment. It isn’t displayed in the browser. Nonetheless, it has DOM properties, just like any other node. appendChild123var testdiv = document.getElementById("testdiv"); var para = document.createElement("p"); testdiv.appendChild(para); The newly created paragraph element is now a child of the “testdiv” element. It has been moved from JavaScript limbo and inserted into the node tree of test.html. createTextNodeIf you want to put some text into that paragraph, you can’t use createElement. That works only for creating element nodes. You need to create a text node. var txt = document.createTextNode(&quot;Hello world&quot;); The variable txt contains a reference to the newly created text node. This node is floating free in JavaScript. It hasn’t been tethered to the node tree of a document. You can use appendChild to make the text the child node of an existing element. You could insert the text into the paragraph element you created. The variable para is a reference to the paragraph element. The variable txt is a reference to the newly created text node: para.appendChild(txt); You can also use appendChild to join nodes that aren’t yet part of the document tree.12345678window.onload = function() &#123;var para = document.createElement("p");var txt = document.createTextNode("Hello world");para.appendChild(txt);var testdiv = document.getElementById("testdiv");testdiv.appendChild(para);&#125; Inserting a new element before an existing oneparentElement.insertBefore(newElement,targetElement) For instance, this is how you could insert the placeholder element before the image gallery list, which has the id “imagegallery“:123var gallery = document.getElementById("imagegallery");gallery.parentNode.insertBefore(placeholder,gallery); Writing the insertAfter functionAlthough the DOM hasn’t provided a method called insertAfter, it does have all the tools you need to insert a node after another node. You can use existing DOM methods and properties to create a function called insertAfter: 12345678function insertAfter(newElement,targetElement) &#123; var parent = targetElement.parentNode; if (parent.lastChild == targetElement) &#123; parent.appendChild(newElement); &#125; else &#123; parent.insertBefore(newElement,targetElement.nextSibling); &#125;&#125; AjaxThe advantage of using Ajax is that with Ajax, the request to the server occurs asynchronously to the page. Instead of serving up a whole new page every time the user sends a request, the browser can process requests in the background while the user continues to view and interact with the page. The XMLHttpRequest objectThe magic of Ajax is achieved by using the XMLHttpRequest object. This object acts as an intermediary between your script in the web browser (client) and the server. Instead of the web browser, JavaScript initiates the requests, and as a result, must also handle the response. The XMLHttpRequest object it is a relatively new standard (part of HTML5), however, it has a long history and is widely supported in modern web browsers. Unfortunately, different browsers implement XMLHttpRequest in different ways. So for the best results, you’ll need to branch your code. 1234567891011121314151617function getNewContent()&#123; var request= getHTTPObject(); if(request)&#123; request.open('GET', 'example.txt', true); request.onreadystatechange=function()&#123; if(request.readyState==4)&#123; var para= document.createElement('p'); var txt= document.createTextNode(request.responseText); para.appendChild(txt); document.getElementById('new').appendChild(para); &#125; &#125;; request.send(null); &#125;else&#123; alert('Sorry, your browser not support XMLHttpRequest'); &#125;&#125; The onreadystatechange property is the event handler that is triggered when the server sends a response back to the XMLHttpRequest object. You can use it to specify what happens with the response. After you’ve specified where the object should send a request and what it should do once it receives a response, you can start the process using the send method. When the server sends a response back to the XMLHttpRequest object, a number of properties are made available. The readyState property is a numeric value that is updated while the server deals with the request. There are five possible values: • 0 for uninitialized• 1 for loading• 2 for loaded• 3 for interactive• 4 for complete Once the readyState property has a value of 4, you have access to the data sent by the server. You can access this data as a string of text provided by the responseText property. If the data is sent back with a Content-Type header of “text/xml”, you can also access the responseXML property, which is effectively a DocumentFragment. You can use all the usual DOM methods to manipulate this DocumentFragment. This is where the XML part of XMLHttpRequest comes from. In the example, the onreadystatechange handler waits for a readyState value of 4, and then dumps the entire responseText property into a paragraph and appends it to the DOM. An easy thing to forget is the asynchronous aspect of the request. Once the XMLHttpRequest request has been sent, the script will continue to execute, without waiting for the response to return. 12345678910111213141516171819function getNewContent()&#123; var request= getHTTPObject(); if(request)&#123; request.open('GET', 'example.txt', true); request.onreadystatechange=function()&#123; if(request.readyState==4)&#123; alert("Response Received"); var para= document.createElement('p'); var txt= document.createTextNode(request.responseText); para.appendChild(txt); document.getElementById('new').appendChild(para); &#125; &#125;; request.send(null); &#125;else&#123; alert('Sorry, your browdser not support XMLHttpRequest'); &#125; alert("Function Done"); &#125; If you try to load your page now, the “Function Done” alert will most likely occur before the “Request Done” alert, because the script won’t wait for send to complete.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_scripting_C04_A_JavaScript_Image_Gallery]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-scripting-C04-A-JavaScript-Image-Gallery%2F</url>
    <content type="text"><![CDATA[Event handlersLet’s take a closer look at how event handling works. When you attach an event handler to an element, you can trigger JavaScript statements with the event. The JavaScript can return a result that is then passed back to the event handler. For example, you can attach some JavaScript to the onclick event of a link so that it returns a Boolean value of true or false. If you click the link, and the event handler receives a value of true, it’s getting the message “yes, this link has been clicked.” If you add some JavaScript to the event handler so that it returns false, then the message being sent back is “no, this link has not been clicked.” You can see this for yourself with this simple test: &lt;a href=&quot;http://www.example.com&quot; onclick=&quot;return false;&quot;&gt;Click me&lt;/a&gt;If you click that link, the default behavior will not be triggered, because the JavaScript is effectively canceling the default behavior. Introducing childNodesThe childNodes property is a way of getting information about the children of any element in a document’s node tree. It returns an array containing all the children of an element node: element.childNodes 1234function countBodyChildren() &#123; var body_element = document.getElementsByTagName("body")[0]; alert (body_element.childNodes.length);&#125; The childNodes property returns an array containing all types of nodes, not just element nodes. It will bring back all the attribute nodes and text nodes as well. In fact, just about everything in a document is some kind of node. Even spaces and line breaks are interpreted as nodes and are included in the childNodes array. Introducing the nodeType propertyThe nodeType property is called with the following syntax: node.nodeType However, instead of returning a string like “element” or “attribute,” it returns a number. There are twelve possible values for nodeType, but only three of them are going to be of much practical use: Element nodes have a nodeType value of 1. Attribute nodes have a nodeType value of 2. Text nodes have a nodeType value of 3. Introducing the nodeValue propertyIf you want to change the value of a text node, there is a DOM property called nodeValue that can be used to get (and set) the value of a node: node.nodeValue It can be used to retrieve the value of a node, but it can also be used to set the value of a node. Here’s a tricky little point. If you retrieve the nodeValue for description, you won’t get the text within the paragraph. You can test this with an alert statement: alert (description.nodeValue); This will return a value of null. The nodeValue of the paragraph element itself is empty. What you actually want is the value of the text within the paragraph. The text within the paragraph is a different node. This text is the first child node of the paragraph. Therefore, you want to retrieve the nodeValue of this child node. This alert statement will give you the value you’re looking for: alert(description.childNodes[0].nodeValue); Introducing firstChild and lastChildThere is a shorthand way of writing childNodes[0]. Whenever you want to get the value of the first node in the childNodes array, you can use firstChild: node.firstChild This is equivalent to: node.childNodes[0] The DOM also provides a corresponding lastChild property: node.lastChild This refers to the last node in the childNodes array. If you wanted to access this node without using the lastChild property, you would have to write: node.childNodes[node.childNodes.length-1]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOM_scripting_C02_Objects_And_C03_Document_Object_Model]]></title>
    <url>%2F2018%2F12%2F15%2FDOM-scripting-C02-Objects-And-C03-Document-Object-Model%2F</url>
    <content type="text"><![CDATA[Objects An object is a self-contained collection of data. This data comes in two forms: properties and methods:• A property is a variable belonging to an object.• A method is a function that the object can invoke. A new instance is created using the new keyword: var jeremy = new Person; Native ObjectsThe Array object is an example of a native object supplied by JavaScript. Other examples includeMath and Date, both of which have very useful methods for dealing with numbers and dates respectively. 123var num = 7.561;var num = Math.round(num);alert(num); Host objectsNative objects aren’t the only kind of premade objects that you can use in your scripts. Another kind ofobject is supplied not by the JavaScript language itself, but by the environment in which it’s running. In the case of the Web, that environment is the web browser. Objects that are supplied by the web browser are called host objects. Host objects include Form, Image, and Element. These objects can be used to get information about forms, images, and form elements within a web page. We’re not going to show you any examples of how to use those host objects. There is another object that can be used to get information about any element in a web page that you might be interested in: the document object. DOMThe M in DOM stands for Model, but it could just as easily stand for Map. A model, like a map, is a representation of something.The DOM represents the web page that’s currently loaded in the browser window. The browser provides a map (or model) of the page. You can use JavaScript to read this map. Maps make use of conventions like direction, contours, and scale. In order to read a map, you need to understand these conventions—and it’s the same with the DOM. In order to gain information from the model, you need to understand what conventions are being used to represent the document. If you can think of the elements of a document in terms of a tree of familial relationships, then you’re using the same terms as the DOM. However, instead of using the term family tree, it’s more accurate to call a document a node tree. NodesThe term node comes from networking, where it used to denote a point of connection in a network. A network is a collection of nodes. In the real world, everything is made up of atoms. Atoms are the nodes of the real world. But atoms can themselves be broken down into smaller, subatomic particles. These subatomic particles are also considered nodes. It’s a similar situation with the DOM. A document is a collection of nodes, with nodes as the branches and leaves on the document tree. There are a number of different types of nodes. Just as atoms contain subatomic particles, some types of nodes contain other types of nodes. Let’s take a quick look at three of them: element, text, and attribute nodes. Element nodesThe DOM’s equivalent of the atom is the element node. When I described the structure of the shopping list document, I did so in terms of elements such as \&lt;body>, \&lt;p>, and \&lt;ul>. Elements are the basic building blocks of documents on the Web, and it’s the arrangement of these elements in a document that gives the document its structure. The tag provides the name of an element. Paragraph elements have the name p, unordered lists have the name ul, and list items have the name li. Elements can contain other elements. All the list item elements in our document are contained within an unordered list element. In fact, the only element that isn’t contained within another element is the \&lt;html> element. It’s the root of our node tree. Text nodesElements are just one type of node. If a document consisted purely of empty elements, it would have a structure, but the document itself wouldn’t contain much content. On the Web, where content is king, most content is provided by text. In our example, the \&lt;p> element contains the text “Don’t forget to buy this stuff.” This is a text node. In XHTML, text nodes are always enclosed within element nodes. But not all elements contain text nodes. In our shopping list document, the \&lt;ul> element doesn’t contain any text directly. It contains other element nodes (the \&lt;li> elements), and these contain text nodes. Attribute nodesAttributes are used to give more specific information about an element. The title attribute, for example, can be used on just about any element to specify exactly what the element contains: &lt;p title=&quot;a gentle reminder&quot;&gt;Don&#39;t forget to buy this stuff.&lt;/p&gt; In the DOM, title=”a gentle reminder” is an attribute node, as shown in the diagram here. Because attributes are always placed within opening tags, attribute nodes are always contained within element nodes. Not all elements contain attributes, but all attributes are contained by elements. Cascading Style SheetsInheritance is a powerful feature of CSS. Like the DOM, CSS views the contents of a document as a node tree. Elements that are nested within the node tree will inherit the style properties of their parents. For instance, declaring colors or fonts on the body element will automatically apply those styles to all the elements contained within the body. Getting Elements123getElementById getElementsByTagNamegetElementsByClassName Getting and Setting AttributesgetAttributegetAttribute is a function. It takes only one argument, which is the attribute that you want to get: object.getAttribute(attribute) you can’t use getAttribute on the document object. It can be used on only an element node object. setAttribute]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>dom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C10_Reflection with SQLAlchemy ORM and Automap]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C10-Reflection-with-SQLAlchemy-ORM-and-Automap%2F</url>
    <content type="text"><![CDATA[Reflection lets you populate a SQLAlchemy object from an existing database; reflection works on tables, views, indexes, and foreign keys. But what if you want to reflect a database schema into ORM-style classes? Fortunately, the handy SQLAlchemy extension automap lets you do just that. Reflecting a Database with AutomapIn order to reflect a database, instead of using the declarative_base we’ve been using with the ORM so far, we’re going to use the automap_base. Let’s start by creating a Base object to work with. _Creating a Base object with automap_base_ 12&gt;&gt;&gt; from sqlalchemy.ext.automap import automap_base&gt;&gt;&gt; Base= automap_base() Next, we need an engine connected to the database that we want to reflect. Initializaing an engine for the Chinook database 12from sqlalchemy import create_engineengine= create_engine('sqlite:///Chinook_Sqlite.sqlite') With the Base and engine setup, we have everything we need to reflect the database. Using the prepare method on the Base object we created will scan everything available on the engine we just created, and reflect everything it can. &gt;&gt;&gt; Base.prepare(engine, reflect= True) That one line of code is all you need to reflect the entire database! This reflection has created ORM objects for each table that is accessible under the class property of the automap Base.123456789101112&gt;&gt;&gt; Base.classes.keys()['Album','Customer','Playlist','Artist','Track','Employee','MediaType','InvoiceLine','Invoice','Genre'] create some objects to reference the Artist and Album tables: 12&gt;&gt;&gt; Artist= Base.classes.Artist&gt;&gt;&gt; Album= Base.classes.Album Using the Artist table1234567891011121314&gt;&gt;&gt; from sqlalchemy.orm import Session&gt;&gt;&gt; session= Session(engine)&gt;&gt;&gt; for artist in session.query(Artist).limit(10):... print(artist.ArtistId, artist.Name)(1, u'AC/DC')(2, u'Accept')(3, u'Aerosmith')(4, u'Alanis Morissette')(5, u'Alice In Chains')(6, u'Ant\xf4nio Carlos Jobim')(7, u'Apocalyptica')(8, u'Audioslave')(9, u'BackBeat')(10, u'Billy Cobham') Reflected RelationshipsAutomap can automatically reflect and establish many-to-one, one-to-many, and many-to-many relationships. When automap creates a relationship, it creates a &lt;related_object&gt;_collection property on the object. Using the relationship between Artist and Album to print related data 12345&gt;&gt;&gt; artist= session.query(Artist).first()&gt;&gt;&gt; for album in artist.album_collection:... print('&#123;&#125;- &#123;&#125;'.format(artist.Name, album.Title))AC/DC - For Those About To Rock We Salute YouAC/DC - Let There Be Rock]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C08_Understanding the Session and Exceptions]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C08-Understanding-the-Session-and-Exceptions%2F</url>
    <content type="text"><![CDATA[The SQLAlchemy SessionUnderstanding the session states can be useful for troubleshooting exceptions and handling unexpected behaviors. There are four possible states for data object instances: Transient The instance is not in session, and is not in the database. Pending The instance has been added to the session with add(), but hasn’t been flushed or committed. Persistent The object in session has a corresponding record in the database. Detached The instance is no longer connected to the session, but has a record in the database. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;&gt;&gt; session.add(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Truepersistent: False detached: False&gt;&gt;&gt; cc_cookie=Cookie('chocolate chip','http://','CC01',12,0.50)&gt;&gt;&gt; from sqlalchemy import inspect&gt;&gt;&gt; insp= inspect(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: True pending: Falsepersistent: False detached: False&gt;&gt;&gt; session.commit()&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Falsepersistent: True detached: False&gt;&gt;&gt; session.expunge(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Falsepersistent: False detached: True&gt;&gt;&gt; session.add(cc_cookie)&gt;&gt;&gt; cc_cookie.cookie_name= 'Change cholocate chip'&gt;&gt;&gt; insp.modifiedTrue&gt;&gt;&gt; for attr, attr_state in insp.attrs.items():... if attr_state.history.has_changes():... print('&#123;&#125;: &#123;&#125;'.format(attr, attr_state.value))... print('History: &#123;&#125;\n'.format(attr_state.history))... cookie_name: Change cholocate chipHistory: History(added=['Change cholocate chip'], unchanged=(), deleted=())&gt;&gt;&gt; ExceptionsMultipleResultsFound ExceptionThis exception occurs when we use the .one() query method, but get more than one result back. 1234567891011121314151617181920212223&gt;&gt;&gt; results=session.query(Cookie).one()Traceback (most recent call last): File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2884, in one ret = self.one_or_none() File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2863, in one_or_none "Multiple rows were found for one_or_none()")sqlalchemy.orm.exc.MultipleResultsFound: Multiple rows were found for one_or_none()During handling of the above exception, another exception occurred:Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2887, in one "Multiple rows were found for one()")sqlalchemy.orm.exc.MultipleResultsFound: Multiple rows were found for one()&gt;&gt;&gt; from sqlalchemy.orm.exc import MultipleResultsFound&gt;&gt;&gt; try:... results= session.query(Cookie).one()... except MultipleResultsFound as error:... print('too many cookies...')... too many cookies...&gt;&gt;&gt; DetachedInstanceErrorThis exception occurs when we attempt to access an attribute on an instance that needs to be loaded from the database, but the instance we are using is not currently attached to the database. 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; cookiemon = User('cookiemon', 'mon@cookie.com', '111-111-1111', 'password')&gt;&gt;&gt; session.add(cookiemon)&gt;&gt;&gt; o1=Order()&gt;&gt;&gt; o1.user= cookiemon&gt;&gt;&gt; session.add(o1)&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='Change chocolate chip').one()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2890, in one raise orm_exc.NoResultFound("No row was found for one()")sqlalchemy.orm.exc.NoResultFound: No row was found for one()&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='Change cholocate chip').one()&gt;&gt;&gt; ccCookie(cookie_name='Change cholocate chip', cookie_recipe_url='http://', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; line1= LineItem(order= o1, cookie= cc, quantity= 2, extended_cost= 1.00)&gt;&gt;&gt; session.add(line1)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; order= session.query(Order).first()&gt;&gt;&gt; session.expunge(order)&gt;&gt;&gt; order.line_itemsTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\attributes.py", line 242, in __get__ return self.impl.get(instance_state(instance), dict_) File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\attributes.py", line 599, in get value = self.callable_(state, passive) File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\strategies.py", line 596, in _load_for_state (orm_util.state_str(state), self.key)sqlalchemy.orm.exc.DetachedInstanceError: Parent instance &lt;Order at 0x665f3b0&gt; is not bound to a Session; lazy load operation of attribute 'line_items' cannot proceed (Background on this error at: http://sqlalche.me/e/bhk3)&gt;&gt;&gt; Transactions12345678910111213141516171819202122232425262728293031323334353637383940414243cookiemon = User('cookiemon', 'mon@cookie.com', '111-111-1111', 'password') cc = Cookie('chocolate chip', 'http://some.aweso.me/cookie/recipe.html', 'CC01', 12, 0.50)dcc = Cookie('dark chocolate chip', 'http://some.aweso.me/cookie/recipe_dark.html', 'CC02', 1, 0.75)session.add(cookiemon)session.add(cc)session.add(dcc)o1 = Order()o1.user = cookiemonsession.add(o1)line1 = LineItem(order=o1, cookie=cc, quantity=9, extended_cost=4.50)session.add(line1)session.commit() o2 = Order()o2.user = cookiemonsession.add(o2)line1 = LineItem(order=o2, cookie=cc, quantity=2, extended_cost=1.50)line2 = LineItem(order=o2, cookie=dcc, quantity=9, extended_cost=6.75)session.add(line1)session.add(line2)session.commit() def ship_it(order_id): order = session.query(Order).get(order_id) for li in order.line_items: li.cookie.quantity = li.cookie.quantity - li.quantity session.add(li.cookie) order.shipped = True session.add(order) session.commit() print('shipped order ID: &#123;&#125;'.format(order_id)) 1234567891011121314from sqlalchemy.exc import IntegrityError def ship_it(order_id): order = session.query(Order).get(order_id) for li in order.line_items: li.cookie.quantity = li.cookie.quantity - li.quantity session.add(li.cookie) order.shipped = True session.add(order) try: session.commit() print('shipped order ID: &#123;&#125;'.format(order_id)) except IntegrityError as error: print('ERROR: &#123;!s&#125;'.format(error.orig)) session.rollback()]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C06_Defining Schema with SQLAlchemy ORM_02]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C06-Defining-Schema-with-SQLAlchemy-ORM-02%2F</url>
    <content type="text"><![CDATA[The Session 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; from sqlalchemy.orm import sessionmaker&gt;&gt;&gt; &gt;&gt;&gt; engine= create_engine('sqlite:///:memory:')&gt;&gt;&gt; Session= sessionmaker(bind= engine)&gt;&gt;&gt; session =Session()&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; from sqlalchemy.orm import sessionmaker&gt;&gt;&gt; engine = create_engine('sqlite:///:memory:')&gt;&gt;&gt; Session = sessionmaker(bind=engine)&gt;&gt;&gt; session = Session()&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; from sqlalchemy import Column, Integer, Numeric, String, DateTime, ForeignKey, Boolean&gt;&gt;&gt; from sqlalchemy.ext.declarative import declarative_base&gt;&gt;&gt; from sqlalchemy.orm import relationship, backref&gt;&gt;&gt; Base= declarative_base()&gt;&gt;&gt; class Cookie(Base):... __tablename__= 'cookies'... ... cookie_id = Column(Integer(), primary_key=True)... cookie_name = Column(String(50), index=True)... cookie_recipe_url = Column(String(255))... cookie_sku = Column(String(55))... quantity = Column(Integer())... unit_cost = Column(Numeric(12, 2))... ... def __repr__(self):... return "Cookie(cookie_name='&#123;self.cookie_name&#125;', " \... "cookie_recipe_url='&#123;self.cookie_recipe_url&#125;', " \... "cookie_sku='&#123;self.cookie_sku&#125;', " \... "quantity=&#123;self.quantity&#125;, " \... "unit_cost=&#123;self.unit_cost&#125;)".format(self=self)... &gt;&gt;&gt; class User(Base):... __tablename__ = 'users'... ... user_id = Column(Integer(), primary_key=True)... username = Column(String(15), nullable=False, unique=True)... email_address = Column(String(255), nullable=False)... phone = Column(String(20), nullable=False)... password = Column(String(25), nullable=False)... created_on = Column(DateTime(), default=datetime.now)... updated_on = Column(DateTime(), default=datetime.now, onupdate=datetime.now)... ... def __repr__(self):... return "User(username='&#123;self.username&#125;', " \... "email_address='&#123;self.email_address&#125;', " \... "phone='&#123;self.phone&#125;', " \... "password='&#123;self.password&#125;')".format(self=self)... &gt;&gt;&gt; class Order(Base):... __tablename__ = 'orders'... order_id = Column(Integer(), primary_key=True)... user_id = Column(Integer(), ForeignKey('users.user_id'))... shipped = Column(Boolean(), default=False)... ... user = relationship("User", backref=backref('orders', order_by=order_id))... ... def __repr__(self):... return "Order(user_id=&#123;self.user_id&#125;, " \... "shipped=&#123;self.shipped&#125;)".format(self=self)... ... &gt;&gt;&gt; class LineItem(Base):... __tablename__ = 'line_items'... line_item_id = Column(Integer(), primary_key=True)... order_id = Column(Integer(), ForeignKey('orders.order_id'))... cookie_id = Column(Integer(), ForeignKey('cookies.cookie_id'))... quantity = Column(Integer())... extended_cost = Column(Numeric(12, 2))... ... order = relationship("Order", backref=backref('line_items', order_by=line_item_id))... cookie = relationship("Cookie", uselist=False)... ... def __repr__(self):... return "LineItems(order_id=&#123;self.order_id&#125;, " \... "cookie_id=&#123;self.cookie_id&#125;, " \... "quantity=&#123;self.quantity&#125;, " \... "extended_cost=&#123;self.extended_cost&#125;)".format(... self=self) ... &gt;&gt;&gt; Base.metadata.create_all(engine) Inserting DataInserting a single object 1234567cc_cookie = Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)session.add(cc_cookie) session.commit() When commit() is called on the session, the cookie is actually inserted into the database. It also updates cc_cookie with the primary key of the record in the database. 12345&gt;&gt;&gt; print(cc_cookie.cookie_id)C:\FluentPython\env\lib\site-packages\sqlalchemy\sql\sqltypes.py:603: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. 'storage.' % (dialect.name, dialect.driver))1&gt;&gt;&gt; When we create the instance of the Cookie class and then add it to the session, nothing is sent to the database. It’s not until we call commit() on the session that anything is sent to the database. Multiple inserts 123456789101112131415dcc = Cookie(cookie_name='dark chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity=1, unit_cost=0.75)mol = Cookie(cookie_name='molasses', cookie_recipe_url='http://some.aweso.me/cookie/recipe_molasses.html', cookie_sku='MOL01', quantity=1, unit_cost=0.80)session.add(dcc) session.add(mol) session.flush() print(dcc.cookie_id)print(mol.cookie_id) we used the flush() method on the session instead of commit().A flush is like a commit; however, it doesn’t perform a database commit and end the transaction. Because of this, the dcc and mol instances are still connected to the session, and can be used to perform additional database tasks without triggering additional database queries. We also issue the session.flush() statement one time, even though we added multiple records into the database. This actually results in two insert statements being sent to the database inside a single transaction. The second method of inserting multiple records into the database is great when you want to insert data into the table and you don’t need to perform additional work on that data. Bulk inserting multiple records 1234567891011121314c1 = Cookie(cookie_name='peanut butter', cookie_recipe_url='http://some.aweso.me/cookie/peanut.html', cookie_sku='PB01', quantity=24, unit_cost=0.25)c2 = Cookie(cookie_name='oatmeal raisin', cookie_recipe_url='http://some.okay.me/cookie/raisin.html', cookie_sku='EWW01', quantity=100, unit_cost=1.00)&gt;&gt;&gt; session.bulk_save_objects([c1,c2])&gt;&gt;&gt; session.commit()&gt;&gt;&gt; c1.cookie_id&gt;&gt;&gt; c1 object isn’t associated with the session, and can’t refresh its cookie_id for printing. The method demonstrated is substantially faster than performing multiple individual adds and inserts.This speed does come at the expense of some features we get in the normal add and commit, such as: Relationship settings and actions are not respected or triggered. The objects are not connected to the session. Fetching primary keys is not done by default. No events will be triggered. If you are inserting multiple records and don’t need access to relationships or the inserted primary key, use bulk_save_objects or its related methods. This is especially true if you are ingesting data from an external data source such as a CSV or a large JSON document with nested arrays. Querying DataGet all the cookies 1234567&gt;&gt;&gt; cookies= session.query(Cookie).all()&gt;&gt;&gt; for cookie in cookies: print(cookie)... Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)Cookie(cookie_name='peanut butter', cookie_recipe_url='http://some.aweso.me/cookie/peanut.html', cookie_sku='PB01', quantity=24, unit_cost=0.25)Cookie(cookie_name='oatmeal raisin', cookie_recipe_url='http://some.okay.me/cookie/raisin.html', cookie_sku='EWW01', quantity=100, unit_cost=1.00)&gt;&gt;&gt; These objects are connected to the session, which means we can change them or delete them and persist that change to the database. In addition to using the query as an iterable or calling the all() method, there are many other ways of accessing the data. You can use the following methods to fetch results: first() Returns the first record object if there is one. one() Queries all the rows, and raises an exception if anything other than a single result is returned. scalar() Returns the first element of the first result, None if there is no result, or an error if there is more than one result. Controlling the Columns in the QueryTo limit the fields that are returned from a query, we need to pass in the columns we want in the query() method constructor separated by columns. _Select only cookie_name_1234&gt;&gt;&gt; session.query(Cookie.cookie_id, Cookie.cookie_name).first()(1, 'chocolate chip')&gt;&gt;&gt; OrderingHowever, if we want the list to be returned in a particular order, we can chain an order_by() statement to our select. Order by quantity ascending 1234567&gt;&gt;&gt; for cookie in session.query(Cookie).order_by(Cookie.quantity):... print('&#123;:3&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name))... 12 - chocolate chip 24 - peanut butter100 - oatmeal raisin&gt;&gt;&gt; If you want to sort in reverse or descending order, use the desc() statement. The desc() function wraps the specific column you want to sort in a descending manner. Order by quantity descending 12345678&gt;&gt;&gt; from sqlalchemy import desc&gt;&gt;&gt; for cookie in session.query(Cookie).order_by(desc(Cookie.quantity)):... print('&#123;:3&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name))... 100 - oatmeal raisin 24 - peanut butter 12 - chocolate chip&gt;&gt;&gt; The desc() function can also be used as a method on a column object, such as Cookie.quantity.desc(). However, that can be a bit more confusing to read in long statements. LimitingIn prior examples, we used the first() method to get just a single row back. While our query() gave us the one row we asked for, the actual query ran over and accessed all the results, not just the single record. If we want to limit the query, we can use array slice notation to actually issue a limit statement as part of our query. Two fewest cookie inventories 1234&gt;&gt;&gt; query= session.query(Cookie).order_by(Cookie.quantity)[:2]&gt;&gt;&gt; print([result.cookie_name for result in query])['chocolate chip', 'peanut butter']&gt;&gt;&gt; In addition to using the array slice notation, it is also possible to use the limit() statement. Two fewest cookie inventories with limit123query = session.query(Cookie).order_by(Cookie.quantity).limit(2)print([result.cookie_name for result in query]) Built-In SQL Functions and LabelsSQLAlchemy can also leverage SQL functions found in the backend database. Two very commonly used database functions are SUM() and COUNT(). To use these functions, we need to import the sqlalchemy.func module generator that makes them available. These functions are wrapped around the column(s) on which they are operating. Summing our cookies 12345&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; inv_count= session.query(func.sum(Cookie.quantity)).scalar()&gt;&gt;&gt; inv_count136&gt;&gt;&gt; Notice the use of scalar, which will return only the leftmost column in the first record. Counting our inventory records 123&gt;&gt;&gt; rec_count= session.query(func.count(Cookie.cookie_name)).first()&gt;&gt;&gt; rec_count(3,) Using functions such as count() and sum() will end up returning tuples or results with column names like count_1. These types of returns are often not what we want. Also, if we have several counts in a query we’d have to know the occurrence number in the statement, and incorporate that into the column name, so the fourth count() function would be count_4. This simply is not as explicit and clear as we should be in our naming, especially when surrounded with other Python code. Renaming our count column 123456&gt;&gt;&gt; rec_count= session.query(func.count(Cookie.cookie_name).label('inventory_count')).first()&gt;&gt;&gt; rec_count.keys()['inventory_count']&gt;&gt;&gt; rec_count.inventory_count3&gt;&gt;&gt; FilteringFiltering queries is done by appending filter() statements to our query. A typical filter() clause has a column, an operator, and a value or column. It is possible to chain multiple filters() clauses together or comma separate multiple ClauseElement expressions in a single filter, and they will act like ANDs in traditional SQL statements. Filtering by cookie name with filter 1234&gt;&gt;&gt; record = session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').first()&gt;&gt;&gt; print(record)Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; There is also a filter_by() method that works similarly to the filter() method except instead of explicity providing the class as part of the filter expression it uses attribute keyword expressions from the primary entity of the query or the last entity that was joined to the statement. It also uses a keyword assignment instead of a Boolean. _Filtering by cookie name with filter_by_ 1234&gt;&gt;&gt; record= session.query(Cookie).filter_by(cookie_name='chocolate chip').first()&gt;&gt;&gt; print(record)Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; Finding names with “chocolate” in them 1234567&gt;&gt;&gt; query= session.query(Cookie).filter(Cookie.cookie_name.like('%chocolate%'))&gt;&gt;&gt; for record in query:... print(record.cookie_name)... chocolate chipdark chocolate chip&gt;&gt;&gt; we are using the Cookie.cookie_name column inside of a filter statement as a type of ClauseElement to filter our results, and we are taking advantage of the like() method that is available on ClauseElements. There are many other methods available. If we don’t use one of the ClauseElement methods, then we will have an operator in our filter clauses. OperatorsString concatenation with + 123456789&gt;&gt;&gt; results= session.query(Cookie.cookie_name, 'SKU-'+Cookie.cookie_sku).all()&gt;&gt;&gt; for row in results:print(row)... ('chocolate chip', 'SKU-CC01')('dark chocolate chip', 'SKU-CC02')('molasses', 'SKU-MOL01')('peanut butter', 'SKU-PB01')('oatmeal raisin', 'SKU-EWW01')&gt;&gt;&gt; Inventory value by cookie 1234567891011121314&gt;&gt;&gt; from sqlalchemy import cast&gt;&gt;&gt; query= session.query(Cookie.cookie_name, cast(... (Cookie.quantity * Cookie.unit_cost),Numeric(12,2)... ).label('inv_cost') # using the label() function to rename the column... )... &gt;&gt;&gt; for result in query:print('&#123;&#125;- &#123;&#125;'.format(result.cookie_name, result.inv_cost))... chocolate chip- 6.00dark chocolate chip- 0.75molasses- 0.80peanut butter- 6.00oatmeal raisin- 100.00&gt;&gt;&gt; cast is a function that allows us to convert types. In this case, we will be getting back results such as 6.0000000000, so by casting it, we can make it look like currency. It is also possible to accomplish the same task in Python with print(‘{} -{:.2f}’.format(row.cookie_name, row.inv_cost)). If we need to combine where statements, we can use a couple of different methods. One of those methods is known as Boolean operators. Boolean Operatorswhen you write A &lt; B &amp; C &lt; D, what you are actually writing is A &lt; (B&amp;C) &lt; D, when you probably intended to get (A &lt; B) &amp;(C &lt; D). Often we want to chain multiple where clauses together in inclusive and exclusionary manners; this should be done via conjunctions. ConjunctionsWhile it is possible to chain multiple filter() clauses together, it’s often more readable and functional to use conjunctions to accomplish the desired effect. I also prefer to use conjunctions instead of Boolean operators, as conjunctions will make your code more expressive. The conjunctions in SQLAlchemy are and_(), or_(), and not_(). They have underscores to separate them from the built-in keywords. Using the or() conjunction 123456789101112131415161718&gt;&gt;&gt; query= session.query(Cookie).filter(... Cookie.quantity &gt; 23,... Cookie.unit_cost &lt; 0.4)... &gt;&gt;&gt; from sqlalchemy import and_, or_, not_&gt;&gt;&gt; query = session.query(Cookie).filter(... or_(... Cookie.quantity.between(10, 50),... Cookie.cookie_name.contains('chip')... )... )... &gt;&gt;&gt; for result in query: print(result.cookie_name)... chocolate chipdark chocolate chippeanut butter&gt;&gt;&gt; Updating Data Updating data via object 123456&gt;&gt;&gt; query=session.query(Cookie)&gt;&gt;&gt; cc_cookie= query.filter(Cookie.cookie_name=='chocolate chip').first()&gt;&gt;&gt; cc_cookie.quantity = cc_cookie.quantity + 120&gt;&gt;&gt; session.commit()&gt;&gt;&gt; print(cc_cookie.quantity)132 Updating data in place 1234567&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query=query.filter(Cookie.cookie_name=='chocolate chip')&gt;&gt;&gt; query.update(&#123;Cookie.quantity: Cookie.quantity - 20&#125;)1&gt;&gt;&gt; cc_cookie= query.first()&gt;&gt;&gt; print(cc_cookie.quantity)112 The update() method causes the record to be updated outside of the session, and returns the number of rows updated. Deleting DataTo create a delete statement, you can use either the delete() function or the delete() method on the table from which you are deleting data. Unlike insert() and update(), delete() takes no values parameter, only an optional where clause (omitting the where clause will delete all rows from the table). Deleting data 12345678&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query= query.filter(Cookie.cookie_name=='dark chocolate chip')&gt;&gt;&gt; dcc_cookie=query.one()&gt;&gt;&gt; session.delete(dcc_cookie)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; dcc_cookie= query.first()&gt;&gt;&gt; print(dcc_cookie)None It is also possible to delete data in place without having the object 1234567&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query= query.filter(Cookie.cookie_name=='molasses')&gt;&gt;&gt; query.delete()1&gt;&gt;&gt; mol_cookie= query.first()&gt;&gt;&gt; print(mol_cookie)None Adding related objects 12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; cookiemon = User(username='cookiemon',... email_address='mon@cookie.com',... phone='111-111-1111',... password='password')... &gt;&gt;&gt; cakeeater = User(username='cakeeater',... email_address='cakeeater@cake.com',... phone='222-222-2222',... password='password')... &gt;&gt;&gt; pieperson = User(username='pieperson',... email_address='person@pie.com',... phone='333-333-3333',... password='password')... &gt;&gt;&gt; session.add(cookiemon)&gt;&gt;&gt; session.add(cakeeater)&gt;&gt;&gt; session.add(pieperson)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; o1=Order()&gt;&gt;&gt; o1.user= cookiemon&gt;&gt;&gt; session.add(o1)&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').one()&gt;&gt;&gt; line1= LineItem(cookie= cc, quantity= 2, extended_cost= 1.00)&gt;&gt;&gt; pb= session.query(Cookie ).filter(Cookie.cookie_name=='peanut butter').one()&gt;&gt;&gt; line2= LineItem(quantity= 12, extended_cost= 3.00)&gt;&gt;&gt; line2.cookie= pb&gt;&gt;&gt; line2.order = o1&gt;&gt;&gt; o1.line_items.append(line1)&gt;&gt;&gt; o1.line_items.append(line2)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; o2=Order()&gt;&gt;&gt; o2.user= cakeeater&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').one()&gt;&gt;&gt; line1= LineItem(cookie=cc, quantity= 24, extended_cost=12.00)&gt;&gt;&gt; oat= session.query(Cookie).filter(Cookie.cookie_name=='oatmeal raisin').one()&gt;&gt;&gt; line2= LineItem(cookie=oat, quantity= 6, extended_cost=6.00)&gt;&gt;&gt; o2.line_items.append(line1)&gt;&gt;&gt; o2.line_items.append(line2)&gt;&gt;&gt; session.add(o2)&gt;&gt;&gt; session.commit() Joinslet’s use the join() and outerjoin() methods to take a look at how to query related data. For example, to fulfill the order placed by the cookiemon user, we need to determine how many of each cookie type were ordered. This requires you to use a total of three joins to get all the way down to the name of the cookies. Using join to select from multiple tables 123456789 Cookie.cookie_name, LineItem.quantity, LineItem.extended_cost)query = query.join(User).join(LineItem).join(Cookie)results = query.filter(User.username == 'cookiemon').all()print(results)[ (u'1', u'cookiemon', u'111-111-1111', u'chocolate chip', 2, Decimal('1.00')) (u'1', u'cookiemon', u'111-111-1111', u'peanut butter', 12, Decimal('3.00'))] Using outerjoin to select from multiple tables 1234567891011&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; query=session.query(User.username, func.count(Order.order_id))&gt;&gt;&gt; query=query.outerjoin(Order).group_by(User.username)&gt;&gt;&gt; print(query)SELECT users.username AS users_username, count(orders.order_id) AS count_1 FROM users LEFT OUTER JOIN orders ON users.user_id = orders.user_id GROUP BY users.username&gt;&gt;&gt; for row in query: print(row)... ('cakeeater', 1)('cookiemon', 0)('pieperson', 0) However, what if we have a self-referential table like a table of managers and their reports? The ORM allows us to establish a relationship that points to the same table; however, we need to specify an option called remote_side to make the relationship a many to one: 1234567&gt;&gt;&gt; class Employee(Base):... __tablename__='employees'... id= Column(Integer(), primary_key= True)... manager_id= Column(Integer(), ForeignKey('employees.id'))... name= Column(String(255), nullable= False)... manager= relationship('Employee',backref=backref('reports'), remote_side=[id])... Establishes a relationship back to the same table, specifies the remote_side, and makes the relationship a many to one. add an employee and another employee that reports to her: 12345marsha = Employee(name='Marsha')fred = Employee(name='Fred')marsha.reports.append(fred)session.add(marsha)session.commit() print the employees that report to Marsha, we would do so by accessing the reports property as follows: 12for report in marsha.reports: print(report.name) GroupingWhen using grouping, you need one or more columns to group on and one or morecolumns that it makes sense to aggregate with counts, sums, etc., as you would innormal SQL. 1234query = session.query(User.username, func.count(Order.order_id))query = query.outerjoin(Order).group_by(User.username)for row in query: print(row) Chaining1234567891011def get_orders_by_customer(cust_name): query = session.query(Order.order_id, User.username, User.phone, Cookie.cookie_name, LineItem.quantity, LineItem.extended_cost) query = query.join(User).join(LineItem).join(Cookie) results = query.filter(User.username == cust_name).all() return resultsget_orders_by_customer('cakeeater')[(u'2', u'cakeeater', u'222-222-2222', u'chocolate chip', 24, Decimal('12.00')),(u'2', u'cakeeater', u'222-222-2222', u'oatmeal raisin', 6, Decimal('6.00'))] Conditional chaining 12345678910111213141516171819202122232425&gt;&gt;&gt; def get_orders_by_customer(cust_name, shipped= None, details= False):... query= session.query(Order.order_id, User.username, User.phone)... ... query= query.join(User)... if details:... query= query.add_columns(Cookie.cookie_name,LineItem.quantity,... LineItem.extended_cost... )... query=query.join(LineItem).join(Cookie)... if shipped is not None:... query= query.where(Order.shipped == shipped)... results= query.filter(User.username== cust_name).all()... return results... &gt;&gt;&gt; &gt;&gt;&gt; query= session.query(Order.order_id, User.username, User.phone)&gt;&gt;&gt; query= query.join(User)&gt;&gt;&gt; print(query)SELECT orders.order_id AS orders_order_id, users.username AS users_username, users.phone AS users_phone FROM orders JOIN users ON users.user_id = orders.user_id&gt;&gt;&gt; query=query.join(LineItem).join(Cookie)&gt;&gt;&gt; print(query)SELECT orders.order_id AS orders_order_id, users.username AS users_username, users.phone AS users_phone FROM orders JOIN users ON users.user_id = orders.user_id JOIN line_items ON orders.order_id = line_items.order_id JOIN cookies ON cookies.cookie_id = line_items.cookie_id&gt;&gt;&gt; Raw Queries12345from sqlalchemy import textquery = session.query(User).filter(text("username='cookiemon'"))print(query.all())[User(username='cookiemon', email_address='mon@cookie.com', phone='111-111-1111', password='password')]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C06_Defining Schema with SQLAlchemy ORM]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C06-Defining-Schema-with-SQLAlchemy-ORM%2F</url>
    <content type="text"><![CDATA[In SQLAlchemy Core, we created a metadata container and then declared a Table object associated with that metadata. In SQLAlchemy ORM, we are going to define a class that inherits from a special base class called the declarative_base. The declarative_base combines a metadata container and a mapper that maps our class to a database table. It also maps instances of the class to records in that table if they have been saved. Inherit from the declarative_base object. Contain __tablename__, which is the table name to be used in the database. Contain one or more attributes that are Column objects. Ensure one or more attributes make up a primary key. Defining Tables via ORM Classes 12345678910111213141516171819202122232425262728293031from sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Table, Column, Integer, Numeric, String, BooleanBase = declarative_base()class Cookie(Base): __tablename__ = 'cookies' cookie_id = Column(Integer(), primary_key=True) cookie_name = Column(String(50), index=True) cookie_recipe_url = Column(String(255)) cookie_sku = Column(String(55)) quantity = Column(Integer()) unit_cost = Column(Numeric(12, 2))Cookie.__table__Table('cookies', MetaData(bind=None), Column('cookie_id', Integer(), table=&lt;cookies&gt;, primary_key=True, nullable=False), Column('cookie_name', String(length=50), table=&lt;cookies&gt;), Column('cookie_recipe_url', String(length=255), table=&lt;cookies&gt;), Column('cookie_sku', String(length=55), table=&lt;cookies&gt;), Column('quantity', Integer(), table=&lt;cookies&gt;), Column('unit_cost', Numeric(precision=12, scale=2), table=&lt;cookies&gt;), schema=None)from datetime import datetimefrom sqlalchemy import DateTimeclass User(Base): __tablename__ = 'users' user_id = Column(Integer(), primary_key=True) username = Column(String(15), nullable=False, unique=True) email_address = Column(String(255), nullable=False) phone = Column(String(20), nullable=False) password = Column(String(25), nullable=False) created_on = Column(DateTime(), default=datetime.now) updated_on = Column(DateTime(), default=datetime.now, onupdate=datetime.now) Keys, Constraints, and IndexesWhen using the ORM, we are building classes and not using the table constructor. In the ORM, these can be added by using the __table_args__ attribute on our class. __table_args__ expects to get a tuple of additional table arguments. 123456class SomeDataClass(Base): __tablename__ = 'somedatatable' __table_args__ = (ForeignKeyConstraint(['id'], ['other_table.id']), CheckConstraint(unit_cost &gt;= 0.00', name='unit_cost_positive')) RelationshipsThe ORM uses a similar ForeignKey column to constrain and link the objects; however, it also uses a relationship directive to provide a property that can be used to access the related object. Table with a relationship 12345678910&gt;&gt;&gt; from sqlalchemy import ForeignKey, Boolean&gt;&gt;&gt; from sqlalchemy.orm import relationship, backref&gt;&gt;&gt; class Order(Base):... __tablename__='orders'... order_id = Column(Integer(), primary_key= True)... user_id = Column(Integer(), ForeignKey('users.user_id'))... shipped= Column(Boolean(), default=False)... user= relationship('User', backref=backref('orders', order_by=order_id))... &gt;&gt;&gt; Looking at the user relationship defined in the Order class, it establishes a one-to-many relationship with the User class. We can get the User related to this Order by accessing the user property. This relationship also establishes an orders property on the User class via the backref keyword argument, which is ordered by the order_id. The relationship directive needs a target class for the relationship, and can optionally include a back reference to be added to target class. SQLAlchemy knows to use the ForeignKey we defined that matches the class we defined in the relationship. In the preceding example, the ForeignKey(users.user_id), which has the users table’s user_id column, maps to the User class via the __tablename__ attribute of users and forms the relationship. More tables with relationships 12345678910&gt;&gt;&gt; class LineItem(Base):... __tablename__='line_items'... line_item_id= Column(Integer(), primary_key= True)... order_id= Column(Integer(),ForeignKey('orders.order_id'))... cookie_id=Column(Integer(), ForeignKey('cookies.cookie_id'))... quantity= Column(Integer())... extended_cost=Column(Numeric(12,2))... order = relationship('Order',backref=backref('line_items', order_by= line_item_id))... cookie=relationship('Cookie', uselist=False) # This establishes a one-to-one relationship.... Persisting the SchemaTo create our database tables, we are going to use the create_all method on the metadata within our Base instance. It requires an instance of an engine, just as it did in SQLAlchemy Core: 123&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; engine= create_engine('sqlite:///:memory:')&gt;&gt;&gt; Base.metadata.create_all(engine)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C05_Reflection]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C05-Reflection%2F</url>
    <content type="text"><![CDATA[Reflection is a technique that allows us to populate a SQLAlchemy object from an existing database. Reflecting Individual TablesFor our first reflection, we are going to generate the Artist table. We’ll need a metadata object to hold the reflected table schema information, and an engine attached to the Chinook database. Setting up our initial objects123from sqlalchemy import MetaData, create_enginemetadata = MetaData()engine = create_engine('sqlite:///Chinook_Sqlite.sqlite') Instead of defining the columns by hand, we are going to use the autoload and autoload_with keyword arguments. This will reflect the schema information into the metadata object and store a reference to the table in the artist variable. Reflecting the Artist table 12from sqlalchemy import Tableartist = Table('Artist', metadata, autoload=True, autoload_with=engine) Using the Artist table 1234567891011121314artist.columns.keys() from sqlalchemy import selects = select([artist]).limit(10) engine.execute(s).fetchall()[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')] Viewing the metadata123456789album = Table('Album', metadata, autoload=True, autoload_with=engine)metadata.tables['album']Table('album', MetaData(bind=None), Column('AlbumId', INTEGER(), table=&lt;album&gt;, primary_key=True, nullable=False), Column('Title', NVARCHAR(length=160), table=&lt;album&gt;, nullable=False), Column('ArtistId', INTEGER(), table=&lt;album&gt;, nullable=False), schema=None)) the foreign key to the Artist table does not appear to have been reflected. 12in: album.foreign_keysout: set() add the missing ForeignKey, and restore the relationship:12345from sqlalchemy import ForeignKeyConstraintalbum.append_constraint( ForeignKeyConstraint(['ArtistId'], ['artist.ArtistId'])) use the relationship to join the tables properly 12str(artist.join(album))'artist JOIN album ON artist.'ArtistId' = album.'ArtistId'' Reflecting a Whole DatabaseIn order to reflect a whole database, we can use the reflect method on the metadata object. The reflect method will scan everything available on the engine supplied, and reflect everything it can. 12345metadata.reflect(bind=engine)metadata.tables.keys()dict_keys(['InvoiceLine', 'Employee', 'Invoice', 'album', 'Genre', 'PlaylistTrack', 'Album', 'Customer', 'MediaType', 'Artist', 'Track', 'artist', 'Playlist']) Query Building with Reflected ObjectsUsing a reflected table in query 1234567891011121314playlist = metadata.tables['Playlist'] from sqlalchemy import selects = select([playlist]).limit(10) engine.execute(s).fetchall()[(1, 'Music'), (2, 'Movies'), (3, 'TV Shows'), (4, 'Audiobooks'), (5, '90’s Music'), (6, 'Audiobooks'), (7, 'Movies'), (8, 'Music'), (9, 'Music Videos'), (10, 'TV Shows')]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C04_Testing]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C04-Testing%2F</url>
    <content type="text"><![CDATA[db.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141from datetime import datetimefrom sqlalchemy import MetaData, Table, Column, Integer, Numeric, String, DateTime, ForeignKey, Boolean, create_enginefrom sqlalchemy.sql import insertclass DataAccessLayer: connection = None engine = None conn_string = None metadata= MetaData() cookies= Table('cookies', metadata, Column('cookie_id',Integer(), primary_key= True), Column('cookie_name',String(50), index= True), Column('cookie_recipe_url',String(255)), Column('cookie_sku', String(55)), Column('quantity', Integer()), Column('unit_cost',Numeric(12,2)) ) users = Table('users', metadata, Column('user_id', Integer(), primary_key=True), Column('customer_number', Integer(), autoincrement=True), Column('username', String(15), nullable=False, unique=True), Column('email_address', String(255), nullable=False), Column('phone', String(20), nullable=False), Column('password', String(25), nullable=False), Column('created_on', DateTime(), default=datetime.now), Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now) ) orders = Table('orders', metadata, Column('order_id', Integer()), Column('user_id', ForeignKey('users.user_id')), Column('shipped', Boolean(), default=False) ) line_items = Table('line_items', metadata, Column('line_items_id', Integer(), primary_key=True), Column('order_id', ForeignKey('orders.order_id')), Column('cookie_id', ForeignKey('cookies.cookie_id')), Column('quantity', Integer()), Column('extended_cost', Numeric(12, 2))) def db_init(self, conn_string): self.engine= create_engine(conn_string or self.conn_string) self.metadata.create_all(self.engine) self.connection= self.engine.connect()dal= DataAccessLayer()def prep_db(): ins= dal.cookies.insert() dal.connection.execute(ins, cookie_name='dark chocolate ship', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity='1', unit_cost='0.75') inventory_list=[ &#123; 'cookie_name': 'peanut butter', 'cookie_recipe_url': 'http://some.aweso.me/cookie/peanut.html', 'cookie_sku': 'PB01', 'quantity': '24', 'unit_cost': '0.25' &#125;, &#123; 'cookie_name': 'oatmeal raisin', 'cookie_recipe_url': 'http://some.okay.me/cookie/raisin.html', 'cookie_sku': 'EWW01', 'quantity': '100', 'unit_cost': '1.00' &#125; ] dal.connection.execute(ins, inventory_list) customer_list=[ &#123; 'username': 'cookiemon', 'email_address': 'mon@cookie.com', 'phone': '111-111-1111', 'password': 'password' &#125;, &#123; 'username': 'cakeeater', 'email_address': 'cakeeater@cake.com', 'phone': '222-222-2222', 'password': 'password' &#125;, &#123; 'username': 'pieguy', 'email_address': 'guy@pie.com', 'phone': '333-333-3333', 'password': 'password' &#125; ] ins=dal.users.insert() dal.connection.execute(ins, customer_list) ins= insert(dal.orders).values(user_id=1, order_id='wlk001') dal.connection.execute(ins) ins=insert(dal.line_items) order_items=[ &#123; 'order_id': 'wlk001', 'cookie_id': 1, 'quantity': 2, 'extended_cost': 1.00 &#125;, &#123; 'order_id': 'wlk001', 'cookie_id': 3, 'quantity': 12, 'extended_cost': 3.00 &#125; ] dal.connection.execute(ins, order_items) ins= insert(dal.orders).values(user_id =2, order_id='o1001') dal.connection.execute(ins) ins=insert(dal.line_items) order_items=[ &#123; 'order_id': 'ol001', 'cookie_id': 1, 'quantity': 24, 'extended_cost': 12.00 &#125;, &#123; 'order_id': 'ol001', 'cookie_id': 4, 'quantity': 6, 'extended_cost': 6.00 &#125; ] dal.connection.execute(ins, order_items) app.py 1234567891011121314151617181920212223242526272829from db import dalfrom sqlalchemy.sql import selectdef get_orders_by_customer(cust_name, shipped=None, details=False): columns=[ dal.orders.c.order_id, dal.users.c.username, dal.users.c.phone ] joins=dal.users.join(dal.orders) if details: columns.extend([ dal.cookies.c.cookie_name, dal.line_items.c.quantity, dal.line_items.c.extended_cost ]) joins=joins.join(dal.line_items).join(dal.cookies) cust_orders=select(columns).select_from( joins ).where( dal.users.c.username == cust_name ) if shipped is not None: cust_orders=cust_orders.where( dal.orders.c.shipped == shipped ) return dal.connection.execute(cust_orders).fetchall() _test_app.py_ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import unittestfrom decimal import Decimalfrom db import dal, prep_dbfrom app import get_orders_by_customerclass TestApp(unittest.TestCase): @classmethod def setUpClass(cls): dal.db_init('sqlite:///:memory:') prep_db() def test_orders_by_customer_blank(self): results= get_orders_by_customer('') self.assertEqual(results,[]) def test_orders_by_customer_blank_shipped(self): results=get_orders_by_customer('',True) self.assertEqual(results, []) def test_orders_by_customer_blank_notshipped(self): results = get_orders_by_customer('', False) self.assertEqual(results, []) def test_orders_by_customer_blank_details(self): results = get_orders_by_customer('', details=True) self.assertEqual(results, []) def test_orders_by_customer_blank_shipped_details(self): results = get_orders_by_customer('', True, True) self.assertEqual(results, []) def test_orders_by_customer_blank_notshipped_details(self): results = get_orders_by_customer('', False, True) self.assertEqual(results, []) def test_orders_by_customer(self): expected_results = [(u'wlk001', u'cookiemon', u'111-111-1111')] results = get_orders_by_customer('cookiemon') self.assertEqual(results, expected_results) def test_orders_by_customer_shipped_only(self): results = get_orders_by_customer('cookiemon', True) self.assertEqual(results, []) def test_orders_by_customer_unshipped_only(self): expected_results = [(u'wlk001', u'cookiemon', u'111-111-1111')] results = get_orders_by_customer('cookiemon', False) self.assertEqual(results, expected_results) def test_orders_by_customer_with_details(self): expected_results = [ (u'wlk001', u'cookiemon', u'111-111-1111', u'dark chocolate chip', 2, Decimal('1.00')), (u'wlk001', u'cookiemon', u'111-111-1111', u'oatmeal raisin', 12, Decimal('3.00')) ] results = get_orders_by_customer('cookiemon', details=True) self.assertEqual(results, expected_results) def test_orders_by_customer_shipped_only_with_details(self): results = get_orders_by_customer('cookiemon', True, True) self.assertEqual(results, []) def test_orders_by_customer_unshipped_only_details(self): expected_results = [ (u'wlk001', u'cookiemon', u'111-111-1111', u'dark chocolate chip', 2, Decimal('1.00')), (u'wlk001', u'cookiemon', u'111-111-1111', u'oatmeal raisin', 12, Decimal('3.00')) ] results = get_orders_by_customer('cookiemon', False, True) self.assertEqual(results, expected_results)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C03_Exceptions and Transaction]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C03-Exceptions-and-Transaction%2F</url>
    <content type="text"><![CDATA[AttributeErrorAttributeErrors occur when you try to access an attribute of an object that isn’t present on that object.Causing an AttributeError 1234567891011121314151617&gt;&gt;&gt; from sqlalchemy import select, insert&gt;&gt;&gt; ins= insert(users).values(... username='cookiemon',... email_address='mon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; result=connection.execute(ins)&gt;&gt;&gt; s=select([users.c.username])&gt;&gt;&gt; results=connection.execute(s)&gt;&gt;&gt; for result in results:... print(result.username,result.password)... Traceback (most recent call last): File "&lt;stdin&gt;", line 2, in &lt;module&gt;AttributeError: Could not locate column in row for column 'password' In this case, it is because our row from the ResultProxy does not have a password column. We only queried for the username. IntegrityErrorAnother common SQLAlchemy error is the IntegrityError, which occurs when we try to do something that would violate the constraints configured on a Column or Table. 1234567891011121314&gt;&gt;&gt; s=select([users.c.username])&gt;&gt;&gt; connection.execute(s).fetchall()[('cookiemon',)]&gt;&gt;&gt; ins= insert(users).values(... username= 'cookiemon',... email_address='damon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; results=connection.execute(ins)Traceback (most recent call last):...sqlite3.IntegrityError: UNIQUE constraint failed: users.username handling ErrorsCatching an exception 123456789101112&gt;&gt;&gt; from sqlalchemy.exc import IntegrityError&gt;&gt;&gt; ins= insert(users).values(... username= 'cookiemon',... email_address='damon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; try: result= connection.execute(ins)... except IntegrityError as error:... print(error.orig.message, error.params)... TransactionsWhen we start a transaction, we record the current state of our database; then we can execute multiple SQL statements. If all the SQL statements in the transaction succeed, the database continues on normally and we discard the prior database state. However, if one or more of those statements fail, we can catch that error and use the prior state to roll back back any statements that succeeded. Transactions are initiated by calling the begin() method on the connection object. The result of this call is a transaction object that we can use to control the result of all our statements. If all our statements are successful, we commit the transaction by calling the commit() method on the transaction object. If not, we call the rollback() method on that same object. Let’s rewrite the ship_it function to use a transaction to safely execute our statements. _Transactional ship_it_123456789101112131415161718192021from sqlalchemy.exc import IntegrityError def ship_it(order_id): s = select([line_items.c.cookie_id, line_items.c.quantity]) s = s.where(line_items.c.order_id == order_id) transaction = connection.begin() cookies_to_ship = connection.execute(s).fetchall() try: for cookie in cookies_to_ship: u = update(cookies).where(cookies.c.cookie_id == cookie.cookie_id) u = u.values(quantity = cookies.c.quantity-cookie.quantity) result = connection.execute(u) u = update(orders).where(orders.c.order_id == order_id) u = u.values(shipped=True) result = connection.execute(u) print('Shipped order ID: &#123;&#125;'.format(order_id)) transaction.commit() except IntegrityError as error: transaction.rollback() print(error)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C02_Built-In SQL Functions and Labels]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C02-Built-In-SQL-Functions-and-Labels%2F</url>
    <content type="text"><![CDATA[Two very commonly used database functions are SUM() and COUNT(). To use these functions, we need to import the sqlalchemy.sql.func module where they are found. These functions are wrapped around the column(s) on which they are operating.Summing our cookies 12345from sqlalchemy.sql import funcs= select([func.sum(cookies.c.quantity)])rp= connection.execute(s)print(rp.scalar())# Notice the use of scalar, which will return only the leftmost column in the first record.This results in: 137 Counting our inventory records 12345s= select([func.count(cookies.c.cookie_name)])rp= connection.execute(s)record= rp.first()print(record.keys())print(record.count_1) # The column name is autogenerated and is commonly &lt;func_name&gt;_&lt;position&gt; Renaming our count column123456s= select([func.count(cookies.c.cookie_name).label('inventory_count')])rp= connection.execute(s)record= rp.first()print(record.keys())print(record.inventory_count) FilteringFiltering queries is done by adding where() statements just like in SQL. A typical where() clause has a column, an operator, and a value or column. It is possible to chain multiple where() clauses together, and they will act like ANDs in traditional SQL statements. 12345s= select([cookies]).where( cookies.c.cookie_name == 'chocolate chip')rp= connection.execute(s)record= rp.first()print(record.items())# calling the items() method on the row object, which will give a list of columns and values. Finding names with chocolate in them 1234s= select([cookies]).where( cookies.c.cookie_name.like('%chocolate%'))rp= connection.execute(s)for record in rp.fetchall(): print(record.cookie_name) String concatenation with +123s= select([cookies.c.cookie_name, 'SKU-'+ cookies.c.cookie_sku])for row in connection.execute(s): print(row) Inventory value by cookie 123456from sqlalchemy import casts= select([... cookies.c.cookie_name,... cast((cookies.c.quantity* cookies.c.unit_cost),Numeric(12,2)).label('inv_cost')... ])for row in connection.execute(s): print('&#123;&#125; - &#123;&#125;'.format(row.cookie_name, row.inv_cost)) Using the and() conjunction 12345678&gt;&gt;&gt; from sqlalchemy import and_, or_, not_&gt;&gt;&gt; s= select([cookies]).where(... and_(... cookies.c.quantity &gt; 23,... cookies.c.unit_cost &lt; 0.40... )... )for row in connection.execute(s): print(row.cookie_name) Using the or() conjunction 12345678from sqlalchemy import and_, or_, not_&gt;&gt;&gt; s= select([cookies]).where(... or_(... cookies.c.quantity.between(10, 50),... cookies.c.cookie_name.contains('chip')... )... )for row in connection.execute(s): print(row.cookie_name) Updating DataUpdating data123456789from sqlalchemy import updateu= update(cookies).where(cookies.c.cookie_name == 'chocolate chip')u=u.values(quantity=(cookies.c.quantity + 120))result= connection.execute(u)print(result.rowcount)s=select([cookies]).where(cookies.c.cookie_name == 'chocolate chip')result= connection.execute(s).first()for key in result.keys(): print('&#123;:&gt;20&#125;: &#123;&#125;'.format(key, result[key])) Deleting Data Deleting data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from sqlalchemy import deleteu=delete(cookies).where(cookies.c.cookie_name == 'dark chocolate chip')result= connection.execute(u)print(result.rowcount)customer_list = [ &#123; 'username': 'cookiemon', 'email_address': 'mon@cookie.com', 'phone': '111-111-1111', 'password': 'password' &#125;, &#123; 'username': 'cakeeater', 'email_address': 'cakeeater@cake.com', 'phone': '222-222-2222', 'password': 'password' &#125;, &#123; 'username': 'pieguy', 'email_address': 'guy@pie.com', 'phone': '333-333-3333', 'password': 'password' &#125;]ins = users.insert()result = connection.execute(ins, customer_list)# Now that we have customers, we can start to enter their orders and line items into the system as well:ins= insert(orders).values(user_id =1, order_id =1)result= connection.execute(ins)ins= insert(line_items)order_items = [ &#123; 'order_id': 1, 'cookie_id': 1, 'quantity': 2, 'extended_cost': 1.00 &#125;, &#123; 'order_id': 1, 'cookie_id': 3, 'quantity': 12, 'extended_cost': 3.00 &#125;]result= connection.execute(ins, order_items)ins= insert(orders).values(user_id =2, order_id = 2)result- connection.execute(ins)ins=insert(line_items)order_items = [ &#123; 'order_id': 2, 'cookie_id': 1, 'quantity': 24, 'extended_cost': 12.00 &#125;, &#123; 'order_id': 2, 'cookie_id': 4, 'quantity': 6, 'extended_cost': 6.00 &#125;]result= connection.execute(ins, order_items) Joins12345678910111213141516171819&gt;&gt;&gt; columns=[orders.c.order_id,... users.c.username,... users.c.phone,... cookies.c.cookie_name,... line_items.c.quantity,... line_items.c.extended_cost... ]&gt;&gt;&gt; cookiemon_orders=select(columns)... &gt;&gt;&gt; &gt;&gt;&gt; cookiemon_orders=cookiemon_orders.select_from(... orders.join(users).join(line_items).join(cookies)... ).where(users.c.username == 'cookiemon')... &gt;&gt;&gt; print(cookiemon_orders)SELECT orders.order_id, users.username, users.phone, cookies.cookie_name, line_items.quantity, line_items.extended_cost FROM orders JOIN users ON users.user_id = orders.user_id JOIN line_items ON orders.order_id = line_items.order_id JOIN cookies ON cookies.cookie_id = line_items.cookie_id WHERE users.username = :username_1&gt;&gt;&gt; using outerjoin to select from multiple tables 12345678910111213141516&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; columns=[... users.c.username,... func.count(orders.c.order_id)... ]... &gt;&gt;&gt; all_orders=select(columns).select_from(... users.outerjoin(orders)... ).group_by(users.c.username)... # SQLAlchemy knows how to join the users and orders tables because of the foreign key defined in the orders table.&gt;&gt;&gt; print(all_orders)SELECT users.username, count(orders.order_id) AS count_1 FROM users LEFT OUTER JOIN orders ON users.user_id = orders.user_id GROUP BY users.username AliasesWhen using joins, it is often necessary to refer to a table more than once. In SQL, this is accomplished by using aliases in the query. For instance, suppose we have the following (partial) schema that tracks the reporting structure within an organization:123456employee_table = Table( 'employee', metadata, Column('id', Integer, primary_key=True), Column('manager', None, ForeignKey('employee.id')), Column('name', String(255))) Now suppose we want to select all the employees managed by an employee named Fred. In SQL, we might write the following:12345SELECT employee.nameFROM employee, employee AS managerWHERE employee.manager_id = manager.id AND manager.name = 'Fred' SQLAlchemy also allows the use of aliasing selectables in this type of situation via the alias() function or method: 12345678&gt;&gt;&gt; manager = employee_table.alias('mgr')&gt;&gt;&gt; stmt = select([employee_table.c.name],... and_(employee_table.c.manager_id==manager.c.id,... manager.c.name=='Fred'))&gt;&gt;&gt; print(stmt)SELECT employee.nameFROM employee, employee AS mgrWHERE employee.manager_id = mgr.id AND mgr.name = ? SQLAlchemy can also choose the alias name automatically, which is useful for guaranteeing that there are no name collisions: 12345678&gt;&gt;&gt; manager = employee_table.alias()&gt;&gt;&gt; stmt = select([employee_table.c.name],... and_(employee_table.c.manager_id==manager.c.id,... manager.c.name=='Fred'))&gt;&gt;&gt; print(stmt)SELECT employee.nameFROM employee, employee AS employee_1WHERE employee.manager_id = employee_1.id AND employee_1.name = ? GroupingWhen using grouping, you need one or more columns to group on and one or more columns that it makes sense to aggregate with counts, sums, etc. Grouping data 12345678&gt;&gt;&gt; columns=[... users.c.username,... func.count(orders.c.order_id)... ]... &gt;&gt;&gt; all_orders=select(columns).select_from(users.outerjoin(orders)).group_by(... users.c.username... ) ChainingChaining 1234567891011121314151617&gt;&gt;&gt; def get_orders_by_customer(cust_name):... columns=[... orders.c.order_id,... users.c.username,... usere.c.phone,... cookies.c.cookie_name,... line_items.c.quantity.... line_items.c.extended_cost... ]... cust_orders=select(columns).select_from(... users.join(orders).join(line_items).join(cookies)... ).where(... users.c.username == cust_name... )... result= connection.execute(cust_orders).fetchall()... return result... Conditional chaining 1234567891011121314151617181920212223&gt;&gt;&gt; def get_orders_by_customer(cust_name, shipped=None, details= False):... columns=[... orders.c.order_id,... users.c.username,... users.c.phone... ]... joins=users.join(orders)... if details:... columns.extend([... cookies.c.cookie_name,... line_items.c.quantity,... line_items.c.extended_cost... ])... joins=joins.join(line_items).join(cookies)... cust_orders=select(columns).select_from(joins).where(... users.c.username == cust_name... )... if shipped is not None:... cust_orders=cust_orders.where(orders.c.shipped == shipped)... result= connection.execute(cust_orders).fetchall()... return result... &gt;&gt;&gt; Raw QueriesIt is also possible to execute raw SQL statements or use raw SQL in part of a SQLAlchemy Core query. It still returns a ResultProxy, and you can continue to interact with it just as you would a query built using the SQL Expression syntax of SQLAlchemy Core. Full raw queries 12result= connection.execute('select * from orders').fetchall()print(result) Partial text query1234&gt;&gt;&gt; from sqlalchemy import text&gt;&gt;&gt; stmt= select([users]).where(text('username="cookiemon"'))print(connection.execute(stmt).fetchall())]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C02_Inserting Data_Querying Data]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C02-Inserting-Data-Querying-Data%2F</url>
    <content type="text"><![CDATA[Inserting Data12345678910&gt;&gt;&gt; ins= cookies.insert().values(... cookie_name='chololate chip',... cookie_recipe_url='http://some.aweso.me/cookie/recipe.html',... quantity= '12',... unit_cost= '0.50')... &gt;&gt;&gt; print(str(ins))INSERT INTO cookies (cookie_name, cookie_recipe_url, quantity, unit_cost) VALUES (:cookie_name, :cookie_recipe_url, :quantity, :unit_cost)&gt;&gt;&gt; The compile() method on the ins object returns a SQLCompiler object that gives us access to the actual parameters that will be sent with the query via the params attribute: 1234&gt;&gt;&gt; ins.compile().params&#123;'cookie_recipe_url': 'http://some.aweso.me/cookie/recipe.html', 'quantity': '12', 'cookie_name': 'chololate chip', 'unit_cost': '0.50'&#125;&gt;&gt;&gt; This compiles the statement via our dialect but does not execute it, and we access the params attribute of that statement. We can use the execute() method on our connection to send the statement to the database.We can also get the ID of the record we just inserted by accessing the inserted_primary_key attribute: 12result= connection.execute(ins)result.inserted_primary_key In addition to having insert as an instance method off a Table object, it is also available as a top-level function for those times that you want to build a statement “generatively” (a step at a time) or when the table may not be initially known. 12345678&gt;&gt;&gt; from sqlalchemy import insert&gt;&gt;&gt; ins= insert(cookies).values(... cookie_name='chocolate chip',... cookie_recipe_url='recipe.html',... quantity='12',... unit_cost = '0.50')... &gt;&gt;&gt; The execute method of the connection object can take more than just statements. It is also possible to provide the values as keyword arguments to the execute method after our statement. When the statement is compiled, it will add each one of the keyword argument keys to the columns list, and it adds each one of their values to the VALUES part of the SQL statement 12345678910ins = cookies.insert()result = connection.execute( ins, cookie_name='dark chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity='1', unit_cost='0.75')result.inserted_primary_key Multiple inserts 1234567891011121314151617inventory_list = [ &#123; 'cookie_name': 'peanut butter', 'cookie_recipe_url': 'http://some.aweso.me/cookie/peanut.html', 'cookie_sku': 'PB01', 'quantity': '24', 'unit_cost': '0.25' &#125;, &#123; 'cookie_name': 'oatmeal raisin', 'cookie_recipe_url': 'http://some.okay.me/cookie/raisin.html', 'cookie_sku': 'EWW01', 'quantity': '100', 'unit_cost': '1.00' &#125;]result = connection.execute(ins, inventory_list) Querying DataTo begin building a query, we start by using the select function, which is analogous to the standard SQL SELECT statement. Initially, let’s select all the records in our cookies table12345from sqlalchemy.sql import selects = select([cookies]) rp = connection.execute(s)results = rp.fetchall() # This tells rp, the ResultProxy, to return all the rows Remember we can use str(s) to look at the SQL statement the database will see, which in this case is SELECT cookies.cookie_id, cookies.cookie_name, cookies.cookie_recipe_url, cookies.cookie_sku, cookies.quantity, cookies.unit_cost FROM cookies. It is also possible to use the select method on the Table object to do this. 1234from sqlalchemy.sql import selects=cookies.select()rp=connection.execute(s)results=rp.fetchall() ResultProxyA ResultProxy is a wrapper around a DBAPI cursor object, and its main goal is to make it easier to use and manipulate the results of a statement. Handling rows with a ResultProxy 123first_row = results[0]first_row.cookie_name # Access column by name.first_row[cookies.c.cookie_name] # Access column by Column object Iterating over a ResultProxy 123rp = connection.execute(s)for record in rp: print(record.cookie_name) You can use the following methods as well to fetch results: first() Returns the first record if there is one and closes the connection. fetchone() Returns one row, and leaves the cursor open for you to make additional fetch calls. scalar() Returns a single value if a query results in a single record with one column. Controlling the Columns in the Query_Select only cookie_name and quantity_ 1234s = select([cookies.c.cookie_name, cookies.c.quantity])rp= connection.execute(s)print(rp.keys()) # (u'chocolate chip', 12),result= rp.first() OrderingOrder by quantity ascending 12345s= select([cookies.c.cookie_name, cookies.c.quantity])s = s.order_by(cookies.c.quantity)rp= connection.execute(s)for cookie in rp: print('&#123;&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name)) We saved the select statement into the s variable, used that s variable and added the order_by statement to it, and then reassigned that to the s variable. This is an example of how to compose statements in a generative or step-by-step fashion. This is the same as combining the select and the order_by statements into one line as shown here: s = select([...]).order_by(...) Order by quantity descending 123from sqlalchemy import descs= select([cookies.c.cookie_name, cookies.c.quantity])s= s.order_by(desc(cookies.c.quantity)) The desc() function can also be used as a method on a Column object, such as cookies.c.quantity.desc(). However, that can be a bit more confusing to read in long statements, so I always use desc() as a function. LimitingWe used the first() or fetchone() methods to get just a single row back. While our ResultProxy gave us the one row we asked for, the actual query ran over and accessed all the results, not just the single record. If we want to limit the query, we can use the limit() function to actually issue a limit statement as part of our query. Two smallest cookie inventories 12345s= select([cookies.c.cookie_name, cookies.c.quantity])s= s.order_by(cookies.c.quantity)s= s.limit(2)rp= connection.execute(s)print([result.cookie_name for result in rp])]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C01_Persisting the Tables]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C01-Persisting-the-Tables%2F</url>
    <content type="text"><![CDATA[All of our tables and additional schema definitions are associated with an instance of metadata. Persisting the schema to the database is simply a matter of calling the create_all() method on our metadata instance with the engine where it should create those tables: metadata.create_all(engine)By default, create_all will not attempt to re-create tables that already exist in the database, and it is safe to run multiple times. It’s wiser to use a database migration tool like Alembic to handle any changes to existing tables or additional schema than to try to handcode changes directly in your application code. from sqlalchemy import (MetaData, Table, Column, Integer, Numeric, String, DateTime, ForeignKey, create_engine)metadata = MetaData()engine= create_engine(‘sqlite:///:memory:’)metadata.create_all(engine)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C01_Keys and Constraints_Indexes_Relationships and ForeignKeyConstraints]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C01-Keys-and-Constraints-Indexes-Relationships-and-ForeignKeyConstraints%2F</url>
    <content type="text"><![CDATA[Keys and ConstraintsKeys and constraints are used as a way to ensure that our data meets certain requirements prior to being stored in the database. The objects that represent keys and constraints can be found inside the base SQLAlchemy module, and three of the more common ones can be imported as shown here:from sqlalchemy import PrimaryKeyConstraint, UniqueConstraint, CheckConstraintThe most common key type is a primary key, which is used as the unique identifier for each record in a database table and is used used to ensure a proper relationship between two pieces of related data in different tables. As you saw earlier in Example 1-1 and Example 1-2, a column can be made a primary key simply by using the primary_key keyword argument. You can also define composite primary keys by assigning the setting primary_key to True on multiple columns. The key will then essentially be treated like a tuple in which the columns marked as a key will be present in the order they were defined in the table. Primary keys can also be defined after the columns in the table constructor, as shown in the following snippet. You can add multiple columns separated by commas to create a composite key. If we wanted to explicitly define the key as shown in Example 1-2, it would look like this:PrimaryKeyConstraint(&#39;user_id&#39;, name=&#39;user_pk&#39;) Another common constraint is the unique constraint, which is used to ensure that no two values are duplicated in a given field. For our online cookie delivery service, for example, we would want to ensure that each customer had a unique username to log into our systems. We can also assign unique constraints on columns, as shown before in the username column, or we can define them manually as shown here: UniqueConstraint(&#39;username&#39;, name=&#39;uix_username&#39;) Not shown in Example 1-2 is the check constraint type. This type of constraint is used to ensure that the data supplied for a column matches a set of user-defined criteria. In the following example, we are ensuring that unit_cost is never allowed to be less than 0.00 because every cookie costs something to make (remember from Economics 101: TINSTAAFC—that is, there is no such thing as a free cookie!): CheckConstraint(&#39;unit_cost &gt;= 0.00&#39;, name=&#39;unit_cost_positive&#39;) IndexesIndexes are used to accelerate lookups for field values, and in Example 1-1, we created an index on the cookie_name column because we know we will be searching by that often. When indexes are created as shown in that example, you will have an index called ix_cookies_cookie_name. We can also define an index using an explicit construction type. Multiple columns can be designated by separating them by a comma. You can also add a keyword argument of unique=True to require the index to be unique as well. When creating indexes explicitly, they are passed to the Table constructor after the columns. To mimic the index created in Example 1-1, we could do it explicitly as shown here:123from sqlalchemy import IndexIndex('ix_cookies_cookie_name', 'cookie_name') We can also create functional indexes that vary a bit by the backend database being used. This lets you create an index for situations where you often need to query based on some unusual context. For example, what if we want to select by cookie SKU and name as a joined item, such as SKU0001 Chocolate Chip? We could define an index like this to optimize that lookup:Index(&#39;ix_test&#39;, mytable.c.cookie_sku, mytable.c.cookie_name)) Relationships and ForeignKeyConstraints One way to implement a relationship is shown in Example 1-3 in the line_items table on the order_id column; this will result in a ForeignKeyConstraint to define the relationship between the two tables. In this case, many line items can be present for a single order. However, if you dig deeper into the line_items table, you’ll see that we also have a relationship with the cookies table via the cookie_id ForeignKey. This is because line_items is actually an association table with some additional data on it between orders and cookies. Association tables are used to enable many-to-many relationships between two other tables. A single ForeignKey on a table is typically a sign of a one-to-many relationship; however, if there are multiple ForeignKey relationships on a table, there is a strong possibility that it is an association table. 123456789101112131415161718192021222324252627282930313233343536373839from sqlalchemy import MetaDatafrom sqlalchemy import Table, Column, Integer, Numeric, String, ForeignKey, DateTime, Booleanfrom datetime import datetimemetadata= MetaData()cookies=Table('cookies', metadata, Column('cookie_id',Integer(),primary_key= True), Column('cookie_name',String(50), index= True), Column('cookie_recipe_url',String(255)), Column('quantity', Integer()), Column('unit_cost',Numeric(12,2)) )users=Table('users', metadata, Column('user_id', Integer(), primary_key= True), Column('username',String(15), nullable=False, unique=True), Column('email_address',String(255), nullable=False), Column('phone',String(20),nullable=False), Column('password',String(25),nullable=False), Column('created_on', DateTime(),default=datetime.now), Column('updated_on',datetime(),default=datetime.now,onupdate=datetime.now) )orders=Table('orders', metadata, Column('order_id', Integer(),primary_key=True), Column('user_id', ForeignKey('users.user_id')), Column('shipped',Boolean(), default=False) )line_items=Table('line_items', metadata, Column('line_items_id', Integer(), primary_key= True), Column('order_id', ForeignKey('orders.order_id')), Column('cookie_id', ForeignKey('cookies.cookie_id')), Column('quantity', Integer()), Column('extended_cost', Numeric(12,2)) )]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_Class Metaprogramming]]></title>
    <url>%2F2018%2F05%2F25%2FC21-Class-Metaprogramming%2F</url>
    <content type="text"><![CDATA[A Class Factory_record_factory.py: a simple class factory_ 1234567891011121314151617181920212223242526272829def record_factory(cls_name, field_names): try: field_names= field_names.replace(',',' ').split() except AttributeError: pass field_names= tuple(field_names) def __init__(self, *args, **kwargs): attrs=dict(zip(self.__slots__, args)) attrs.update(kwargs) for name, value in attrs.items(): setattr(self, name, value) def __iter__(self): for name in self.__slots__: yield getattr(self, name) def __repr__(self): values=', '.join('&#123;&#125;=&#123;!r&#125;'.format(*i) for i in zip(self.__slots__, self)) return '&#123;&#125;(&#123;&#125;)'.format(self.__class__.__name__, values) cls_attrs=dict( __slots__= field_names, __init__= __init__, __iter__= __iter__, __repr__= __repr__ ) return type(cls_name, (object,), cls_attrs) We usually think of type as a function, because we use it like one, e.g., type(my_object) to get the class of the object—same as my_object.__class__. However, type is a class. It behaves like a class that creates a new class when invoked with three arguments: 12MyClass = type('MyClass', (MySuperClass, MyMixin), &#123;'x': 42, 'x2': lambda self: self.x * 2&#125;) The three arguments of type are named name, bases, and dict—the latter being a mapping of attribute names and attributes for the new class. The preceding code is functionally equivalent to this: 1234class MyClass(MySuperClass, MyMixin): x = 42 def x2(self): return self.x * 2 The novelty here is that the instances of type are classes, like MyClass here. In summary, the last line of record_factory builds a class named by the value of cls_name, with object as its single immediate superclass and with class attributes named __slots__, __init__, __iter__, and __repr__, of which the last three are instance methods. Instances of classes created by record_factory have a limitation: they are not serializable—that is, they can’t be used with the dump/load functions from the pickle module. A Class Decorator for Customizing DescriptorsA class decorator is very similar to a function decorator: it’s a function that gets a class object and returns the same class or a modified one. _bulkfood_v6.py: LineItem using Quantity and NonBlank descriptors_ 123456789101112131415import model_v6 as model@model.entityclass LineItem: description= model.NonBlank() weight= model.Quantity() price= model.Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price _model_v6.py: a class decorator_ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def entity(cls): for key, attr in cls.__dict__.items(): if isinstance(attr, Validated): type_name = type(attr).__name__ attr.storage_name = '_&#123;&#125;#&#123;&#125;'.format(type_name, key) return clsimport abcclass AutoStorage: __counter = 0 def __init__(self): cls= self.__class__ prefix= cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.storage_name) def __set__(self, instance,value): setattr(instance, self.storage_name, value)class Validated(abc.ABC, AutoStorage): #__set__ delegates validation to a validate method def __set__(self, instance, value): value= self.validate(instance, value) super().__set__(instance, value) # uses the returned value to invoke __set__ on a superclass, which # performs the actual storage. @abc.abstractmethod def validate(self, instance, value): '''return validated value or raise ValueError'''class Quantity(Validated): def validate(self, instance, value): if value &lt; 0: raise ValueError('value must be &gt; 0') return valueclass NonBlank(Validated): def validate(self, instance, value): value = value.strip() if len(value) == 0: raise ValueError('value cannot be empty or blank') return value _bulkfood_v6.py: doctests for new storage_name descriptor attributes_ 123456789&gt;&gt;&gt; raisins = LineItem('Golden raisins', 10, 6.95) &gt;&gt;&gt; dir(raisins)[:3] ['_NonBlank#description', '_Quantity#price', '_Quantity#weight'] &gt;&gt;&gt; LineItem.description.storage_name '_NonBlank#description' &gt;&gt;&gt; raisins.description 'Golden raisins' &gt;&gt;&gt; getattr(raisins, '_NonBlank#description') 'Golden raisins' A significant drawback of class decorators is that they act only on the class where they are directly applied. This means subclasses of the decorated class may or may not inherit the changes made by the decorator, depending on what those changes are. What Happens When: Import Time Versus RuntimeAlthough compiling is definitely an import-time activity, other things may happen at that time, because almost every statement in Python is executable in the sense that they potentially run user code and change the state of the user program. In particular, the import statement is not merely a declaration but it actually runs all the top-level code of the imported module when it’s imported for the first time in the process—further imports of the same module will use a cache, and only name binding occurs then. That top-level code may do anything, including actions typical of “runtime”, such as connecting to a database. That’s why the border between “import time” and “runtime” is fuzzy: the import statement can trigger all sorts of “runtime” behavior. The interpreter executes a def statement on the top level of a module when the module is imported, but what does that achieve? The interpreter compiles the function body (if it’s the first time that module is imported), and binds the function object to its global name, but it does not execute the body of the function, obviously. In the usual case, this means that the interpreter defines top-level functions at import time, but executes their bodies only when—and if—the functions are invoked at runtime. For classes, the story is different: at import time, the interpreter executes the body of every class, even the body of classes nested in other classes. Execution of a class body means that the attributes and methods of the class are defined, and then the class object itself is built. In this sense, the body of classes is “top-level code”: it runs at import time. Metaclasses 101Consider the Python object model: classes are objects, therefore each class must be an instance of some other class. By default, Python classes are instances of type. In other words, type is the metaclass for most built-in and user-defined classes: 123456789&gt;&gt;&gt; 'spam'.__class__&lt;class 'str'&gt;&gt;&gt;&gt; str.__class__&lt;class 'type'&gt;&gt;&gt;&gt; from bulkfood_v6 import LineItem&gt;&gt;&gt; LineItem.__class__&lt;class 'type'&gt;&gt;&gt;&gt; type.__class__&lt;class 'type'&gt; To avoid infinite regress, type is an instance of itself, as the last line shows. str and LineItem are instances of type. They all are subclasses of object. The classes object and type have a unique relationship: object is an instance of type, and type is a subclass of object. This relationship is “magic”: it cannot be expressed in Python because either class would have to exist before the other could be defined. The fact that type is an instance of itself is also magical. The important takeaway here is that all classes are instances of type, but metaclasses are also subclasses of type, so they act as class factories. In particular, a metaclass can customize its instances by implementing __init__. A metaclass __init__ method can do everything a class decorator can do. A Metaclass for Customizing Descriptors_model_v7.py: the EntityMeta metaclass and one instance of it, Entity model_v7.py: the EntityMeta metaclass and one instance of it,Entity_ 1234567891011class EntityMeta(type): def __init__(self, name, bases, attr_dict): super().__init__(name, bases, attr_dict) for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name= type(attr).__name__ attr.storage_name= '_&#123;&#125;#&#123;&#125;'.format(type_name, key)class Entity(metaclass=EntityMeta): '''Business entity with validated fields''' _bulkfood_v7.py: inheriting from model.Entity can work, if a metaclass is behind the scenes_ 1234567891011121314import model_v7 as modelclass LineItem(model.Entity): description= model.NonBlank() weight= model.Quantity() price= model.Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price The Metaclass __prepare__ Special MethodIn some applications it’s interesting to be able to know the order in which the attributes of a class are defined. For example, a library to read/write CSV files driven by userdefined classes may want to map the order of the fields declared in the class to the order of the columns in the CSV file. As we’ve seen, both the type constructor and the __new__ and __init__ methods of metaclasses receive the body of the class evaluated as a mapping of names to attributes. However, by default, that mapping is a dict, which means the order of the attributes as they appear in the class body is lost by the time our metaclass or class decorator can look at them. The solution to this problem is the __prepare__ special method, introduced in Python 3. This special method is relevant only in metaclasses, and it must be a class method (i.e., defined with the @classmethod decorator). The __prepare__ method is invoked by the interpreter before the__new__ method in the metaclass to create the mapping that will be filled with the attributes from the class body. Besides the metaclass as first argument, __prepare__ gets the name of the class to be constructed and its tuple of base classes, and it must return a mapping, which will be received as the last argument by __new__ and then __init__ when the metaclass builds a new class. _model_v8.py: the EntityMeta metaclass uses prepare, and Entity now has a field_names class method_ 12345678910111213141516171819202122232425262728import collectionsclass EntityMeta(type): # Return an empty OrderedDict instance, where the class attributes will be # stored. @classmethod def __prepare__(cls, name, bases): return collections.OrderedDict() def __init__(cls, name, bases, attr_dict): super().__init__(name, bases, attr_dict) cls._field_names= [] # attr_dict here is the OrderedDict obtained by the interpreter when it # called __prepare__ before calling __init__. for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name= type(attr).__name__ attr.storage_name= '_&#123;&#125;#&#123;&#125;'.format(type_name, key) cls._field_names.append(key)class Entity(metaclass=EntityMeta): '''Business entity with validated fields''' @classmethod def field_names(cls): for name in cls._field_names: yield name _bulkfood_v8.py: doctest showing the use of field_names—no changes are needed in the LineItem class; field_names is inherited from model.Entity_ 123456&gt;&gt;&gt; for name in LineItem.field_names(): ... print(name) ... description weight price Classes as Objectscls.\_bases___The tuple of base classes of the class. cls.\_qualname___A new attribute in Python 3.3 holding the qualified name of a class or function, which is a dotted path from the global scope of the module to the class definition. For example, in Example 21-6, the __qualname__ of the inner class ClassTwo is the string ‘ClassOne.ClassTwo‘, while its __name__ is just ‘ClassTwo‘. cls.\_subclasses__()_This method returns a list of the immediate subclasses of the class. The implementation uses weak references to avoid circular references between the superclass and its subclasses—which hold a strong reference to the superclasses in their __bases__ attribute. The method returns the list of subclasses that currently exist in memory. cls.mro()The interpreter calls this method when building a class to obtain the tuple of superclasses that is stored in the __mro__ attribute of the class. A metaclass can override this method to customize the method resolution order of the class under construction.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_Attribute Descriptors_02]]></title>
    <url>%2F2018%2F05%2F25%2FC20-Attribute-Descriptors-02%2F</url>
    <content type="text"><![CDATA[Overriding Versus Nonoverriding DescriptorsRecall that there is an important asymmetry in the way Python handles attributes. Reading an attribute through an instance normally returns the attribute defined in the instance, but if there is no such attribute in the instance, a class attribute will be retrieved. On the other hand, assigning to an attribute in an instance normally creates the attribute in the instance, without affecting the class at all.This asymmetry also affects descriptors, in effect creating two broad categories of descriptors depending on whether the __set__ method is defined. Overriding DescriptorA descriptor that implements the __set__ method is called an overriding descriptor, because although it is a class attribute, a descriptor implementing __set__ will override attempts to assign to instance attributes. Properties are also overriding descriptors: if you don’t provide a setter function, the default __set__ from the property class will raise AttributeError to signal that the attribute is read-only. descriptorkinds.py: simple classes for studying descriptor overriding behavior 12345678910111213141516171819202122232425262728293031323334353637383940414243def cls_name(obj_or_cls): cls= type(obj_or_cls) if cls is type: cls= obj_or_cls return cls.__name__.split('.')[-1]def display(obj): cls= type(obj) if cls in None: return '&lt;class &#123;&#125;&gt;'.format(obj.__name__) elif cls in [type(None), int]: return repr(obj) else: return '&lt;&#123;&#125; object&gt;'.format(cls_name(obj))def print_args(name, *args): pseudo_args= ', '.join(display(x) for x in args) print('-&gt; &#123;&#125;.__&#123;&#125;__(&#123;&#125;)'.format(cls_name(args[0]),name, pseudo_args))class Overriding: '''data descriptor or enforced descriptor''' def __get__(self, instance, owner): print_args('get',self, instance, owner) def __set__(self, instance, value): print_args('set', self, instance, value)class OverridingNoGet: '''without __get__''' def __set__(self, instance, value): print_args('set', self, instance, value)class NonOveriding: def __get__(self, instance, owner): print_args('get',self, instance, owner)class Managed: over= Overriding() over_no_get= OverridingNoGet() non_over= NonOveriding() def spam(self): print('-&gt; Managed.spam(&#123;&#125;)'.format(display(self))) Behavior of an overriding descriptor: obj.over is an instance of Overriding 123456789101112131415161718 &gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) &gt;&gt;&gt; Managed.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, None, &lt;class Managed&gt;) &gt;&gt;&gt; obj.over = 7 -&gt; Overriding.__set__(&lt;Overriding object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) &gt;&gt;&gt; obj.__dict__['over'] = 8 &gt;&gt;&gt; vars(obj) &#123;'over': 8&#125; &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;)# even with an instance attribute named over, the Managed.over descriptor still overrides attempts to read obj.over. Overriding Descriptor Without __get__Usually, overriding descriptors implement both __set__ and __get__, but it’s also possible to implement only __set__. In this case, only writing is handled by the descriptor. Reading the descriptor through an instance will return the descriptor object itself because there is no __get__ to handle that access. If a namesake instance attribute is created with a new value via direct access to the instance __dict__, the __set__ method will still override further attempts to set that attribute, but reading that attribute will simply return the new value from the instance, instead of returning the descriptor object. In other words, the instance attribute will shadow the descriptor, but only when reading. _Overriding descriptor without get: obj.over_no_get is an instance of OverridingNoGet_ 123456789101112131415&gt;&gt;&gt; obj.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; Managed.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; obj.over_no_get = 7 -&gt; OverridingNoGet.__set__(&lt;OverridingNoGet object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; obj.__dict__['over_no_get'] = 9 &gt;&gt;&gt; obj.over_no_get 9 &gt;&gt;&gt; obj.over_no_get = 7 -&gt; OverridingNoGet.__set__(&lt;OverridingNoGet object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over_no_get 9 Nonoverriding DescriptorIf a descriptor does not implement __set__, then it’s a nonoverriding descriptor. Setting an instance attribute with the same name will shadow the descriptor, rendering it ineffective for handling that attribute in that specific instance. Methods are implemented as nonoverriding descriptors. _Behavior of a nonoverriding descriptor: obj.non_over is an instance of NonOverriding_ 1234567891011121314151617&gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;)# The obj now has an instance attribute named non_over, which shadows the namesake descriptor # attribute in the Managed class. &gt;&gt;&gt; obj.non_over = 7 &gt;&gt;&gt; obj.non_over 7 &gt;&gt;&gt; Managed.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, None, &lt;class Managed&gt;) &gt;&gt;&gt; del obj.non_over &gt;&gt;&gt; obj.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) Python contributors and authors use different terms when discussing these concepts. Overriding descriptors are also called data descriptors or enforced descriptors. Nonoverriding descriptors are also known as nondata descriptors or shadowable descriptors. Overwriting a Descriptor in the ClassRegardless of whether a descriptor is overriding or not, it can be overwritten by assignment to the class. Example 20-12 Any descriptor can be overwritten on the class itself 123456&gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; Managed.over = 1 &gt;&gt;&gt; Managed.over_no_get = 2 &gt;&gt;&gt; Managed.non_over = 3 &gt;&gt;&gt; obj.over, obj.over_no_get, obj.non_over (1, 2, 3) Example 20-12 reveals another asymmetry regarding reading and writing attributes: although the reading of a class attribute can be controlled by a descriptor with __get__ attached to the managed class, the writing of a class attribute cannot be handled by a descriptor with __set__ attached to the same class. Methods Are DescriptorsA function within a class becomes a bound method because all user-defined functions have a __get__ method, therefore they operate as descriptors when attached to a class. A method is a nonoverriding descriptor12345678910 &gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.spam # Reading from obj.spam retrieves a bound method object. &lt;bound method Managed.spam of &lt;descriptorkinds.Managed object at 0x74c80c&gt;&gt; &gt;&gt;&gt; Managed.spam # But reading from Managed.spam retrieves a function. &lt;function Managed.spam at 0x734734&gt; &gt;&gt;&gt; obj.spam = 7 &gt;&gt;&gt; obj.spam 7# Assigning a value to obj.spam shadows the class attribute, rendering the spam method inaccessible # from the obj instance. Because functions do not implement __set__, they are nonoverriding descriptors. obj.spam and Managed.spam retrieve different objects. As usual with descriptors, the __get__ of a function returns a reference to itself when the access happens through the managed class. But when the access goes through an instance, the __get__ of the function returns a bound method object: a callable that wraps the function and binds the managed instance (e.g., obj) to the first argument of the function (i.e., self), like the functools.partial function does. _method_is_descriptor.py: a Text class, derived from UserString_12345678910import collectionsclass Text(collections.UserString): def __repr__(self): return 'Text(&#123;!r&#125;)'.format(self.data) def reverse(self): return self[::-1] Experiments with a method12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; word = Text('forward') &gt;&gt;&gt; word Text('forward') &gt;&gt;&gt; word.reverse() Text('drawrof') &gt;&gt;&gt; Text.reverse(Text('backward')) Text('drawkcab') &gt;&gt;&gt; type(Text.reverse), type(word.reverse) (&lt;class 'function'&gt;, &lt;class 'method'&gt;)# Text.reverse operates as a function, even working with objects that are not instances of Text. &gt;&gt;&gt; list(map(Text.reverse, ['repaid', (10, 20, 30), Text('stressed')])) ['diaper', (30, 20, 10), Text('desserts')]# Any function is a nonoverriding descriptor. Calling its __get__ with an instance retrieves a method # bound to that instance. &gt;&gt;&gt; Text.reverse.__get__(word) &lt;bound method Text.reverse of Text('forward')&gt;# Calling the function’s __get__ with None as the instance argument retrieves the function itself. &gt;&gt;&gt; Text.reverse.__get__(None, Text) &lt;function Text.reverse at 0x101244e18&gt;# The expression word.reverse actually invokes Text.reverse.__get__(word), returning the bound method. &gt;&gt;&gt; word.reverse &lt;bound method Text.reverse of Text('forward')&gt;# The bound method object has a __self__ attribute holding a reference to the instance on which the # method was called. &gt;&gt;&gt; word.reverse.__self__ Text('forward')# The __func__ attribute of the bound method is a reference to the original function attached to the # managed class. &gt;&gt;&gt; word.reverse.__func__ is Text.reverse True The bound method object also has a __call__ method, which handles the actual invocation. This method calls the original function referenced in __func__, passing the __self__ attribute of the method as the first argument. That’s how the implicit binding of the conventional self argument works. The way functions are turned into bound methods is a prime example of how descriptors are used as infrastructure in the language. Descriptor Usage TipsThe following list addresses some practical consequences of the descriptor characteristics just described: Use property to Keep It SimpleThe property built-in actually creates overriding descriptors implementing both __set__ and __get__, even if you do not define a setter method. The default __set__ of a property raises AttributeError: can’t set attribute, so a property is the easiest way to create a read-only attribute, avoiding the issue described next. Read-only descriptors require \_set___If you use a descriptor class to implement a read-only attribute, you must remember to code both __get__ and __set__, otherwise setting a namesake attribute on an instance will shadow the descriptor. The __set__ method of a read-only attribute should just raise AttributeError with a suitable message. Validation descriptors can work with \_set__ only_In a descriptor designed only for validation, the __set__ method should check the value argument it gets, and if valid, set it directly in the instance __dict__ using the descriptor instance name as key. That way, reading the attribute with the same name from the instance will be as fast as possible, because it will not require a __get__. Caching can be done efficiently with \_get__ only_If you code just the __get__ method, you have a nonoverriding descriptor. These are useful to make some expensive computation and then cache the result by setting an attribute by the same name on the instance. The namesake instance attribute will shadow the descriptor, so subsequent access to that attribute will fetch it directly from the instance __dict__ and not trigger the descriptor __get__ anymore. Nonspecial methods can be shadowed by instance attributesBecause functions and methods only implement __get__, they do not handle attempts at setting instance attributes with the same name, so a simple assignment like _my_obj.the_method = 7_ means that further access to the_method through that instance will retrieve the number 7—without affecting the class or other instances. However, this issue does not interfere with special methods. The interpreter only looks for special methods in the class itself, in other words, repr(x) is executed as x.__class__.__repr__(x), so a __repr__ attribute defined in x has no effect on repr(x). For the same reason, the existence of an attribute named __getattr__ in an instance will not subvert the usual attribute access algorithm.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_Attribute Descriptors_01]]></title>
    <url>%2F2018%2F05%2F25%2FC20-Attribute-Descriptors-01%2F</url>
    <content type="text"><![CDATA[Descriptors are a way of reusing the same access logic in multiple attributes. For example, field types in ORMs such as the Django ORM and SQL Alchemy are descriptors, managing the flow of data from the fields in a database record to Python object attributes and vice versa.A descriptor is a class that implements a protocol consisting of the __get__, __set__, and __delete__ methods. The property class implements the full descriptor protocol. As usual with protocols, partial implementations are OK. In fact, most descriptors we see in real code implement only __get__ and __set__, and many implement only one of these methods. Descriptor Example: Attribute ValidationWe’ll continue the series of LineItem examples where we left it, in “Coding a Property Factory” , by refactoring the quantity property factory into a Quantity descriptor class. LineItem Take # 3: A Simple DescriptorA class implementing a __get__, a __set__, or a __delete__ method is a descriptor.You use a descriptor by declaring instances of it as class attributes of another class. Descriptor classA class implementing the descriptor protocol. That’s Quantity in Figure 20-1. Managed classThe class where the descriptor instances are declared as class attributes—LineItem in Figure 20-1. _bulkfood_v3.py: quantity descriptors manage attributes in LineItem_ 1234567891011121314151617181920212223242526class Quantity: def __init__(self, storage_name): self.storage_name = storage_name # __set__ is called when there is an attempt to assign to the managed # attribute. Here, self is the descriptor instance (i.e., LineItem.weight or # LineItem.price), instance is the managed instance (a LineItem instance), and # value is the value being assigned. def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.storage_name] = value else: raise ValueError('value must be &gt; 0')class LineItem: weight= Quantity('weight') price= Quantity('price') def __init__(self, description, weight, price): self.description = description self.weight = weight self.price = price def subtotal(self): return self.weight * self.price When coding a __set__ method, you must keep in mind what the self and instance arguments mean: self is the descriptor instance, and instance is the managed instance. Descriptors managing instance attributes should store values in the managed instances. That’s why Python provides the instance argument to the descriptor methods. It may be tempting, but wrong, to store the value of each managed attribute in the descriptor instance itself. In other words, in the __set__ method, instead of coding: instance.__dict__[self.storage_name] = value the tempting but bad alternative would be: self.__dict__[self.storage_name] = value To understand why this would be wrong, think about the meaning of the first two arguments to __set__: self and instance. Here, self is the descriptor instance, which is actually a class attribute of the managed class. You may have thousands of LineItem instances in memory at one time, but you’ll only have two instances of the descriptors: LineItem.weight and LineItem.price. So anything you store in the descriptor instances themselves is actually part of a LineItem class attribute, and therefore is shared among all LineItem instances. A drawback is the need to repeat the names of the attributes when the descriptors are instantiated in the managed class body. It would be nice if the LineItem class could be declared like this: 123class LineItem: weight = Quantity() price = Quantity() The problem is that the righthand side of an assignment is executed before the variable exists. The expression Quantity() is evaluated to create a descriptor instance, and at this time there is no way the code in the Quantity class can guess the name of the variable to which the descriptor will be bound (e.g., weight or price). LineItem Take # 4: Automatic Storage Attribute NamesTo avoid retyping the attribute name in the descriptor declarations, we’ll generate a unique string for the storage_name of each Quantity instance. _bulkfood_v4.py: each Quantity descriptor gets a unique storage_name_ 123456789101112131415161718192021222324252627282930class Quantity: __counter = 0 def __init__(self): cls = self.__class__ prefix = cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): return getattr(instance, self.storage_name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.storage_name, value) else: raise ValueError('value must be &gt; 0')class LineItem: weight = Quantity() price = Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price Here we can use the higher-level getattr and setattr built-ins to store the value—instead of resorting to instance.__dict__—because the managed attribute and the storage attribute have different names, so calling getattr on the storage attribute will not trigger the descriptor, avoiding the infinite recursion. If you test bulkfood_v4.py, you can see that the weight and price descriptors work as expected, and the storage attributes can also be read directly, which is useful for debugging: 123456&gt;&gt;&gt; from bulkfood_v4 import LineItem&gt;&gt;&gt; coconuts = LineItem('Brazilian coconut', 20, 17.95)&gt;&gt;&gt; coconuts.weight, coconuts.price(20, 17.95)&gt;&gt;&gt; getattr(raisins, '_Quantity#0'), getattr(raisins, '_Quantity#1')(20, 17.95) Note that __get__ receives three arguments: self, instance, and owner. The owner argument is a reference to the managed class (e.g., LineItem), and it’s handy when the descriptor is used to get attributes from the class. If a managed attribute, such as weight, is retrieved via the class like LineItem.weight, the descriptor __get__ method receives None as the value for the instance argument. This explains the Attribute error in the next console session: 1234567&gt;&gt;&gt; from bulkfood_v4 import LineItem&gt;&gt;&gt; LineItem.weightTraceback (most recent call last): ... File ".../descriptors/bulkfood_v4.py", line 54, in __get__ return getattr(instance, self.storage_name)AttributeError: 'NoneType' object has no attribute '_Quantity#0' _bulkfood_v4b.py (partial listing): when invoked through the managed class, get returns a reference to the descriptor itself_ 123456789101112131415161718192021222324252627282930313233class Quantity: __counter = 0 def __init__(self): cls = self.__class__ prefix = cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self # If the call was not through an instance, return the descriptor itself return getattr(instance, self.storage_name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.storage_name, value) else: raise ValueError('value must be &gt; 0')class LineItem: weight = Quantity() price = Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price Trying out: 123456&gt;&gt;&gt; from bulkfood_v4b import LineItem&gt;&gt;&gt; LineItem.price&lt;bulkfood_v4b.Quantity object at 0x100721be0&gt;&gt;&gt;&gt; br_nuts = LineItem('Brazil nuts', 10, 34.95)&gt;&gt;&gt; br_nuts.price34.95 LineItem Take # 5: A New Descriptor Type _model_v5.py: the refactored descriptor classes_ 12345678910111213141516171819202122232425262728293031323334353637383940414243import abcclass AutoStorage: __counter = 0 def __init__(self): cls= self.__class__ prefix= cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.storage_name) def __set__(self, instance,value): setattr(instance, self.storage_name, value)class Validated(abc.ABC, AutoStorage): #__set__ delegates validation to a validate method def __set__(self, instance, value): value= self.validate(instance, value) super().__set__(instance, value) # uses the returned value to invoke __set__ on a superclass, which # performs the actual storage. @abc.abstractmethod def validate(self, instance, value): '''return validated value or raise ValueError'''class Quantity(Validated): def validate(self, instance, value): if value &lt; 0: raise ValueError('value must be &gt; 0') return valueclass NonBlank(Validated): def validate(self, instance, value): value = value.strip() if len(value) == 0: raise ValueError('value cannot be empty or blank') return value]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_Dynamic Attributes and Properties_02]]></title>
    <url>%2F2018%2F05%2F25%2FC19-Dynamic-Attributes-and-Properties-02%2F</url>
    <content type="text"><![CDATA[Using a Property for Attribute ValidationSo far, we have only seen the @property decorator used to implement read-only properties. In this section, we will create a read/write property. LineItem Take #1: Class for an Item in an Order _bulkfood_v1.py: the simplest LineItem class_ 123456789class LineItem: def __init__(self, description, weight, price): self.description = description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price A negative weight results in a negative subtotal 123456&gt;&gt;&gt; raisins = LineItem('Golden raisins', 10, 6.95) &gt;&gt;&gt; raisins.subtotal() 69.5 &gt;&gt;&gt; raisins.weight = -20 # garbage in... &gt;&gt;&gt; raisins.subtotal() # garbage out... -139.0 LineItem Take #2: A Validating PropertyImplementing a property will allow us to use a getter and a setter, but the interface of LineItem will not change (i.e., setting the weight of a LineItem will still be written as raisins.weight = 12). _bulkfood_v2.py: a LineItem with a weight property_ 12345678910111213141516171819202122class LineItem: def __init__(self, description, weight, price): self.description = description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price @property def weight(self): # The methodsthat implement a property all have the name of the public # attribute: weight. return self.__weight # The actual value is stored in a private attribute __weight @property.setter def weight(self, value): if value &gt; 0: self.__weight = value else: raise ValueError('value must be &gt; 0') Note how a LineItem with an invalid weight cannot be created now: 1234&gt;&gt;&gt; walnuts = LineItem('walnuts', 0, 10.00)Traceback (most recent call last): ...ValueError: value must be &gt; 0 Now we have protected weight from users providing negative values. Although buyers usually can’t set the price of an item, a clerical error or a bug may create a LineItem with a negative price. To prevent that, we could also turn price into a property, but this would entail some repetition in our code. There are two ways to abstract away property definitions: using a property factory or a descriptor class. The descriptor class approach is more flexible, and we’ll devote Chapter 20 to a full discussion of it. Properties are in fact implemented as descriptor classes themselves. A Proper Look at PropertiesAlthough often used as a decorator, the property built-in is actually a class. In Python, functions and classes are often interchangeable, because both are callable and there is no new operator for object instantiation, so invoking a constructor is no different than invoking a factory function. And both can be used as decorators, as long as they return a new callable that is a suitable replacement of the decorated function. This is the full signature of the property constructor: property(fget=None, fset=None, fdel=None, doc=None) All arguments are optional, and if a function is not provided for one of them, the corresponding operation is not allowed by the resulting property object. Properties Override Instance AttributesProperties are always class attributes, but they actually manage attribute access in the instances of the class. Instance attribute does not shadow class property 1234567891011121314151617181920212223&gt;&gt;&gt; class Class:... data='the class data attr'... @property... def prop(self): return 'the prop value'... &gt;&gt;&gt; obj=Class()&gt;&gt;&gt; Class.prop&lt;property object at 0x05B54CC0&gt;&gt;&gt;&gt; obj.prop'the prop value'&gt;&gt;&gt; obj.prop= 'foo'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; obj.__dict__['prop'] = 'foo'&gt;&gt;&gt; vars(obj)&#123;'prop': 'foo'&#125;&gt;&gt;&gt; obj.prop # reading obj.prop still runs the property getter. The property is not shadowed by an instance attribute.'the prop value'&gt;&gt;&gt; Class.prop ='baz' # Overwriting Class.prop destroys the property object.&gt;&gt;&gt; obj.prop # Now obj.prop retrieves the instance attribute. Class.prop is not a property anymore, so it no longer overrides obj.prop.'foo' New class property shadows existing instance attribute 123456789101112&gt;&gt;&gt; obj.data'the class data attr'&gt;&gt;&gt; obj.data= 'bar'&gt;&gt;&gt; Class.data'the class data attr'&gt;&gt;&gt; Class.data = property(lambda self: 'the "data" prop value')&gt;&gt;&gt; obj.data # obj.data is now shadowed by the Class.data property.'the "data" prop value'&gt;&gt;&gt; del Class.data&gt;&gt;&gt; obj.data'bar'&gt;&gt;&gt; The main point of this section is that an expression like obj.attr does not search for attr starting with obj. The search actually starts at obj.__class__, and only if there is no property named attr in the class, Python looks in the obj instance itself. This rule applies not only to properties but to a whole category of descriptors, the overriding descriptors. Property DocumentationWhen tools such as the console help() function or IDEs need to display the documentation of a property, they extract the information from the _\doc__ attribute of the property. If used with the classic call syntax, property can get the documentation string as the doc argument:weight = property(get_weight, set_weight, doc=&#39;weight in kilograms&#39;) When property is deployed as a decorator, the docstring of the getter method—the one with the @property decorator itself—is used as the documentation of the property as a whole. Coding a Property Factory_bulkfood_v2prop.py: the quantity property factory in use_ 12345678910111213141516171819202122232425262728class LineItem: weight= quantity('weight') price= quantity('price') # Use the factory to define the first custom property, weight, as a class # attribute. def __init__(self, description, weight, price): self.description= description self.weight= weight self.price= price def subtotal(self): return self.weight * self.pricedef quantity(storage_name): # qty_getter references storage_name, so it will be preserved in the closure # of this function; the value is retrieved directly from the instance.__dict__ # to bypass the property and avoid an infinite recursion. def qty_getter(instance): return instance.__dict__[storage_name] def qty_setter(instance, value): if value &gt; 0: instance.__dict__[storage_name] = value else: raise ValueError('value must be &gt; 0') return property(qty_getter, qty_setter) _bulkfood_v2prop.py: the quantity property factory_ 12345678910&gt;&gt;&gt; nutmeg = LineItem('Moluccan nutmeg', 8, 13.95)# Reading the weight and price through the properties shadowing the namesake instance attributes.&gt;&gt;&gt; nutmeg.weight, nutmeg.price (8, 13.95)# Using vars to inspect the nutmeg instance: here we see the actual instance attributes used to store the values.&gt;&gt;&gt; sorted(vars(nutmeg).items()) [('description', 'Moluccan nutmeg'), ('price', 13.95), ('weight', 8)] The weight property overrides the weight instance attribute so that every reference to self.weight or nutmeg.weight is handled by the property functions, and the only way to bypass the property logic is to access the instance __dict__ directly. Handling Attribute DeletionRecall from the Python tutorial that object attributes can be deleted using the del statement: del my_object.an_attribute In practice, deleting attributes is not something we do every day in Python, and the requirement to handle it with a property is even more unusual. But it is supported, and I can think of a silly example to demonstrate it. In a property definition, the @my_propety.deleter decorator is used to wrap the method in charge of deleting the attribute managed by the property. blackknight.py 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; class BlackKnight:... ... def __init__(self):... self.members = ['an arm', ... 'another arm',... 'a leg'... ]... ... self.phrases = ["'Tis but a scratch.",... "It's just a flesh wound.",... "I'm invincible!"]... ... @property... def member(self):... print('next member is:')... return self.members[0]... ... @member.deleter... def member(self):... text='BLACK KNIGHT (loses &#123;&#125;)\n-- &#123;&#125;'... print(text.format(self.members.pop(0), self.phrases.pop(0)))... &gt;&gt;&gt; knight= BlackKnight()&gt;&gt;&gt; knight.membernext member is:'an arm'&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses an arm)-- 'Tis but a scratch.&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses another arm)-- It's just a flesh wound.&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses a leg)-- I'm invincible!&gt;&gt;&gt; Using the classic call syntax instead of decorators, the fdel argument is used to set the deleter function. For example, the member property would be coded like this in the body of the BlackKnight class: member = property(member_getter, fdel=member_deleter) If you are not using a property, attribute deletion can also be handled by implementing the lower-level __delattr__ special method. Essential Attributes and Functions for Attribute HandlingSpecial Attributes that Affect Attribute HandlingThe behavior of many of the functions and special methods listed in the following sections depend on three special attributes: __class__A reference to the object’s class (i.e., obj.__class__ is the same as type(obj)). Python looks for special methods such as __getattr__ only in an object’s class, and not in the instances themselves. __dict__A mapping that stores the writable attributes of an object or class. An object that has a __dict__ can have arbitrary new attributes set at any time. If a class has a __slots__ attribute, then its instances may not have a __dict__. See __slots__ (next). __slots__An attribute that may be defined in a class to limit the attributes its instances can have. __slots__ is a tuple of strings naming the allowed attributes. If the ‘__dict__‘ name is not in __slots__ , then the instances of that class will not have a __dict__ of their own, and only the named attributes will be allowed in them. Built-In Functions for Attribute HandlingThese five built-in functions perform object attribute reading, writing, and introspection: dir([object])Lists most attributes of the object. The official docs say dir is intended for interactive use so it does not provide a comprehensive list of attributes, but an “interesting” set of names. dir can inspect objects implemented with or without a __dict__. The __dict__ attribute itself is not listed by dir, but the __dict__ keys are listed. Several special attributes of classes, such as __mro__, __bases__, and __name__ are not listed by dir either. If the optional object argument is not given, dir lists the names in the current scope. getattr(object, name[, default])Gets the attribute identified by the name string from the object. This may fetch an attribute from the object’s class or from a superclass. If no such attribute exists, getattr raises AttributeError or returns the default value, if given. hasattr(object, name)Returns True if the named attribute exists in the object, or can be somehow fetched through it (by inheritance, for example). The documentation explains: “This is implemented by calling getattr(object, name) and seeing whether it raises an AttributeError or not.” setattr(object, name, value)Assigns the value to the named attribute of object, if the object allows it. This may create a new attribute or overwrite an existing one. vars([object])Returns the __dict__ of object; vars can’t deal with instances of classes that define __slots__ and don’t have a __dict__ (contrast with dir, which handles such instances). Without an argument, vars() does the same as locals(): returns a dict representing the local scope. Special Methods for Attribute HandlingAttribute access using either dot notation or the built-in functions getattr, hasattr, and setattr trigger the appropriate special methods listed here. Reading and writing attributes directly in the instance __dict__ does not trigger these special methods— and that’s the usual way to bypass them if needed. In other words, assume that the special methods will be retrieved on the class itself, even when the target of the action is an instance. For this reason, special methods are not shadowed by instance attributes with the same name. In the following examples, assume there is a class named Class, obj is an instance of Class, and attr is an attribute of obj. For every one of these special methods, it doesn’t matter if the attribute access is done using dot notation or one of the built-in functions listed in “Built-In Functions for Attribute Handling”. For example, both obj.attr and getattr(obj, ‘attr’, 42) trigger Class.__getattribute__(obj, ‘attr’). delattr(self, name)Always called when there is an attempt to delete an attribute using the del statement; e.g., del obj.attr triggers Class.__delattr__(obj, ‘attr’). __dict__(self)Called when dir is invoked on the object, to provide a listing of attributes; e.g.,dir(obj) triggers Class.__dict__(obj). __getattr__(self, name)Called only when an attempt to retrieve the named attribute fails, after the obj, Class, and its superclasses are searched. The expressions obj.no_such_attr, get attr(obj, ‘.no_such_attr’), and hasattr(obj, ‘.no_such_attr’) may trigger Class.__getattr__(obj, ‘.no_such_attr’), but only if an attribute by that name cannot be found in obj or in Class and its superclasses. __getattribute__(self, name)Always called when there is an attempt to retrieve the named attribute, except when the attribute sought is a special attribute or method. Dot notation and the getattr and hasattr built-ins trigger this method. __getattr__ is only invoked after __getattribute__, and only when __getattribute__ raises AttributeError. To retrieve attributes of the instance obj without triggering an infinite recursion, implementations of __getattribute__ should use super().__getattribute__(obj, name). __setattr__(self, name, value)Always called when there is an attempt to set the named attribute. Dot notation and the setattr built-in trigger this method; e.g., both obj.attr = 42 and setattr(obj, ‘attr’, 42) trigger Class.__setattr__(obj, ‘attr’, 42). In practice, because they are unconditionally called and affect practically every attribute access, the __getattribute__ and __setattr__ special methods are harder to use correctly than __getattr__—which only handles nonexisting attribute names. Using properties or descriptors is less error prone than defining these special methods.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_Dynamic Attributes and Properties_01]]></title>
    <url>%2F2018%2F05%2F25%2FC19-Dynamic-Attributes-and-Properties-01%2F</url>
    <content type="text"><![CDATA[Data attributes and methods are collectively known as attributes in Python: a method is just an attribute that is callable. Besides data attributes and methods, we can also create properties, which can be used to replace a public data attribute with accessor methods (i.e., getter/setter), without changing the class interface.Besides properties, Python provides a rich API for controlling attribute access and implementing dynamic attributes. The interpreter calls special methods such as __getattr__ and __setattr__ to evaluate attribute access using dot notation (e.g.,obj.attr). A user-defined class implementing __getattr__ can implement “virtual attributes” by computing values on the fly whenever somebody tries to read a nonexistent attribute like obj.no_such_attribute. Data Wrangling with Dynamic Attributes123456789101112131415161718192021222324252627282930313233343536373839**_osconfeed.py: downloading osconfeed.json_**from urllib.request import urlopenimport warningsimport osimport jsonURL='http://www.oreilly.com/pub/sc/osconfeed'JSON= 'data/osconfeed.json'def load(): if not os.path.exists(JSON): msg= 'downloading &#123;&#125; to &#123;&#125;'.format(URL, JSON) warnings.warn(msg) with urlopen(URL) as remote, open(JSON, 'wb') as local: local.write(remote.read()) with open(JSON) as fp: return json.load(fp) # The json.load function parses a JSON file and returns native Python objects.&gt;&gt;&gt; feed = load() &gt;&gt;&gt; sorted(feed['Schedule'].keys()) ['conferences', 'events', 'speakers', 'venues'] &gt;&gt;&gt; for key, value in sorted(feed['Schedule'].items()): ... print('&#123;:3&#125; &#123;&#125;'.format(len(value), key)) ...1 conferences 484 events 357 speakers 53 venues &gt;&gt;&gt; feed['Schedule']['speakers'][-1]['name'] 'Carina C. Zona' &gt;&gt;&gt; feed['Schedule']['speakers'][-1]['serial'] 141590 &gt;&gt;&gt; feed['Schedule']['events'][40]['name'] 'There *Will* Be Bugs' &gt;&gt;&gt; feed['Schedule']['events'][40]['speakers'] [3471, 5199] Exploring JSON-Like Data with Dynamic AttributesThe syntax feed[‘Schedule’][‘events’][40][‘name’] is cumbersome. In JavaScript, you can get the same value by writing feed.Schedule.events[40].name. It’s easy to implement a dict-like class that does the same in Python. The keystone of the FrozenJSON class is the __getattr__ method, It’s essential to recall that the __getattr__ special method is only invoked by the interpreter when the usual process fails to retrieve an attribute (i.e., when the named attribute cannot be found in the instance, nor in the class or in its superclasses). explore0.py: turn a JSON dataset into a FrozenJSON holding nested FrozenJSON objects, lists, and simple types 1234567891011121314151617181920212223from collections import abcclass FrozenJson: ''' read-only for navigating a JSON-like object using attribute notation ''' def __init__(self, mapping): self.__data= dict(mapping) def __getattr__(self, name): if hasattr(self.__data, name): return getattr(self.__data, name) else: return FrozenJson.build(self.__data[name]) @classmethod def build(cls, obj): if isinstance(obj, abc.Mapping): return cls(obj) elif isinstance(obj, abc.MutableSequence): return [cls.build(item) for item in obj] else: return obj The Invalid Attribute Name ProblemThe FrozenJSON class has a limitation: there is no special handling for attribute namesthat are Python keywords.12345678910&gt;&gt;&gt; grad=FrozenJson(&#123;'name':'Jim','class':1982&#125;)&gt;&gt;&gt; grad.class File "&lt;stdin&gt;", line 1 grad.class ^SyntaxError: invalid syntax&gt;&gt;&gt; getattr(grad, 'class')1982&gt;&gt;&gt; But the idea of FrozenJSON is to provide convenient access to the data, so a better solution is checking whether a key in the mapping given to FrozenJSON.__init__ is a keyword, and if so, append an _ to it, so the attribute can be read like this: _explore1.py: append a _ to attribute names that are Python keywords_ 123456789101112131415import keyword def __init__(self, mapping): self.__data= &#123;&#125; for key, value in mapping.items(): if keyword.iskeyword(key): key+='_' self.__data[key]=value# A similar problem may arise if a key in the JSON is not a valid Python identifier:&gt;&gt;&gt; x = FrozenJSON(&#123;'2be':'or not'&#125;)&gt;&gt;&gt; x.2be File "&lt;stdin&gt;", line 1 x.2be ^SyntaxError: invalid syntax Such problematic keys are easy to detect in Python 3 because the str class provides the s.isidentifier() method, which tells you whether s is a valid Python identifier according to the language grammar. Flexible Object Creation with __new__We often refer to__init__ as the constructor method, but that’s because we adopted jargon from other languages. The special method that actually constructs an instance is __new__: it’s a class method (but gets special treatment, so the @classmethod decorator is not used), and it must return an instance. That instance will in turn be passed as the first argument self of __init__. Because __init__ gets an instance when called, and it’s actually forbidden from returning anything, __init__ is really an “initializer.” The real constructor is __new__—which we rarely need to code because the implementation inherited from object suffices. The path just described, from __new__ to __init__, is the most common, but not the only one. The __new__ method can also return an instance of a different class, and when that happens, the interpreter does not call __init__. In other words, the process of building an object in Python can be summarized with this pseudocode: pseudo-code for object construction12345678def object_maker(the_class, some_arg): new_object = the_class.__new__(some_arg) if isinstance(new_object, the_class): the_class.__init__(new_object, some_arg) return new_object# the following statements are roughly equivalentx = Foo('bar')x = object_maker(Foo, 'bar') explore2.py: using new instead of build to construct new objects that may or may not be instances of FrozenJSON 1234567891011121314151617181920212223242526272829from collections import abcfrom keyword import iskeywordclass FrozenJSON: def __new__(cls, arg): if isinstance(arg, abc.Mapping): return super().__new__(cls) # The default behavior is to delegate to the __new__ of a super class. In # this case, we are calling __new__ from the object base class, passing # FrozenJSON as the only argument. elif isinstance(arg, abc.MutableSequence): return [cls(item) for item in arg] else: return arg def __init__(self, mapping): self.__data=&#123;&#125; for key, value in mapping.items(): if iskeyword(key): key+='_' self.__data[key]= value def __getattr__(self, name): if hasattr(self.__data, name): return getattr(self.__data, name) else: return FrozenJSON(self.__data[name]) Restructuring the OSCON Feed with shelveThe funny name of the standard shelve module makes sense when you realize that pickle is the name of the Python object serialization format—and of the module that converts objects to/from that format. Because pickle jars are kept in shelves, it makes sense that shelve provides pickle storage. The shelve.open high-level function returns a shelve.Shelf instance—a simple key-value object database backed by the dbm module, with these characteristics: shelve.Shelf subclasses abc.MutableMapping, so it provides the essential methods we expect of a mapping type In addition, shelve.Shelf provides a few other I/O management methods, like sync and close; it’s also a context manager. Keys and values are saved whenever a new value is assigned to a key. The keys must be strings. The values must be objects that the pickle module can handle. schedule1.py: exploring OSCON schedule data saved to a shelve.Shelf 12345678910111213141516171819202122232425import warningsimport osconfeedDB_NAME= 'data/schedule1_db'CONFERENCE= 'conference.115'class Record: def __init__(self, **kwargs): self.__dict__.update(kwargs) # a common shortcut to build an instance with attributes created from # keyword argumentsdef load_db(db): raw_data= osconfeed.load() warnings.warn('loading '+ DB_NAME) for collection, rec_list in raw_data['Shedule'].items(): record_type=collection[:-1] #record_type is set to the collection name without the trailing 's' for record in rec_list: key= '&#123;&#125;.&#123;&#125;'.format(record_type, record['serial']) record['serial'] = key db[key]= Record(**record) Trying out the functionality provided by schedule1.py 1234567891011&gt;&gt;&gt; import shelve &gt;&gt;&gt; db = shelve.open(DB_NAME) &gt;&gt;&gt; if CONFERENCE not in db: ... load_db(db) ... &gt;&gt;&gt; speaker = db['speaker.3471'] &gt;&gt;&gt; type(speaker) &lt;class 'schedule1.Record'&gt; &gt;&gt;&gt; speaker.name, speaker.twitter ('Anna Martelli Ravenscroft', 'annaraven') &gt;&gt;&gt; db.close() The Record.__init__ method illustrates a popular Python hack. Recall that the__dict__ of an object is where its attributes are kept—unless __slots__ is declared inthe class.So, updating an instance __dict__ with a mapping is a quick way to create a bunch ofattributes in that instance. schedule2.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import warningsimport inspectimport osconfeedDB_NAME= 'data/schedule2_db'CONFERENCE='conference.115'class Record: def __init__(self, **kwargs): self.__dict__.update(**kwargs) def __eq__(self, other): if isinstance(other, Record): return self.__dict__== other.__dict__ else: return NotImplementedclass MissingDatabaseError(RuntimeError): '''when a database id required but was not set'''class DbRecord(Record): __db = None @staticmethod def set_db(db): DbRecord.__db = db @staticmethod def get_db(): return DbRecord.__db @classmethod def fetch(cls, ident): db= cls.get_db() try: return db[ident] except TypeError: if db is None: msg= 'database not set; call "&#123;&#125;.set_db(my_db)"' raise MissingDatabaseError(msg.format(cls.__name__)) else: raise def __repr__(self): if hasattr(self, 'serial'): cls_name = self.__class__.__name__ return '&lt;&#123;&#125; serial=&#123;!r&#125;&gt;'.format(cls_name, self.serial) else: return super().__repr__()class Event(DbRecord): @property def venue(self): key= 'venue.&#123;&#125;'.format(self.venue_serial) return self.__class__.fetch(key) @property def speakers(self): if not hasattr(self, '_speaker_objs'): spkr_serials= self.__dict__['speakers'] fetch= self.__class__.fetch self._speaker_objs=[fetch('speaker.&#123;&#125;'.format(key)) for key in spkr_serials] return self._speaker_objs def __repr__(self): if hasattr(self, 'name'): cls_name = self.__class__.__name__ return '&lt;&#123;&#125; &#123;!r&#125;&gt;'.format(cls_name, self.name) else: return super().__repr__()def load_db(db): raw_data= osconfeed.load() warnings.warn('loading '+DB_NAME) for collection, rec_list in raw_data['Schedule'].items(): record_type= collection[:-1] cls_name= record_type.capitalize() cls= globals().get(cls_name, DbRecord) if inspect.isclass(cls) and issubclass(cls, DbRecord): factory= cls else: factory= DbRecord for record in rec_list: key= '&#123;&#125;.&#123;&#125;'.format(record_type, record['serial']) record['serial']= key db[key]= factory(**record) Extract from the doctests of schedule2.py1234567891011121314&gt;&gt;&gt; DbRecord.set_db(db) &gt;&gt;&gt; event = DbRecord.fetch('event.33950') &gt;&gt;&gt; event &lt;Event 'There *Will* Be Bugs'&gt; &gt;&gt;&gt; event.venue &lt;DbRecord serial='venue.1449'&gt; &gt;&gt;&gt; event.venue.name 'Portland 251' &gt;&gt;&gt; for spkr in event.speakers: ... print('&#123;0.serial&#125;: &#123;0.name&#125;'.format(spkr)) ... speaker.3471: Anna Martelli Ravenscroft speaker.5199: Alex Martelli]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C17_Concurrency with Futures(Not finished)]]></title>
    <url>%2F2018%2F05%2F25%2FC17-Concurrency-with-Futures%2F</url>
    <content type="text"><![CDATA[flags.py: sequential download script; some functions will be reused by the other scripts import osimport timeimport sysimport requests POP20_CC = (‘CN IN US ID BR PK NG BD RU JP ‘ ‘MX PH VN ET EG DE IR TR CD FR’).split() BASE_URL= ‘http://flupy.org/data/flags&#39; DEST_DIR= ‘downloads/‘ def save_flag(img, filename): path= os.path.join(DEST_DIR, filename) with open(path, ‘wb’) as fp: fp.write(img) def get_flag(cc): url=’{}/{cc}/{cc}.gif’.format(BASE_URL, cc= cc.lower()) resp = requests.get(url) return resp.content Display a string and flush sys.stdout so we can see progress in a one-linedisplay; this is needed because Python normally waits for a line break to flushthe stdout buffer.def show(text): print(text,end= ‘ ‘) sys.stdout.flush() def download_many(cc_list): for cc in sorted(cc_list): image= get_flag(cc) show(cc) save_flag(image, cc.lower()+’.gif’) return len(cc_list) def main(download_many): t0=time.time() count= download_many(POP20_CC) elapsed= time.time() - t0 msg= ‘\n{} flags downloaded in {:.2f}s’ print(msg.format(count, elapsed)) if name== ‘main‘: main(download_many) Downloading with concurrent.futures_flags_threadpool.py: threaded download script using futures.ThreadPoolExecutor_ from concurrent import futures from flags import save_flag, get_flag, show, main MAX_WORKERS = 20 def download_one(cc): image= get_flag(cc) show(cc) save_flag(img, cc.lower()+’.gif’) return cc def download_many(cc_list): workers= min(MAX_WORKERS, len(cc_list)) with futures.ThreadPoolExecutor(workers) as executor: res=executor.map(download_one, sorted(cc_list)) return len(list(res)) if name== ‘main‘: main(download_many) flags_threadpool_ac.pydef download_many(cc_list): cc_list= cc_list[:5] with futures.ThreadPoolExecutor(max_workers = 3) as executor: to_do=[] for cc in sorted(cc_list): future = executor.submit(download_one, cc) # executor.submit schedules the callable to be executed, and returns a # future representing this pending operation. to_do.append(future) msg= &apos;Scheduled for {}: {}&apos; print(msg.format(cc, future)) results=[] for future in futures.as_completed(to_do):#as_completed yields futures as they are completed. res= future.result() # Get the result of this future. msg=&apos;{} result: {!r}&apos; print(msg.format(future, res)) results.append(res) return len(results) Blocking I/O and the GILThe CPython interpreter is not thread-safe internally, so it has a Global Interpreter Lock(GIL), which allows only one thread at a time to execute Python bytecodes. That’s whya single Python process usually cannot use multiple CPU cores at the same time. When we write Python code, we have no control over the GIL, but a built-in functionor an extension written in C can release the GIL while running time-consuming tasks.In fact, a Python library coded in C can manage the GIL, launch its own OS threads,and take advantage of all available CPU cores. This complicates the code of the libraryconsiderably, and most library authors don’t do itHowever, all standard library functions that perform blocking I/O release the GIL whenwaiting for a result from the OS. This means Python programs that are I/O bound canbenefit from using threads at the Python level: while one Python thread is waiting fora response from the network, the blocked I/O function releases the GIL so anotherthread can run. Launching Processes with concurrent.futures]]></content>
  </entry>
  <entry>
    <title><![CDATA[C16_Coroutines]]></title>
    <url>%2F2018%2F05%2F25%2FC16-Coroutines%2F</url>
    <content type="text"><![CDATA[We find two main senses for the verb “to yield” in dictionaries: to produce or to give way. Both senses apply in Python when we use the yield keyword in a generator. A line such as yield item produces a value that is received by the caller of next(…), and it also gives way, suspending the execution of the generator so that the caller may proceed until it’s ready to consume another value by invoking next() again. The caller pulls values from the generator.A coroutine is syntactically like a generator: just a function with the yield keyword in its body. However, in a coroutine, yield usually appears on the right side of an expression (e.g., datum = yield), and it may or may not produce a value—if there is no expression after the yield keyword, the generator yields None. The coroutine may receive data from the caller, which uses .send(datum) instead of next(…) to feed the coroutine. Usually, the caller pushes values into the coroutine. It is even possible that no data goes in or out through the yield keyword. Regardless of the flow of data, yield is a control flow device that can be used to implement cooperative multitasking: each coroutine yields control to a central scheduler so that other coroutines can be activated. How Coroutines Evolved from Generators Using .send(…), the caller of the generator can post data that then becomes the value of the yield expression inside the generator function. This allows a generator to be used as a coroutine: a procedure that collaborates with the caller, yielding and receiving values from the caller. In addition to .send(…), PEP 342 also added .throw(…) and .close() methods that respectively allow the caller to throw an exception to be handled inside the generator, and to terminate it. When you start thinking of yield primarily in terms of control flow, you have the mindset to understand coroutines. Basic Behavior of a Generator Used as a Coroutine 123456789101112131415&gt;&gt;&gt; def simple_coroutine():... print('-&gt; coroutine started')... x = yield # 1... print('-&gt; coroutine received:', x)... &gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro &lt;generator object simple_coroutine at 0x05D8CA80&gt;&gt;&gt;&gt; next(my_coro) # 2-&gt; coroutine started&gt;&gt;&gt; my_coro.send(42) # 3-&gt; coroutine received: 42Traceback (most recent call last): # 4 File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration yield is used in an expression; when the coroutine is designed just to receive data from the client it yields None—this is implicit because there is no expression to the right of the yield keyword. The first call is next(…) because the generator hasn’t started so it’s not waiting in a yield and we can’t send it any data initially. This call makes the yield in the coroutine body evaluate to 42; now the coroutine resumes and runs until the next yield or termination. In this case, control flows off the end of the coroutine body, which prompts the generator machinery to raise StopIteration, as usual. A coroutine can be in one of four states. You can determine the current state using the inspect.getgeneratorstate(…) function, which returns one of these strings: ‘GEN_CREATED’Waiting to start execution. ‘GEN_RUNNING’Currently being executed by the interpreter. ‘GEN_SUSPENDED’Currently suspended at a yield expression. ‘GEN_CLOSED’Execution has completed. Because the argument to the send method will become the value of the pending yield expression, it follows that you can only make a call like my_coro.send(42) if the coroutine is currently suspended. But that’s not the case if the coroutine has never been activated—when its state is ‘GEN_CREATED’. That’s why the first activation of a coroutine is always done with next(my_coro)—you can also call my_coro.send(None), and the effect is the same. If you create a coroutine object and immediately try to send it a value that is not None, this is what happens: 12345&gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro.send(1729)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: can't send non-None value to a just-started generator The initial call next(my_coro) is often described as “priming” the coroutine (i.e., advancing it to the first yield to make it ready for use as a live coroutine). A coroutine that yields twice 123456789101112131415161718192021222324&gt;&gt;&gt; def simple_coro2(a):... print('-&gt; Started: a =', a)... b = yield a... print('-&gt; Received: b =', b)... c = yield a + b... print('-&gt; Received: c =', c)... &gt;&gt;&gt; my_coro2=simple_coro2(14)&gt;&gt;&gt; from inspect import getgeneratorstate&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_CREATED'&gt;&gt;&gt; next(my_coro2)-&gt; Started: a = 1414&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_SUSPENDED'&gt;&gt;&gt; my_coro2.send(28) # 1-&gt; Received: b = 2842&gt;&gt;&gt; my_coro2.send(99) # 2-&gt; Received: c = 99Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration Send number 28 to suspended coroutine; the yield expression evaluates to 28 and that number is bound to b. The -&gt; Received: b = 28 message is displayed, the value of a + b is yielded (42), and the coroutine is suspended waiting for the value to be assigned to c. Send number 99 to suspended coroutine; the yield expression evaluates to 99 the number is bound to c. The -&gt; Received: c = 99 message is displayed, then the coroutine terminates, causing the generator object to raise StopIteration. It’s crucial to understand that the execution of the coroutine is suspended exactly at the yield keyword. As mentioned before, in an assignment statement, the code to the right of the = is evaluated before the actual assignment happens. This means that in a line like b = yield a, the value of b will only be set when the coroutine is activated later by the client code. It takes some effort to get used to this fact, but understanding it is essential to make sense of the use of yield in asynchronous programming. Execution of the simple_coro2 coroutine can be split in three phases: next(my_coro2) prints first message and runs to yield a, yielding number 14. my_coro2.send(28) assigns 28 to b, prints second message, and runs to yield a +b, yielding number 42. my_coro2.send(99) assigns 99 to c, prints third message, and the coroutine ter‐minates. Example: Coroutine to Compute a Running Average123456789101112131415161718&gt;&gt;&gt; def averager():... total= 0.0... count = 0... average = None... while True: # 1... term = yield average # 2... total += term... count += 1... average = total / count... &gt;&gt;&gt; coro_avg = averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(30)20.0&gt;&gt;&gt; coro_avg.send(5)15.0 This infinite loop means this coroutine will keep on accepting values and producing results as long as the caller sends them. This coroutine will only terminate when the caller calls .close() on it, or when it’s garbage collected because there are no more references to it. The yield statement here is used to suspend the coroutine, produce a result to the caller, and—later—to get a value sent by the caller to the coroutine, which resumes its infinite loop. The call next(coro_avg) makes the coroutine advance to the yield, yielding the initial value for average, which is None, so it does not appear on the console. At this point, the coroutine is suspended at the yield, waiting for a value to be sent. The line coro_avg.send(10) provides that value, causing the coroutine to activate, assigning it to term, updating the total, count, and average variables, and then starting another iteration in the while loop, which yields the average and waits for another term. Decorators for Coroutine PrimingYou can’t do much with a coroutine without priming it: we must always remember to call next(my_coro) before my_coro.send(x). To make coroutine usage more convenient, a priming decorator is sometimes used. coroutil.py: decorator for priming coroutine 123456789101112131415161718192021222324252627&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def coroutine(func):... @wraps(func)... def primer(*args, **kwargs):... gen= func(*args, **kwargs)... next(gen)... return gen... return primer... &gt;&gt;&gt; @coroutine... def averager():... total= 0.0... count = 0... average= None... while True:... term= yield average... total += term... count += 1... average = total/count... &gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; getgeneratorstate(coro_avg)'GEN_SUSPENDED'&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(30)20.0 Coroutine Termination and Exception HandlingAn unhandled exception within a coroutine propagates to the caller of the next or send that triggered it. How an unhandled exception kills a coroutine12345678910&gt;&gt;&gt; coro_avg.send('spam') # 1Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 8, in averagerTypeError: unsupported operand type(s) for +=: 'float' and 'str'&gt;&gt;&gt; coro_avg.send(60) # 2Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration Sending a nonnumeric value causes an exception inside the coroutine. Because the exception was not handled in the coroutine, it terminated. Any attempt to reactivate it will raise StopIteration. Since Python 2.5, generator objects have two methods that allow the client to explicitly send exceptions into the coroutine—throw and close: generator.throw(exc_type[, exc_value[, traceback]])Causes the yield expression where the generator was paused to raise the exception given. If the exception is handled by the generator, flow advances to the next yield, and the value yielded becomes the value of the generator.throw call. If the exception is not handled by the generator, it propagates to the context of the caller. generator.close()Causes the yield expression where the generator was paused to raise a GeneratorExit exception. No error is reported to the caller if the generator does not handle that exception or raises StopIteration—usually by running to completion. When receiving a GeneratorExit, the generator must not yield a value, otherwise a RuntimeError is raised. If any other exception is raised by the generator, it propagates to the caller. _coro_exc_demo.py: test code for studying exception handling in a coroutine_ 1234567891011121314151617181920212223&gt;&gt;&gt; class DemoException(Exception):... '''An exception type for the demonstration.'''... &gt;&gt;&gt; def demo_exc_handling():... print('-&gt; coroutine started')... while True:... try:... x = yield... except DemoException:... print('*** DemoException handled. Continuing...')... else:... print('-&gt; coroutine received: &#123;!r&#125;'.format(x))... raise RuntimeError('This line should never run.')... &gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.close()&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_CLOSED'&gt;&gt;&gt; _Throwing DemoException into demo_exc_handling does not break it_ 12345678910&gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.throw(DemoException)*** DemoException handled. Continuing...&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_SUSPENDED'&gt;&gt;&gt; Coroutine terminates if it can’t handle an exception thrown into it 123456789101112&gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.throw(ZeroDivisionError)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 5, in demo_exc_handlingZeroDivisionError&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_CLOSED' If it’s necessary that some cleanup code is run no matter how the coroutine ends, you need to wrap the relevant part of the coroutine body in a try/finally block. _coro_finally_demo.py: use of try/finally to perform actions on coroutine termination_ 123456789101112131415class DemoException(Exception): '''An exception type for the demonstration.'''def demo_finally(): print('-&gt; coroutine started') try: while True: try: x = yield except DemoException: print('*** DemoException handled. Continuing...') else: print('-&gt; coroutine received: &#123;!r&#125;'.format(x)) finally: print('-&gt; coroutine ending') Returning a Value from a Coroutinecoroaverager2.py: code for an averager coroutine that returns a result 12345678910111213141516from collections import namedtupleResult= namedtuple('Result', 'count average')def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: # 1 break total += term count += 1 average = total / count return Result(count, average) # 2 In order to return a value, a coroutine must terminate normally; this is why this version of averager has a condition to break out of its accumulating loop. Return a namedtuple with the count and average. Before Python 3.3, it was a syntax error to return a value in a generator function. coroaverager2.py: doctest showing the behavior of averager 12345678910&gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)&gt;&gt;&gt; coro_avg.send(30)&gt;&gt;&gt; coro_avg.send(None)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration: Result(count=2, average=20.0)# Sending None terminates the loop, causing the coroutine to end by returning the result. As usual, the # generator object raises StopIteration. The value attribute of the exception carries the value returned. Note that the value of the return expression is smuggled to the caller as an attribute of the StopIteration exception. This is a bit of a hack, but it preserves the existing behavior of generator objects: raising StopIteration when exhausted. Catching StopIteration lets us get the value returned by averager 123456789101112&gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)&gt;&gt;&gt; coro_avg.send(30)&gt;&gt;&gt; try:... coro_avg.send(None)... except StopIteration as exc:... result= exc.value... &gt;&gt;&gt; resultResult(count=2, average=20.0)&gt;&gt;&gt; Using yield fromThe first thing to know about yield from is that it is a completely new language construct. It does so much more than yield that the reuse of that keyword is arguably misleading. Similar constructs in other languages are called await, and that is a much better name because it conveys a crucial point: when a generator gen calls yield from subgen(), the subgen takes over and will yield values to the caller of gen; the caller will in effect drive subgen directly. Meanwhile gen will be blocked, waiting until subgen terminates. Chaining iterables with yield from 12345678&gt;&gt;&gt; def chain(*iterables):... for it in iterables:... yield from it...&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; t = tuple(range(3))&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2] The first thing the yield from x expression does with the x object is to call iter(x) to obtain an iterator from it. This means that x can be any iterable. However, if replacing nested for loops yielding values was the only contribution of yield from, this language addition wouldn’t have had a good chance of being accepted. The real nature of yield from cannot be demonstrated with simple iterables; it requires the mind-expanding use of nested generators. That’s why PEP 380, which introduced yield from, is titled “Syntax for Delegating to a Subgenerator.” The main feature of yield from is to open a bidirectional channel from the outermost caller to the innermost subgenerator, so that values can be sent and yielded back and forth directly from them, and exceptions can be thrown all the way in without adding a lot of exception handling boilerplate code in the intermediate coroutines. This is what enables coroutine delegation in a way that was not possible before. delegating generatorThe generator function that contains the yield from expression. subgeneratorThe generator obtained from the part of the yield from expression. This is the “subgenerator” mentioned in the title of PEP 380: “Syntax for Delegating to a Subgenerator.” callerPEP 380 uses the term “caller” to refer to the client code that calls the delegating generator. Depending on context, I use “client” instead of “caller,” to distinguish from the delegating generator, which is also a “caller” (it calls the subgenerator) coroaverager3.py: using yield from to drive averager and report statistics 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from collections import namedtupleResult= namedtuple('Result', 'count average')# the subgeneratordef averager(): total= 0.0 count= 0 average= None while True: term= yield # Each value sent by the client code in main will be bound to term here. if term is None: # The crucial terminating condition. Without it, a yield # from calling this coroutine will block forever. break total+= term count+= 1 average= total/ count return Result(count, average) # The returned Result will be the value of the yield from expression in grouper# the delegating generatordef grouper(results, key): while True:# Each iteration in thisloop creates a new instance of averager; # each is a generator object operating as a coroutine. results[key] = yield from averager() # Whenever grouper is sent a value, it’s piped into the averager instance # by the yield from. grouper will be suspended here as long as the # averager instance is consuming values sent by the client. When an # averager instance runs to the end, the value it returns is bound to # results[key]. The while loop then proceeds to create another averager # instance to consume more values.# the client codedef main(data): results=&#123;&#125; for key, values in data.items(): group= grouper(results, key) # group is a generator objectresulting from calling grouper with the # results dict to collect the results, and a particular key. It will # operate as a coroutine. next(group) # Prime the coroutine. for value in values: group.send(value) # Send each value into the grouper. That value ends up in the term = # yield line of averager; grouper never has a chance to see it. group.send(None) # Sending None into grouper causes the current averager instance to # terminate, and allows grouper to run again, which creates another # averager for the next group of values report(results)def report(results): for key, result in sorted(results.items()): group, unit = key.split(';') print('&#123;:2&#125; &#123;:5&#125; averaging &#123;:.2f&#125;&#123;&#125;'.format( result.count, group, result.average, unit ))data = &#123; 'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],&#125;if __name__ == '__main__': main(data) What would happen if we omitted the call group.send(None) marked “important!” in main:• Each iteration of the outer for loop creates a new grouper instance named group; this is the delegating generator.• The call next(group) primes the grouper delegating generator, which enters its while True loop and suspends at the yield from, after calling the subgenerator averager.• The inner for loop calls group.send(value); this feeds the subgenerator averager directly. Meanwhile, the current group instance of grouper is suspended at the yield from.• When the inner for loop ends, the group instance is still suspended at the yield from, so the assignment to results[key] in the body of grouper has not happened yet.• Without the last group.send(None) in the outer for loop, the averager subgenerator never terminates, the delegating generator group is never reactivated, and the assignment to results[key] never happens.• When execution loops back to the top of the outer for loop, a new grouper instance is created and bound to group. The previous grouper instance is garbage collected (together with its own unfinished averager subgenerator instance). Example 16-17 demonstrates the simplest arrangement of yield from, with only one delegating generator and one subgenerator. Because the delegating generator works as a pipe, you can connect any number of them in a pipeline: one delegating generator uses yield from to call a subgenerator, which itself is a delegating generator calling another subgenerator with yield from, and so on. Eventually this chain must end in a simple generator that uses just yield, but it may also end in any iterable object. Every yield from chain must be driven by a client that calls next(…) or .send(…) on the outermost delegating generator. This call may be implicit, such as a for loop. The Meaning of yield from• Any values that the subgenerator yields are passed directly to the caller of the delegating generator (i.e., the client code).• Any values sent to the delegating generator using send() are passed directly to the subgenerator. If the sent value is None, the subgenerator’s __next__() method is called. If the sent value is not None, the subgenerator’s send() method is called. If the call raises StopIteration, the delegating generator is resumed. Any other exception is propagated to the delegating generator.• return expr in a generator (or subgenerator) causes StopIteration(expr) to be raised upon exit from the generator.• The value of the yield from expression is the first argument to the StopIteration exception raised by the subgenerator when it terminates. The other two features of yield from have to do with exceptions and termination:• Exceptions other than GeneratorExit thrown into the delegating generator are passed to the throw() method of the subgenerator. If the call raises StopIteration, the delegating generator is resumed. Any other exception is propagated to the delegating generator.• If a GeneratorExit exception is thrown into the delegating generator, or the close() method of the delegating generator is called, then the close() method of the subgenerator is called if it has one. If this call results in an exception, it is propagated to the delegating generator. Otherwise, GeneratorExit is raised in the delegating generator.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C15_Context Managers and else Blocks]]></title>
    <url>%2F2018%2F05%2F25%2FC15-Context-Managers-and-else-Blocks%2F</url>
    <content type="text"><![CDATA[The with statement sets up a temporary context and reliably tears it down, under the control of a context manager object. This prevents errors and reduces boilerplate code, making APIs at the same time safer and easier to use. Python programmers are finding lots of uses for with blocks beyond automatic file closing. This is no secret, but it is an underappreciated language feature: the else clause can be used not only in if statements but also in for, while, and try statements. Do This, Then That: else Blocks Beyond ifforThe else block will run only if and when the for loop runs to completion (i.e., not if the for is aborted with a break). whileThe else block will run only if and when the while loop exits because the condition became falsy (i.e., not when the while is aborted with a break). tryThe else block will only run if no exception is raised in the try block. The official docs also state: “Exceptions in the else clause are not handled by the precedingexcept clauses.” In all cases, the else clause is also skipped if an exception or a return, break, or continue statement causes control to jump out of the main block of the compound statement. Context Managers and with BlocksThe context manager protocol consists of the __enter__ and __exit__ methods. At the start of the with, __enter__ is invoked on the context manager object. The role of the finally clause is played by a call to __exit__ on the context manager object at the end of the with block. mirror.py123456789101112131415161718192021222324class LookingGlass: def __enter__(self): import sys self.original_write= sys.stdout.write sys.stdout.write= self.reverse_write return 'JABBERWOCKY' def reverse_write(self, text): self.original_write(text[::-1]) #Python calls __exit__ with None, None, None if all went well; if an #exception is raised, the three arguments get the exception data, as #described next. def __exit__(self, exc_type, exc_value, traceback): import sys sys.stdout.write= self.original_write if exc_type is ZeroDivisionError: print('Please DO NOT divide by zero!') return True #return True to tell the interpreter that the exception was handled. #If __exit__ returns None or anything but True, any exception raised in #the with block will be propagated. The interpreter calls the __enter__ method with no arguments—beyond the implicit self. The three arguments passed to __exit__ are: exc_typeThe exception class (e.g., ZeroDivisionError). exc_valueThe exception instance. Sometimes, parameters passed to the exception constructor—such as the error message—can be found in exc_value.args. tracebackA traceback object. Exercising LookingGlass without a with block 1234567891011121314&gt;&gt;&gt; from mirror import LookingGlass &gt;&gt;&gt; manager = LookingGlass() &gt;&gt;&gt; manager &lt;mirror.LookingGlass object at 0x2a578ac&gt; &gt;&gt;&gt; monster = manager.__enter__() &gt;&gt;&gt; monster == 'JABBERWOCKY' eurT &gt;&gt;&gt; monster 'YKCOWREBBAJ' &gt;&gt;&gt; manager &gt;ca875a2x0 ta tcejbo ssalGgnikooL.rorrim&lt; &gt;&gt;&gt; manager.__exit__(None, None, None) &gt;&gt;&gt; monster 'JABBERWOCKY' The contextlib UtilitiesclosingA function to build context managers out of objects that provide a close() method but don’t implement the __enter__/__exit__ protocol. suppressA context manager to temporarily ignore specified exceptions. @contextmanagerA decorator that lets you build a context manager from a simple generator function, instead of creating a class and implementing the protocol. ContextDecoratorA base class for defining class-based context managers that can also be used as function decorators, running the entire function within a managed context. ExitStackA context manager that lets you enter a variable number of context managers. When the with block ends, ExitStack calls the stacked context managers’ __exit__ methods in LIFO order (last entered, first exited). Use this class when you don’t know beforehand how many context managers you need to enter in your with block; for example, when opening all files from an arbitrary list of files at the same time. Using @contextmanagerThe @contextmanager decorator reduces the boilerplate of creating a context manager: instead of writing a whole class with __enter__/__exit__ methods, you just implement a generator with a single yield that should produce whatever you want the __enter__ method to return. In a generator decorated with @contextmanager, yield is used to split the body of the function in two parts: everything before the yield will be executed at the beginning of the while block when the interpreter calls __enter__; the code after yield will run when __exit__ is called at the end of the block. _mirror_gen.py_ 12345678910111213141516171819202122import contextlib@contextlib.contextmanagerdef looking_glass(): import sys original_write= sys.stdout.write #Define custom reverse_write function; original_write will be available in #the closure. def reverse_write(text): original_write(text[::-1]) sys.stdout.write = reverse_write #Yield the value that will be bound to the target variable in the as clause #of the with statement. This function pauses at this point while the body of #the with executes. yield 'JABBERWOCKY' #When control exits the with block in any way, execution continues after #the yield; here the original sys.stdout.write is restored. sys.stdout.write = original_write _Test driving the looking_glass context manager function_ 123456789&gt;&gt;&gt; from mirror_gen import looking_glass &gt;&gt;&gt; with looking_glass() as what: ... print('Alice, Kitty and Snowdrop') ... print(what) ... pordwonS dna yttiK ,ecilA YKCOWREBBAJ &gt;&gt;&gt; what 'JABBERWOCKY' Essentially the contextlib.contextmanager decorator wraps the function in a class that implements the __enter__ and __exit__ methods. The __enter__ method of that class: Invokes the generator function and holds on to the generator object—let’s call it gen. Calls next(gen) to make it run to the yield keyword. Returns the value yielded by next(gen), so it can be bound to a target variable in the with/as form. When the with block terminates, the __exit__ method: Checks an exception was passed as exc_type; if so, gen.throw(exception) is invoked, causing the exception to be raised in the yield line inside the generator function body. Otherwise, next(gen) is called, resuming the execution of the generator function body after the yield. _mirror_gen_exc.py: generator-based context manager implementing ex‐ception handling_ 1234567891011121314151617181920import contextlib@contextlib.contextmanagerdef looking_glass(): import sys original_write= sys.stdout.write def reverse_write(text): original_write(text[::-1]) sys.stdout.write= reverse_write msg='' try: yield 'JABBERWOCKY' except ZeroDivisionError: msg= 'Please DO NOT divide by zero!' finally: sys.stdout.write= original_write if msg: print(msg)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C14_Iterables, Iterators, and Generators_02]]></title>
    <url>%2F2018%2F05%2F25%2FC14-Iterables-Iterators-and-Generators-02%2F</url>
    <content type="text"><![CDATA[The ArithmeticProgression class 1234567891011121314151617class ArithmeticProgression: def __init__(self, begin, step, end=None): self.begin=begin self.step=step self.end=end def __iter__(self): #This line produces a result value equal to self.begin, but coerced to #the type of the subsequent additions. result=type(self.begin+self.step)(self.begin) forever=self.end is None index = 0 while forever or result &lt; self.end: yield result index +=1 result =self.begin + self.step * index _The aritprog_gen generator function_ 12345678def aritprog_gen(begin, step, end=None): result= type(begin + step)(begin) forever=end is None index = 0 while forever or result &lt; end: yield result index +=1 result = begin + step * index Arithmetic Progression with itertoolsitertools.count function returns a generator that produces numbers. Without arguments, it produces a series of integers starting with 0. But you can provide optional start and step values to achieve a result very similar to our aritprog_gen functions: 123456789&gt;&gt;&gt; import itertools&gt;&gt;&gt; gen=itertools.count(1,.5)&gt;&gt;&gt; next(gen)1&gt;&gt;&gt; next(gen)1.5&gt;&gt;&gt; &gt;&gt;&gt; next(gen)2.0 itertools.count never stops, so if you call list(count()), Python will try to build a list larger than available memory and your machine will be very grumpy long before the call fails. On the other hand, there is the itertools.takewhile function: it produces a generator that consumes another generator and stops when a given predicate evaluates to False. So we can combine the two and write this: 123&gt;&gt;&gt; gen=itertools.takewhile(lambda n: n &lt; 3, itertools.count(1, .5))&gt;&gt;&gt; list(gen)[1, 1.5, 2.0, 2.5] _aritprog_v3.py_ 12345678import itertoolsdef aritprog_gen(begin, step, end= None): first= type(begin+ step)(begin) ap_gen=itertools.count(first, step) if end is not None: ap_gen= itertools.takewhile(lambda n: n &lt; end, ap_gen) return ap_gen Generator Functions in the Standard LibraryFiltering generator functions examples 12345678910111213141516&gt;&gt;&gt; def vowel(c): return c.lower() in 'aeiou'... &gt;&gt;&gt; list(filter(vowel, 'Aardvardk'))['A', 'a', 'a']&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.filterfalse(vowel, 'Aardvark'))['r', 'd', 'v', 'r', 'k']&gt;&gt;&gt; list(itertools.dropwhile(vowel, 'Aardvark'))['r', 'd', 'v', 'a', 'r', 'k']&gt;&gt;&gt; list(itertools.takewhile(vowel, 'Aardvark'))['A', 'a']&gt;&gt;&gt; list(itertools.compress('Aardvark', (1,0,1,1,0,1)))['A', 'r', 'd', 'a']&gt;&gt;&gt; list(itertools.islice('Aardvark', 1,7,2))['a', 'd', 'a']&gt;&gt;&gt; itertools.accumulate generator function examples 12345678910111213&gt;&gt;&gt; sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.accumulate(sample)) #[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]&gt;&gt;&gt; list(itertools.accumulate(sample, min)) #[5, 4, 2, 2, 2, 2, 2, 0, 0, 0]&gt;&gt;&gt; list(itertools.accumulate(sample, max)) #[5, 5, 5, 8, 8, 8, 8, 8, 9, 9]&gt;&gt;&gt; import operator&gt;&gt;&gt; list(itertools.accumulate(sample, operator.mul)) #[5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]&gt;&gt;&gt; list(itertools.accumulate(range(1, 11), operator.mul))[1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800] Mapping generator function examples1234567891011121314151617&gt;&gt;&gt; list(enumerate('albatroz', 1)) #[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')]&gt;&gt;&gt; import operator&gt;&gt;&gt; list(map(operator.mul, range(11), range(11))) #[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&gt;&gt;&gt; list(map(operator.mul, range(11), [2, 4, 8])) #[0, 4, 16]&gt;&gt;&gt; list(map(lambda a, b: (a, b), range(11), [2, 4, 8])) # [(0, 2), (1, 4), (2, 8)]&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.starmap(operator.mul, enumerate('albatroz', 1))) #['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz']&gt;&gt;&gt; sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]&gt;&gt;&gt; list(itertools.starmap(lambda a, b: b/a,... enumerate(itertools.accumulate(sample), 1))) #[5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333,5.0, 4.375, 4.888888888888889, 4.5] Merging generator function examples 123456789101112131415&gt;&gt;&gt; list(itertools.chain('ABC', range(2))) #['A', 'B', 'C', 0, 1]&gt;&gt;&gt; list(itertools.chain(enumerate('ABC'))) #chain does nothing useful when called with a single iterable[(0, 'A'), (1, 'B'), (2, 'C')]&gt;&gt;&gt; list(itertools.chain.from_iterable(enumerate('ABC'))) #[0, 'A', 1, 'B', 2, 'C']&gt;&gt;&gt; list(zip('ABC', range(5))) #[('A', 0), ('B', 1), ('C', 2)]&gt;&gt;&gt; list(zip('ABC', range(5), [10, 20, 30, 40])) #[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5))) #[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5), fillvalue='?')) #[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)] itertools.product generator function examples 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; list(itertools.product('ABC', range(2))) #[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]&gt;&gt;&gt; suits = 'spades hearts diamonds clubs'.split()&gt;&gt;&gt; list(itertools.product('AK', suits)) #[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'),('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')]&gt;&gt;&gt; list(itertools.product('ABC')) #[('A',), ('B',), ('C',)]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2)) #[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'),('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]#The repeat=N keyword argument tells product to consume each input iterable N times.&gt;&gt;&gt; list(itertools.product(range(2), repeat=3))[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),(1, 0, 1), (1, 1, 0), (1, 1, 1)]&gt;&gt;&gt; rows = itertools.product('AB', range(2), repeat=2)&gt;&gt;&gt; for row in rows: print(row)...('A', 0, 'A', 0)('A', 0, 'A', 1)('A', 0, 'B', 0)('A', 0, 'B', 1)('A', 1, 'A', 0)('A', 1, 'A', 1)('A', 1, 'B', 0)('A', 1, 'B', 1)('B', 0, 'A', 0)('B', 0, 'A', 1)('B', 0, 'B', 0)('B', 0, 'B', 1)('B', 1, 'A', 0)('B', 1, 'A', 1)('B', 1, 'B', 0)('B', 1, 'B', 1) Generator functions that expand each input item into multiple output items count, cycle, and repeat 12345678910111213141516171819&gt;&gt;&gt; ct = itertools.count() #&gt;&gt;&gt; next(ct) #0&gt;&gt;&gt; next(ct), next(ct), next(ct) #(1, 2, 3)&gt;&gt;&gt; list(itertools.islice(itertools.count(1, .3), 3)) #[1, 1.3, 1.6]&gt;&gt;&gt; cy = itertools.cycle('ABC') #&gt;&gt;&gt; next(cy)'A'&gt;&gt;&gt; list(itertools.islice(cy, 7)) #['B', 'C', 'A', 'B', 'C', 'A', 'B']&gt;&gt;&gt; rp = itertools.repeat(7) #&gt;&gt;&gt; next(rp), next(rp)(7, 7)&gt;&gt;&gt; list(itertools.repeat(8, 4)) #[8, 8, 8, 8]&gt;&gt;&gt; list(map(operator.mul, range(11), itertools.repeat(5))) #[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50] Combinatoric generator functions yield multiple values per input item123456789&gt;&gt;&gt; list(itertools.combinations('ABC', 2)) #All combinations of len()==2 from the items in 'ABC'[('A', 'B'), ('A', 'C'), ('B', 'C')]&gt;&gt;&gt; list(itertools.combinations_with_replacement('ABC', 2)) #including combinations with repeated items.[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]&gt;&gt;&gt; list(itertools.permutations('ABC', 2)) #All permutations of len()==2 from the items in 'ABC'[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2)) #Cartesian product from 'ABC' and 'ABC' (that’s the effect of repeat=2)[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] Rearranging generator functions Note that itertools.groupby assumes that the input iterable is sorted by the grouping criterion, or at least that the items are clustered by that criterion—even if not sorted. itertools.groupby 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; list(itertools.groupby('LLLLAAGGG')) #[('L', &lt;itertools._grouper object at 0x102227cc0&gt;),('A', &lt;itertools._grouper object at 0x102227b38&gt;),('G', &lt;itertools._grouper object at 0x102227b70&gt;)]&gt;&gt;&gt; for char, group in itertools.groupby('LLLLAAAGG'): #... print(char, '-&gt;', list(group))...L -&gt; ['L', 'L', 'L', 'L']A -&gt; ['A', 'A',]G -&gt; ['G', 'G', 'G']&gt;&gt;&gt; animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear',... 'bat', 'dolphin', 'shark', 'lion']&gt;&gt;&gt; animals.sort(key=len) #To use groupby, the input should be sorted;&gt;&gt;&gt; animals['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark','giraffe', 'dolphin']&gt;&gt;&gt; for length, group in itertools.groupby(animals, len): #... print(length, '-&gt;', list(group))...3 -&gt; ['rat', 'bat']4 -&gt; ['duck', 'bear', 'lion']5 -&gt; ['eagle', 'shark']7 -&gt; ['giraffe', 'dolphin']&gt;&gt;&gt; for length, group in itertools.groupby(reversed(animals), len): #... print(length, '-&gt;', list(group))...7 -&gt; ['dolphin', 'giraffe']5 -&gt; ['shark', 'eagle']4 -&gt; ['lion', 'bear', 'duck']3 -&gt; ['bat', 'rat'] itertools.tee yields multiple generators, each yielding every item of the input generator123456789101112131415&gt;&gt;&gt; list(itertools.tee('ABC'))[&lt;itertools._tee object at 0x10222abc8&gt;, &lt;itertools._tee object at 0x10222ac08&gt;]&gt;&gt;&gt; g1, g2 = itertools.tee('ABC')&gt;&gt;&gt; next(g1)'A'&gt;&gt;&gt; next(g2)'A'&gt;&gt;&gt; next(g2)'B'&gt;&gt;&gt; list(g1)['B', 'C']&gt;&gt;&gt; list(g2)['C']&gt;&gt;&gt; list(zip(*itertools.tee('ABC')))[('A', 'A'), ('B', 'B'), ('C', 'C')] New Syntax in Python 3.3: yield from a homemade implementation of a chaining generator: 123456789101112131415161718&gt;&gt;&gt; def chain(*iterables):... for it in iterables:... for i in it:... yield i... &gt;&gt;&gt; s='ABC'&gt;&gt;&gt; t=tuple(range(3))&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2]&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2]&gt;&gt;&gt; def chain(*iterables):... for i in iterables:... yield from i... &gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2] As you can see, yield from i replaces the inner for loop completely. The use of yield from in this example is correct, and the code reads better, but it seems like mere syntactic sugar. Besides replacing a loop, yield from creates a channel connecting the inner generator directly to the client of the outer generator. This channel becomes really important when generators are used as coroutines and not only produce but also consume values from the client code. Iterable Reducing FunctionsBuilt-in functions that read iterables and return single values 12345678910111213141516171819&gt;&gt;&gt; all([1, 2, 3])True&gt;&gt;&gt; all([1, 0, 3])False&gt;&gt;&gt; all([])True&gt;&gt;&gt; any([1, 2, 3])True&gt;&gt;&gt; any([1, 0, 3])True&gt;&gt;&gt; any([0, 0.0])False&gt;&gt;&gt; any([])False&gt;&gt;&gt; g = (n for n in [0, 0.0, 7, 8])&gt;&gt;&gt; any(g)True&gt;&gt;&gt; next(g)8 Another built-in that takes an iterable and returns something else is sorted. Unlike reversed, which is a generator function, sorted builds and returns an actual list. After all, every single item of the input iterable must be read so they can be sorted, and the sorting happens in a list, therefore sorted just returns that list after it’s done. I mention sorted here because it does consume an arbitrary iterable. Of course, sorted and the reducing functions only work with iterables that eventually stop. Otherwise, they will keep on collecting items and never return a result. A Closer Look at the iter Functioniter has another trick: it can be called with two arguments to create an iterator from a regular function or any callable object. In this usage, the first argument must be a callable to be invoked repeatedly (with no arguments) to yield values, and the second argument is a sentinel: a marker value which, when returned by the callable, causes the iterator to raise StopIteration instead of yielding the sentinel. The following example shows how to use iter to roll a six-sided die until a 1 is rolled: 12345678910111213&gt;&gt;&gt; def d6():... return randint(1, 6)...&gt;&gt;&gt; d6_iter = iter(d6, 1)&gt;&gt;&gt; d6_iter&lt;callable_iterator object at 0x00000000029BE6A0&gt;&gt;&gt;&gt; for roll in d6_iter:... print(roll)...4363 reads lines from a file until a blank line is found or the end of file is reached: 123with open('mydata.txt') as fp: for line in iter(fp.readline, ''): process_line(line)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C14_Iterables, Iterators, and Generators_01]]></title>
    <url>%2F2018%2F05%2F25%2FC14-Iterables-Iterators-and-Generators-01%2F</url>
    <content type="text"><![CDATA[Every collection in Python is iterable, and iterators are used internally to support:• for loops• Collection types construction and extension• Looping over text files line by line• List, dict, and set comprehensions• Tuple unpacking• Unpacking actual parameters with * in function callssentence.py: A Sentence as a sequence of words 123456789101112131415161718192021import reimport reprlibRE_WORD=re.compile('\w+')class Sentence: def __init__(self, text): self.text = text self.words=RE_WORD.findall(text) def __getitem__(self, index): return self.words[index] def __len__(self): return len(self.words) # reprlib.repr is a utility function to generate abbreviated string # representations of data structures that can be very large. def __repr__(self): return 'Sentence(%s)'% reprlib.repr(self.text) Why Sequences Are Iterable: The iter FunctionWhenever the interpreter needs to iterate over an object x, it automatically calls iter(x). The iter built-in function: Checks whether the object implements __iter__, and calls that to obtain an iterator. If __iter__ is not implemented, but __getitem__ is implemented, Python creates an iterator that attempts to fetch items in order, starting from index 0 (zero). If that fails, Python raises TypeError, usually saying “C object is not iterable,” where C is the class of the target object. That is why any Python sequence is iterable: they all implement __getitem__. In fact,the standard sequences also implement __iter__, and yours should too, because the special handling of __getitem__ exists for backward compatibility reasons and may be gone in the future. In the goose-typing approach, the definition for an iterable is simpler but not as flexible: an object is considered iterable if it implements the __iter__ method. No subclassing or registration is required, because abc.Iterable implements the __subclasshook__. 12345678910&gt;&gt;&gt; class Foo:... def __iter__(self):... pass...&gt;&gt;&gt; from collections import abc&gt;&gt;&gt; issubclass(Foo, abc.Iterable)True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; isinstance(f, abc.Iterable)True However, note that our initial Sentence class does not pass the issubclass(Sentence, abc.Iterable) test, even though it is iterable in practice. Iterables Versus IteratorsiterableAny object from which the iter built-in function can obtain an iterator. Objects implementing an __iter__ method returning an iterator are iterable. Sequences are always iterable; as are objects implementing a __getitem__ method that takes 0-based indexes. It’s important to be clear about the relationship between iterables and iterators: Python obtains iterators from iterables. 123456789101112&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; it = iter(s) # Build an iterator it from the iterable.&gt;&gt;&gt; while True:... try:... print(next(it)) # Repeatedly call next on the iterator to obtain the next item.... except StopIteration: # The iterator raises StopIteration when there are no further items.... del it # Release reference to it—the iterator object is discarded.... break #...ABC StopIteration signals that the iterator is exhausted. This exception is handled internally in for loops and other iteration contexts like list comprehensions, tuple unpacking, etc. The standard interface for an iterator has two methods: __next__Returns the next available item, raising StopIteration when there are no more items. __iter__Returns self; this allows iterators to be used where an iterable is expected, for example, in a for loop. This is formalized in the collections.abc.Iterator ABC, which defines the __next__ abstract method, and subclasses Iterable—where the abstract __iter__ method is defined. The best way to check if an object x is an iterator is to call isinstance(x, abc.Iterator). Thanks to Iterator.__subclasshook__, this test works even if the class of x is not a real or virtual subclass of Iterator. iteratorAny object that implements the__next__ no-argument method that returns the next item in a series or raises StopIteration when there are no more items. Python iterators also implement the __iter__ method so they are iterable as well. A Classic Iterator_sentence_iter.py_ 123456789101112131415161718192021222324252627282930313233import reimport reprlibRE_WORD=re.compile('\w+')class Sentence: def __init__(self, text): self.text=text self.words=RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return SentenceIterator(self.words)class SentenceIterator: def __init__(self, words): self.words=words self.index=0 def __next__(self): try: word=self.words[self.index] except IndexError: raise StopIteration() self.index += 1 return word def __iter__(self): return self Note that implementing__iter__ in SentenceIterator is not actually needed for this example to work, but the it’s the right thing to do: iterators are supposed to implement both __next__ and __iter__, and doing so makes our iterator pass the issubclass(Sen tenceInterator, abc.Iterator) test. If we had subclassed SentenceIterator from abc.Iterator, we’d inherit the concrete abc.Iterator.__iter__ method. Making Sentence an Iterator: Bad IdeaA common cause of errors in building iterables and iterators is to confuse the two. To be clear: iterables have an __iter__ method that instantiates a new iterator every time. Iterators implement a __next__ method that returns individual items, and an __iter__ method that returns self. Therefore, iterators are also iterable, but iterables are not iterators. It may be tempting to implement __next__ in addition to __iter__ in the Sentence class, making each Sentence instance at the same time an iterable and iterator over itself. But this is a terrible idea. It’s also a common anti-pattern, according to Alex Martelli who has a lot of experience with Python code reviews. A Generator Function _sentence_gen.py_ 123456789101112131415161718192021import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text self.words= RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for word in self.words: yield word return #This return is not needed; the function can just “fall-through” and return#automatically. Either way, a generator function doesn’t raise StopIteration: #it simply exits when it’s done producing values. __iter__ is a generator function which, when called, builds a generator object that implements the iterator interface, so the SentenceIterator class is no longer needed. How a Generator Function WorksAny Python function that has the yield keyword in its body is a generator function: a function which, when called, returns a generator object. In other words, a generator function is a generator factory. 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; def gen_123():... yield 1... yield 2... yield 3... &gt;&gt;&gt; gen_123&lt;function gen_123 at 0x05155C90&gt;&gt;&gt;&gt; gen_123()&lt;generator object gen_123 at 0x05157540&gt; #when invoked, gen_123() returns a generator object.#Generators are iterators that produce the values of the expressions passed to yield.# To iterate, the for machinery does the equivalent of g = iter(gen_123()) to get a generator object, and # then next(g) at each iteration.&gt;&gt;&gt; for i in gen_123(): print(i)... 123&gt;&gt;&gt; g=gen_123()&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)2&gt;&gt;&gt; next(g)3&gt;&gt;&gt; next(g)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration When the generator function body runs to the end, the generator object raises StopIteration. The for loop machinery catches that exception, and the loop terminates cleanly. A Lazy ImplementationThe Iterator interface is designed to be lazy: next(my_iterator) produces one item at a time. The opposite of lazy is eager: lazy evaluation and eager evaluation are actual technical terms in programming language theory. Our Sentence implementations so far have not been lazy because the __init__ eagerly builds a list of all words in the text, binding it to the self.words attribute. This will entail processing the entire text, and the list may use as much memory as the text itself. The re.finditer function is a lazy version of re.findall which, instead of a list, returns a generator producing re.MatchObject instances on demand. If there are many matches, re.finditer saves a lot of memory. Using it, our third version of Sentence is now lazy: it only produces the next word when it is needed. _sentence_gen2.py_ 1234567891011121314151617181920import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): #finditer builds an iterator over the matches of RE_WORD on self.text, # yielding MatchObject instances. for match in RE_WORD.finditer(self.text): yield match.group() #match.group() extracts the actual matched text from the #MatchObject instance A Generator ExpressionSimple generator functions like the one in the previous Sentence class can be replaced by a generator expression. A generator expression can be understood as a lazy version of a list comprehension: it does not eagerly build a list, but returns a generator that will lazily produce the items on demand. In other words, if a list comprehension is a factory of lists, a generator expression is a factory of generators. _sentence_genexp.py: Sentence implemented using a generator expression_ 123456789101112131415import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text))]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C13_Operator Overloading: Doing It Right]]></title>
    <url>%2F2018%2F05%2F25%2FC13-Operator-Overloading-Doing-It-Right%2F</url>
    <content type="text"><![CDATA[We cannot overload operators for the built-in types. We cannot create new operators, only overload existing ones. A few operators can’t be overloaded: is, and, or, not (but the bitwise &amp;, |, ~, can). Unary OperatorsIt’s easy to support the unary operators. Simply implement the appropriate special method, which will receive just one argument: self. Use whatever logic makes sense in your class, but stick to the fundamental rule of operators: always return a new object. In other words, do not modify self, but create and return a new instance of a suitable type. In the case of - and +, the result will probably be an instance of the same class as self; for +, returning a copy of self is the best approach most of the time. For abs(…), the result should be a scalar number. As for ~, it’s difficult to say what would be a sensible result if you’re not dealing with bits in an integer, but in an ORM it could make sense to return the negation of an SQL WHERE clause, for example. _vector_v6.py: unary operators - and + added_ 123456def __abs__(self): return math.sqrt(sum(x * x for x in self)) def __neg__(self): return Vector(-x for x in self) def __pos__(self): #To compute +v, build a new Vector with every component of self. return Vector(self) Overloading + for Vector Addition1234**_Vector.add method, take #1_**def __add__(self, other): pairs=itertools.zip_longest(self, other, fillvalue = 0.0) return Vector(a +b for a, b in pairs) To support operations involving objects of different types, Python implements a special dispatching mechanism for the infix operator special methods. Given an expression a+ b, the interpreter will perform these steps: If a has __add__, call a.__add__(b) and return result unless it’s NotImplemented. If a doesn’t have __add__, or calling it returns NotImplemented, check if b has __radd__, then call b.__radd__(a) and return result unless it’s NotImplemented. If b doesn’t have __add__, or calling it returns NotImplemented, raise TypeError with an unsupported operand types message. The __radd__ method is called the “reflected” or “reversed” version of __add__. I prefer to call them “reversed” special methods. Three of this book’s technical reviewers—Alex,Anna, and Leo—told me they like to think of them as the “right” special methods, because they are called on the righthand operand. Therefore, to make the mixed-type additions work, we need to implement the Vector.__radd__ method, which Python will invoke as a fall back if the left operand does not implement __add__ or if it does but returns NotImplemented to signal that it doesn’t know how to handle the right operand. Vector.add and radd methods1234567def __add__(self, other): pairs=itertools.zip_longest(self, other, fillvalue = 0.0) return Vector(a +b for a, b in pairs)def __radd__(self, other): return self + other The methods work with Vector objects, or any iterable with numeric items, such as a Vector2d, a tuple of integers, or an array of floats. But if provided with a noniterable object,__add__ fails with a message that is not very helpful. Vector.add method needs an iterable operand 123456&gt;&gt;&gt; v1 + 1Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "vector_v6.py", line 328, in __add__ pairs = itertools.zip_longest(self, other, fillvalue=0.0)TypeError: zip_longest argument #2 must support iteration Vector.add method needs an iterable with numeric items 12345678910&gt;&gt;&gt; v1 + 'ABC'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "vector_v6.py", line 329, in __add__ return Vector(a + b for a, b in pairs) File "vector_v6.py", line 243, in __init__ self._components = array(self.typecode, components) File "vector_v6.py", line 329, in &lt;genexpr&gt; return Vector(a + b for a, b in pairs)TypeError: unsupported operand type(s) for +: 'float' and 'str' In the spirit of duck typing, we will refrain from testing the type of the other operand, or the type of its elements. We’ll catch the exceptions and return NotImplemented.By returning NotImplemented, you leave the door open for the implementer of the other operand type to perform the operation when Python tries the reversed method call. If the reverse method call returns NotImplemented, then Python will raise issue TypeError with a standard error message like “unsupported operand type(s) for +: Vector and str.” _vector_v6.py: operator + methods added to vector_v5.py_12345678910def __add__(se;f, other): try: pairs=itertools.zip_longest(self, other, fillvalue= 0.0) return Vector(a+b for a, b in pairs) except TypeError: return NotImplementeddef __radd__(self, other): return self + other Overloading * for Scalar MultiplicationWe could use the same duck typing technique and catch aTypeError in __mul__, but there is another, more explicit way that makes sense in this situation: goose typing. We use isinstance() to check the type of scalar, but instead of hardcoding some concrete types, we check against the numbers.Real ABC, which covers all the types we need, and keeps our implementation open to future numeric types that declare themselves actual or virtual subclasses of the numbers.Real ABC. _vector_v7.py: operator * methods added_ 12345678910111213141516171819202122from array import arrayimport reprlibimport mathimport functoolsimport operatorsimport itertoolsimport numbersclass Vector: typecode='d' def __init__(self, components): self._components=array(self.typecode, compoents) def __mul__(self, scalar): if isinstance(scalar, number.Real): return Vector(n*scalar for n in self) else: return NotImplemented def __rmul__(self, scalar): return self * scalar Rich Comparison OperatorsThe handling of the rich comparison operators ==, !=, &gt;, &lt;, &gt;=, &lt;= by the Python interpreter is similar to what we just saw, but differs in two important aspects: The same set of methods are used in forward and reverse operator calls. The rules are summarized in Table 13-2. For example, in the case of ==, both the forward and reverse calls invoke __eq__, only swapping arguments; and a forward call to __gt__ is followed by a reverse call to __lt__ with the swapped arguments. In the case of == and !=, if the reverse call fails, Python compares the object IDs instead of raising TypeError. _vector_v8.py: improved eq in the Vector class_ 12345678910111213141516171819def __eq__(self, other): if isinstance(other, Vector): return (len(self) == len(other) and all(a==b for a,b in zip(self, other))) else: return NotImplemented**_Same comparisons_:**&gt;&gt;&gt; va = Vector([1.0, 2.0, 3.0])&gt;&gt;&gt; vb = Vector(range(1, 4))&gt;&gt;&gt; va == vb #True&gt;&gt;&gt; vc = Vector([1, 2])&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; v2d = Vector2d(1, 2)&gt;&gt;&gt; vc == v2d #Same result as before, but why? Explanation coming up.True&gt;&gt;&gt; t3 = (1, 2, 3)&gt;&gt;&gt; va == t3 #False Here is what happens in the example with a Vector and a Vector2d, step by step: To evaluate vc == v2d, Python calls Vector.__eq__(vc, v2d). Vector.__eq__(vc, v2d) verifies that v2d is not a Vector and returns NotImplemented. Python gets NotImplemented result, so it tries Vector2d.__eq__(v2d, vc). Vector2d.__eq__(v2d, vc) turns both operands into tuples an compares them: the result is True. As for the comparison between Vector and tuple, the actual steps are: To evaluate va == t3, Python calls Vector.__eq__(va, t3). Vector.__eq__(va, t3) verifies that t3 is not a Vector and returns NotImplemented. Python gets NotImplemented result, so it tries tuple.__eq__(t3, va). tuple.__eq__(t3, va) has no idea what a Vector is, so it returns NotImplemented. In the special case of ==, if the reversed call returns NotImplemented, Python compares object IDs as a last resort. How about !=? We don’t need to implement it because the fallback behavior of the __ne__ inherited from object suits us: when __eq__ is defined and does not return NotImplemented, __ne__ returns that result negated. If a class does not implement the in-place operators., the augmented assignment operators are just syntactic sugar: a += b is evaluated exactly as a = a +b. That’s the expected behavior for immutable types, and if you have __add__ then += will work with no additional code. However, if you do implement an in-place operator method such as __iadd__, that method is called to compute the result of a += b. As the name says, those operators are expected to change the lefthand operand in place, and not create a new object as the result. bingoaddable.py: AddableBingoCage extends BingoCage to support + and +=123456789101112131415161718192021222324import itertoolsfrom tombola import Tombolafrom bingo import BingoCageclass AddableBingoCage(BingoCage): def __add__(self, other): if isinstance(other, Tombola): return AddableBingoCage(self.inspect()+other.inspect()) else: return NotImplemented def __iadd__(self, other): if isinstance(other, Tombola): other_iterable=other.inspect() else: try: other_iterable=iter(other) except TypeError: self_cls=type(self).__name__ msg="right operand in += must be &#123;!r&#125; or an iterable" raise TypeError(msg.format(self_cls)) self.load(other_iterable) return self __add__The result is produced by calling the constructor AddableBingoCage to build a new instance. __iadd__The result is produced by returning self, after it has been modified.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C12_Inheritance: For Good or For Worse_02]]></title>
    <url>%2F2018%2F05%2F25%2FC12-Inheritance-For-Good-or-For-Worse-02%2F</url>
    <content type="text"><![CDATA[Distinguish Interface Inheritance from Implementation InheritanceWhen dealing with multiple inheritance, it’s useful to keep straight the reasons why subclassing is done in the first place. The main reasons are: Inheritance of interface creates a subtype, implying an “is-a” relationship. Inheritance of implementation avoids code duplication by reuse. In practice, both uses are often simultaneous, but whenever you can make the intent clear, do it. Inheritance for code reuse is an implementation detail, and it can often be replaced by composition and delegation. On the other hand, interface inheritance is the backbone of a framework. Make Interfaces Explicit with ABCsIn modern Python, if a class is designed to define an interface, it should be an explicit ABC. Use Mixins for Code ReuseIf a class is designed to provide method implementations for reuse by multiple unrelated subclasses, without implying an “is-a” relationship, it should be an explicit mixin class.Conceptually, a mixin does not define a new type; it merely bundles methods for reuse. A mixin should never be instantiated, and concrete classes should not inherit only from a mixin. Each mixin should provide a single specific behavior, implementing few and very closely related methods. Make Mixins Explicit by NamingThere is no formal way in Python to state that a class is a mixin, so it is highly recommended that they are named with a …Mixin suffix. Tkinter does not follow this advice, but if it did, XView would be XViewMixin, Pack would be PackMixin, and so on. An ABC May Also Be a Mixin; The Reverse Is Not TrueBecause an ABC can implement concrete methods, it works as a mixin as well. An ABC also defines a type, which a mixin does not. And an ABC can be the sole base class of any other class, while a mixin should never be subclassed alone except by another, more specialized mixin—not a common arrangement in real code. One restriction applies to ABCs and not to mixins: the concrete methods implemented in an ABC should only collaborate with methods of the same ABC and its superclasses.This implies that concrete methods in an ABC are always for convenience, because everything they do, a user of the class can also do by calling other methods of the ABC. Don’t Subclass from More Than One Concrete ClassConcrete classes should have zero or at most one concrete superclass.In other words, all but one of the superclasses of a concrete class should be ABCs or mixins. For example, in the following code, if Alpha is a concrete class, then Beta and Gamma must be ABCs or mixins: 123class MyConcreteClass(Alpha, Beta, Gamma): """This is a concrete class: it can be instantiated.""" # ... more code ... Provide Aggregate Classes to UsersIf some combination of ABCs or mixins is particularly useful to client code, provide a class that brings them together in a sensible way. Grady Booch calls this an aggregate class. 12345class Widget(BaseWidget, Pack, Place, Grid): """Internal class. Base class for a widget which can be positioned with the geometry managers Pack, Place or Grid.""" pass The body of Widget is empty, but the class provides a useful service: it brings together four superclasses so that anyone who needs to create a new widget does not need to remember all those mixins, or wonder if they need to be declared in a certain order in a class statement. “Favor Object Composition Over Class Inheritance.”Favoring composition leads to more flexible designs. For example, in the case of the tkinter.Widget class, instead of inheriting the methods from all geometry managers, widget instances could hold a reference to a geometry manager, and invoke its methods. After all, a Widget should not “be” a geometry manager, but could use the services of one via delegation. Then you could add a new geometry manager without touching the widget class hierarchy and without worrying about name clashes. Even with single inheritance, this principle enhances flexibility, because subclassing is a form of tight coupling, and tall inheritance trees tend to be brittle. Composition and delegation can replace the use of mixins to make behaviors available to different classes, but cannot replace the use of interface inheritance to define a hierarchy of types.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C12_Inheritance: For Good or For Worse_01]]></title>
    <url>%2F2018%2F05%2F25%2FC12-Inheritance-For-Good-or-For-Worse-01%2F</url>
    <content type="text"><![CDATA[It was not possible to subclass built-in types such as list or dict. Since then, it can be done but there is a major caveat: the code of the built-ins (written in C) does not call special methods overridden by user-defined classes.1234567891011121314151617181920&gt;&gt;&gt; class DoppelDict(dict):... def __setitem__(self, key, value):... super().__setitem__(key, [value] * 2)... &gt;&gt;&gt; dd = DoppelDict(one=1)&gt;&gt;&gt; dd&#123;'one': 1&#125;#The __init__ method inherited from dict clearly ignored that __setitem__ was overridden: the value of #'one' is not duplicated.&gt;&gt;&gt; dd['two'] = 2&gt;&gt;&gt; dd&#123;'one': 1, 'two': [2, 2]&#125;#The [] operator calls our __setitem__ and works as expected&gt;&gt;&gt; dd.update(three=3)&gt;&gt;&gt; dd&#123;'one': 1, 'two': [2, 2], 'three': 3&#125;# The update method from dict does not use our version of __setitem__ either This built-in behavior is a violation of a basic rule of object-oriented programming: the search for methods should always start from the class of the target instance (self), even when the call happens inside a method implemented in a superclass. If you subclass collections.UserDict instead of dict, the issues exposed are both fixed.123456789101112131415&gt;&gt;&gt; import collections&gt;&gt;&gt; class DoppleDict2(collections.UserDict):... def __setitem__(self,key,value):... super().__setitem__(key,[value]*2)... &gt;&gt;&gt; dd=DoppleDict2(one=1)&gt;&gt;&gt; dd&#123;'one': [1, 1]&#125;&gt;&gt;&gt; dd['two']=2&gt;&gt;&gt; dd&#123;'one': [1, 1], 'two': [2, 2]&#125;&gt;&gt;&gt; dd.update(three=3)&gt;&gt;&gt; dd&#123;'one': [1, 1], 'two': [2, 2], 'three': [3, 3]&#125; To summarize: the problem described in this section applies only to method delegation within the C language implementation of the built-in types, and only affects user defined classes derived directly from those types. If you subclass from a class coded in Python, such as UserDict or MutableMapping, you will not be troubled by this. Multiple Inheritance and Method Resolution Order 123456789101112131415161718192021222324class A: def ping(self): print('ping:',self)class B(A): def pong(self): print('pong:',self)class C(A): def pong(self): print('PONG:',self)class D(B,C): def ping(self): super().ping() print('post-ping:',self) def pingpong(self): self.ping() super().ping() self.pong() super().pong() C.pong(self) Two ways of invoking method pong on an instance of class D123456&gt;&gt;&gt; from diamond import *&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.pong() #pong: &lt;diamond.D object at 0x10066c278&gt;&gt;&gt;&gt; C.pong(d) #PONG: &lt;diamond.D object at 0x10066c278&gt; The ambiguity of a call like d.pong() is resolved because Python follows a specific order when traversing the inheritance graph. That order is called MRO: Method Resolution Order. Classes have an attribute called __mro__ holding a tuple of references to the superclasses in MRO order, from the current class all the way to the object class. For the D class, this is the __mro__. 12&gt;&gt;&gt; D.__mro__(&lt;class 'diamond.D'&gt;, &lt;class 'diamond.B'&gt;, &lt;class 'diamond.C'&gt;,&lt;class 'diamond.A'&gt;, &lt;class 'object'&gt;) The recommended way to delegate method calls to superclasses is the super() built-in function.It’s safest and more future-proof to use super(), especially when calling methods on a framework, or any class hierarchies you do not control. The five calls made by pingpong: 123456789&gt;&gt;&gt; from diamond import D&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.pingpong()ping: &lt;diamond.D object at 0x10bf235c0&gt; #post-ping: &lt;diamond.D object at 0x10bf235c0&gt;ping: &lt;diamond.D object at 0x10bf235c0&gt; #pong: &lt;diamond.D object at 0x10bf235c0&gt; #pong: &lt;diamond.D object at 0x10bf235c0&gt; #PONG: &lt;diamond.D object at 0x10bf235c0&gt; #]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C11_ABCs in the Standard Library]]></title>
    <url>%2F2018%2F05%2F25%2FC11-ABCs-in-the-Standard-Library%2F</url>
    <content type="text"><![CDATA[ABCs are available in the standard library. Most are defined in the collections.abc module, but there are others. You can find ABCs in the numbers and io packages ABCs in collections.abc Iterable, Container, and SizedEvery collection should either inherit from these ABCs or at least implement compatible protocols. Iterable supports iteration with __iter__, Container supports the in operator with __contains__, and Sized supports len() with __len__. Sequence, Mapping, and SetThese are the main immutable collection types, and each has a mutable subclass. A detailed diagram for MutableSequence is in Figure 11-2; for MutableMapping and MutableSet, there are diagrams in Chapter 3. MappingViewIn Python 3, the objects returned from the mapping methods .items(), .keys(), and .values() inherit from ItemsView, ValuesView, and ValuesView, respectively. The first two also inherit the rich interface of Set. Callable and HashableThese ABCs are not so closely related to collections, but collections.abc was the first package to define ABCs in the standard library, and these two were deemed important enough to be included. I’ve never seen subclasses of either Callable or Hashable. Their main use is to support the insinstance built-in as a safe way of determining whether an object is callable or hashable. IteratorNote that iterator subclasses Iterable. The Numbers Tower of ABCsThe numbers package defines the so-called “numerical tower” ,where Number is the topmost superclass, Complex is its immediate subclass, and so on, down to Integral: Number Complex Real Rational Integral So if you need to check for an integer, use isinstance(x, numbers.Integral) to accept int, bool (which subclasses int) or other integer types that may be provided by external libraries that register their types with the numbers ABCs. If, on the other hand, a value can be a floating-point type, you write isinstance(x, numbers.Real), and your code will happily take bool, int, float, fractions.Fraction, or any other noncomplex numerical type provided by an external library, such as NumPy. Defining and Using an ABCThe Tombola ABC has four methods. The two abstract methods are:.load(…): put items into the container..pick(): remove one item at random from the container, returning it. The concrete methods are:.loaded(): return True if there is at least one item in the container. .inspect(): return a sorted tuple built from the items currently in the container, without changing its contents (its internal ordering is not preserved). tombola.py: Tombola is an ABC with two abstract methods and two concrete methods 123456789101112131415161718192021222324252627282930import abcclass Tombola(abc.ABC): #To define an ABC, subclass abc.ABC @abc.abstractclassmethod def load(self, iterable): """Add items from an iterable""" @abc.abstractclassmethod def pick(self): ''' Remove item at random, returning it. This method should raise LookUpError when the instance is empty ''' def loaded(self): #An ABC may include concrete methods. ''' return True if there is at least 1 item, False otherwise ''' #Concrete methods in an ABC must rely only on the interface defined by the ABC return bool(self.inspect()) def inspect(self): '''Return a sorted tuple with the items currently inside''' items=[] while True: try: items.append(self.pick()) except LookupError: break self.load(items) return tuple(sorted(items)) ABC Syntax DetailsThe best way to declare an ABC is to subclass abc.ABC or any other ABC. However, the abc.ABC class is new in Python 3.4, so if you are using an earlier version of Python—and it does not make sense to subclass another existing ABC—then you must use the metaclass= keyword in the class statement, pointing to abc.ABCMeta (not abc.ABC). The metaclass= keyword argument was introduced in Python 3. In Python 2, you must use the __metaclass__ class attribute. Subclassing the Tombola ABCThis BingoCage implements the required abstract methods load and pick, inherits loaded from Tombola, overrides inspect, and adds __call__. bingo.py: BingoCage is a concrete subclass of Tombola 1234567891011121314151617181920212223242526272829303132333435363738394041424344import randomfrom tombola import Tombolaclass BingoCage(Tombola): def __init__(self): self._randomizer=random.SystemRandom() self._items=[] self.load(items) def load(self, items): self._items.extend(items) self._randomizer.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError(&apos;pick from empty BingoCage&apos;) def __call__(self): self.pick()from tombola import Tombolaclass BingoCage(Tombola): def __init__(self): self._randomizer=random.SystemRandom() self._items=[] self.load(items) def load(self, items): self._items.extend(items) self._randomizer.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError(&apos;pick from empty BingoCage&apos;) def __call__(self): self.pick() A Virtual Subclass of TombolaVirtual subclasses do not inherit from their registered ABCs, and are not checked for conformance to the ABC interface at any time, not even when they are instantiated. It’s up to the subclass to actually implement all the methods needed to avoid runtime errors. The register method is usually invoked as a plain function, but it can also be used as a decorator. We use the decorator syntax and implement TomboList, a virtual subclass of Tombola. tombolist.py: class TomboList is a virtual subclass of Tombola 12345678910111213141516171819202122from random import randrangefrom tombola import Tombola#Tombolist is registered as a virtual subclass of Tombola.@Tombola.registerclass TomboList(list): def pick(self): if self: position=randrange(len(self)) return self.pop(position) else: raise LookupError('pop from empty TomboList') #Tombolist.load is the same as list.extend. load=list.extend def loaded(self): return bool(self) def inspect(self): return tuple(sorted(self)) Note that because of the registration, the functions issubclass and isinstance act as if TomboList is a subclass of Tombola: 1234567&gt;&gt;&gt; from tombola import Tombola&gt;&gt;&gt; from tombolist import TomboList&gt;&gt;&gt; issubclass(TomboList, Tombola)True&gt;&gt;&gt; t = TomboList(range(100))&gt;&gt;&gt; isinstance(t, Tombola)True However, inheritance is guided by a special class attribute named __mro__—the Method Resolution Order. It basically lists the class and its superclasses in the order Python uses to search for methods. If you inspect the mro of TomboList, you’ll see that it lists only the “real” superclasses—list and object: 12&gt;&gt;&gt; TomboList.__mro__(&lt;class 'tombolist.TomboList'&gt;, &lt;class 'list'&gt;, &lt;class 'object'&gt;) Tombola is not in Tombolist.__mro__, so Tombolist does not inherit any methods from Tombola. How the Tombola Subclasses Were Testedtest the Tombola example: __subclasses__()Method that returns a list of the immediate subclasses of the class. The list does not include virtual subclasses. _abc_registryData attribute—available only in ABCs—that is bound to a WeakSet with weak references to registered virtual subclasses of the abstract class. Usage of register in PracticeEven if register can now be used as a decorator, it’s more widely deployed as a function to register classes defined elsewhere. For example, in the source code for the collections.abc module, the built-in types tuple, str, range, and memoryview are registered as virtual subclasses of Sequence like this: 1234Sequence.register(tuple)Sequence.register(str)Sequence.register(range)Sequence.register(memoryview)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C11_Interfaces: From Protocols to ABCs]]></title>
    <url>%2F2018%2F05%2F25%2FC11-Interfaces-From-Protocols-to-ABCs%2F</url>
    <content type="text"><![CDATA[_vector2d_v0.py: x and y are public data attributes_ 123456789class Vector2d: typecode='d' def __init__(self,x, y): self.x=float(x) self.y=float(y) def __iter__(self): return (i for i in (self.x, self.y)) _vector2d_v3.py: x and y reimplemented as properties_1234567891011121314151617class Vector2d: typecode='d' def __init__(self,x,y): self.__x=float(x) self.__y=float(y) @property def x(self): return self.__x @property def y(self): return self.__y def __iter__(self): return (i for i in (self.x,self.y)) Python digs SequencePartial sequence protocol implementation with \_getitem__: enough for item access, iteration, and the in operator_ 12345678910111213&gt;&gt;&gt; class Foo:... def __getitem__(self, pos):... return range(0,30,10)[pos]... &gt;&gt;&gt; f=Foo()&gt;&gt;&gt; for i in f: print(i)... 01020&gt;&gt;&gt; 20 in fTrue&gt;&gt;&gt; There is no method __iter__ yet Foo instances are iterable because—as a fallback—when Python sees a __getitem__ method, it tries to iterate over the object by calling that method with integer indexes starting with 0. Because Python is smart enough to iterate over Foo instances, it can also make the in operator work even if Foo has no __contains__ method: it does a full scan to check if an item is present. In summary, given the importance of the sequence protocol, in the absence __iter__ and __contains__ Python still manages to make iteration and the in operator work by invoking __getitem__. Monkey-Patching to Implement a Protocol at Runtime random.shuffle cannot handle FrenchDeck 123456789&gt;&gt;&gt; from random import shuffle&gt;&gt;&gt; from frenchdeck import FrenchDeck&gt;&gt;&gt; deck = FrenchDeck()&gt;&gt;&gt; shuffle(deck)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File ".../python3.3/random.py", line 265, in shuffle x[i], x[j] = x[j], x[i]TypeError: 'FrenchDeck' object does not support item assignment FrenchDeck object does not support item assignment.” The problem is that shuffle operates by swapping items inside the collection, and FrenchDeck only implements the immutable sequence protocol. Mutable sequences must also provide a __setitem__ method. Monkey patching FrenchDeck to make it mutable and compatible with random.shuffle 12345678&gt;&gt;&gt; def set_card(deck, position, card):... deck._cards[position] = card...&gt;&gt;&gt; FrenchDeck.__setitem__ = set_card&gt;&gt;&gt; shuffle(deck)&gt;&gt;&gt; deck[:5][Card(rank='3', suit='hearts'), Card(rank='4', suit='diamonds'), Card(rank='4',suit='clubs'), Card(rank='7', suit='hearts'), Card(rank='9', suit='spades')] The trick is that set_card knows that the deck object has an attribute named _cards, and _cards must be a mutable sequence. The set_card function is then attached to the FrenchDeck class as the __setitem__ special method. This is an example of monkey patching: changing a class or module at runtime, without touching the source code. ABCs are meant to encapsulate very general concepts, abstractions, introduced by a framework—thingslike “a sequence” and “an exact number.” [Readers] most likely don’t need to write any new ABCs, just use existing ones correctly, to get 99.9% of the benefits without serious risk of misdesign. Subclassing an ABCFrenchDeck2 is explicitly declared a subclass of collections.MutableSequence. frenchdeck2.py:12345678910111213141516171819202122232425262728293031import collectionsCard=collections.namedtuple('Card',['rank','suit'])class FrenchDeck2(collections.MutableSequence): ranks=[str(n) for n in range(2,11)+list('JQKA')] suits='spades diamonds clubs hearts'.split() def __init__(self): self._cards=[Card(rank, suit) for suit in self.suits for rank in self.ranks ] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] def __setitem__(self, position, value): self._cards[position]=value #subclassing MutableSequence forces us to implement __delitem__, an #abstract method of that ABC. def __delitem__(self,position): del self._cards[position] #required to implement insert, the third abstract method of #MutableSequence. def insert(self, position, value): self._cards.insert(position, value) Python does not check for the implementation of the abstract methods at import time, but only at runtime when we actually try to instantiate FrenchDeck2. Then, if we fail to implement any abstract method, we get a TypeError exception. From Sequence, FrenchDeck2 inherits the following ready-to-use concrete methods: __contains__, __iter__, __reversed__, index, and count. From MutableSequence, it gets append, reverse, extend, pop, remove, and __iadd__.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C10_Sequence Hacking, Hashing, and Slicing]]></title>
    <url>%2F2018%2F05%2F25%2FC10-Sequence-Hacking-Hashing-and-Slicing%2F</url>
    <content type="text"><![CDATA[A Slice-Aware __getitem___Part of vector_v2.py: __len__ and __getitem_\ methods added to Vector class from vector_v1.py_ 1234567891011def __len__(self): return len(self._components)def __getitem__(self, index): cls = type(self) if isinstance(index, slice): return cls(self._components[index]) elif isinstance(index, numbers.Integral): return self._components[index] else: msg = '&#123;cls.__name__&#125; indices must be integers' raise TypeError(msg.format(cls=cls)) Dynamic Attribute AccessThe __getattr__ method is invoked by the interpreter when attribute lookup fails. In simple terms,given the expression my_obj.x, Python checks if the my_obj instance has an attribute namedx; if not, the search goes to the class (my_obj.__class__), and then up the inheritancegraph. If the x attribute is not found, then the __getattr__ method defined in the class ofmy_obj is called with self and the name of the attribute as a string. _Part of vector_v3.py: __getattr__method added to Vector class from vector_v2.py_12345678910shortcut_names = 'xyzt'def __getattr__(self, name): cls = type(self)if len(name) == 1: pos = cls.shortcut_names.find(name)if 0 &lt;= pos &lt; len(self._components): return self._components[pos]msg = '&#123;.__name__!r&#125; object has no attribute &#123;!r&#125;'raise AttributeError(msg.format(cls, name)) _Part of vector_v3.py: __setattr__ method in Vector class_ 12345678910111213def __setattr__(self, name, value): cls = type(self) if len(name) == 1: if name in cls.shortcut_names: error = 'readonly attribute &#123;attr_name!r&#125;' elif name.islower(): error = "can't set attributes 'a' to 'z' in &#123;cls_name!r&#125;" else: error = '' if error: msg = error.format(cls_name=cls.__name__, attr_name=name) raise AttributeError(msg) super().__setattr__(name, value) Hashing and a Faster ==_Part of vector_v4.py: two imports and __hash__ method added to Vector class from vector_v3.py_ 12345678910111213141516171819202122from array import arrayimport reprlibimport mathimport functools #import operator class Vector: typecode = 'd' # many lines omitted in book listing... def __eq__(self, other): # if len(self) != len(other): # return False for a, b in zip(self, other): # if a != b: # return False return True # or you can just write as: return len(self) == len(other) and all(a == b for a, b in zip(self, other)) def __hash__(self): hashes = (hash(x) for x in self._components) # return functools.reduce(operator.xor, hashes, 0) # Formatting12345678910111213141516171819def angle(self, n): r = math.sqrt(sum(x * x for x in self[n:])) a = math.atan2(r, self[n-1]) if (n == len(self) - 1) and (self[-1] &lt; 0): return math.pi * 2 - a else: return a def angles(self): return (self.angle(n) for n in range(1, len(self))) def __format__(self, fmt_spec=''): if fmt_spec.endswith('h'): # hyperspherical coordinates fmt_spec = fmt_spec[:-1] coords = itertools.chain([abs(self)], self.angles()) outer_fmt = '&lt;&#123;&#125;&gt;' else: coords = self outer_fmt = '(&#123;&#125;)' components = (format(c, fmt_spec) for c in coords) return outer_fmt.format(', '.join(components))]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C09_A Pythonic Object]]></title>
    <url>%2F2018%2F05%2F25%2FC09-A-Pythonic-Object%2F</url>
    <content type="text"><![CDATA[Object RepresentationsEvery object-oriented language has at least one standard way of getting a string representation fromany object. Python has two:repr() Return a string representing the object as the developer wants to see it. str() Return a string representing the object as the user wants to see it. There are two additional special methods to support alternative representations of objects:__bytes__ and __format__. The __bytes__ method is analogous to __str_\: it’scalled by bytes() to get the object represented as a byte sequence. Regarding **\_format__,both the built-in function format() and the str.format()** method call it to get string displays ofobjects using special formatting codes. _vector2d_v1.py_ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from array import arrayimport mathclass Vector2d: typecode='d' def __init__(self, x, y): self.x = float(x) self.y = float(y) #__iter__ makes a Vector2d iterable; this is what makes #unpacking work (e.g,x, y = my_vector). def __iter__(self): return (i for i in (self.x, self.y)) #because Vector2d is iterable, *self feeds the x and y #components to format. def __repr__(self): class_name= type(self).__name__ return '&#123;&#125;(&#123;!r&#125;, &#123;!r&#125;)'.format(class_name, *self) #From an iterable Vector2d, it’s easy to build a tuple #for display as an ordered pair. def __str__(self): return str(tuple(self)) def __bytes__(self): return (bytes([ord(self.typecode)])+ bytes(array(self.typecode, self)) ) def __eq__(self, other): return tuple(self) == tuple(other) def __abs__(self): return math.hypot(self.x+ self.y) def __bool__(self): return bool(abs(self)) #No self argument; instead, the class itself is passed as #cls. @classmethod def frombytes(cls, octets): typecode=chr(octets[0]) #Read the typecode from the first byte. #Create a memoryview from the octets binary sequence and use the #typecode to cast it memv=memoryview(octets[1:].cast(typecode)) #Unpack the memoryview resulting from the cast into the pair of #arguments needed for the constructor. return cls(*memv) classmethod Versus staticmethodclassmethod changes the way the method is called, so it receives the class itself as the firstargument, instead of an instance. Its most common use is for alternative constructors, like frombytes in_vector2d_v1.py_. staticmethod decorator changes a method so that it receives no special first argument. In essence, astatic method is just like a plain function that happens to live in a class body, instead of beingdefined at the module level. Formatted DisplaysThe format() built-in function and the str.format() method delegate the actual formatting to eachtype by calling their .__format__(format_spec) method. The format_spec is a formattingspecifier, which is either the second argument in format(my_obj, format_spec), or Whatever appearsafter the colon in a replacement field delimited with {} inside a format string used with str.format(). A few built-in types have their own presentation codes in the Format Specification MiniLanguage. Forexample—among several other codes—the int type supports b and x for base 2 and base 16 output,respectively, while float implements f for a fixed-point display and % for a percentage display: 12345&gt;&gt;&gt; format(42, 'b')'101010'&gt;&gt;&gt; format(2/3, '.1%')'66.7%' The classes in the datetime module use the same format codes in the strftime() functions and in their__format__ methods. Here are a couple examples using the format() built-in and thestr.format() method: 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; format(now, '%H:%M:%S')'18:49:05'&gt;&gt;&gt; "It's now &#123;:%I:%M %p&#125;".format(now)"It's now 06:49 PM" If a class has no __format__, the method inherited from object returns str(my_object). 123456789&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; format(v1)&apos;(3.0, 4.0)&apos;# if you pass a format specifier, object.__format__ raises TypeError:&gt;&gt;&gt; format(v1, &apos;.3f&apos;)Traceback (most recent call last): ...TypeError: non-empty format string passed to object.__format Vector2d.format method, now with polar coordinates1234567891011121314def angle(self): return math.atan2(self.y, self.x)def __format__(self, fmt_spec=''): if fmt_spec.endswith('p'): fmt_spec=fmt_spec[:-1] coords=(abs(self), self.angle()) outer_fmt='&lt;&#123;&#125; &#123;&#125;&gt;' else: coords=self outer_fmt='(&#123;&#125;, &#123;&#125;)' components=(format(c, fmt_spec) for c in self) return outer_fmt.format(*components) A Hashable Vector2dVector2d instances are unhashable, so we can’t put them in a set 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; hash(v1)Traceback (most recent call last): ...TypeError: unhashable type: 'Vector2d'&gt;&gt;&gt; set([v1])Traceback (most recent call last): ...TypeError: unhashable type: 'Vector2d'class Vector2d: typecode='d' #Use exactly two leading underscores to make an attribute #private def __init__(self,x,y): self.__x=float(x) self.__y=float(y) #The @property decorator marks the getter method of a #property. @property def x(self): return self.__x @property def y(self): return self.__y def __iter__(self): return (i for i in (self.x, self.y)) def __hash__(self): return hash(self.x) ^ hash(self.y)&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; v2 = Vector2d(3.1, 4.2)&gt;&gt;&gt; hash(v1), hash(v2)(7, 384307168202284039)&gt;&gt;&gt; set([v1, v2])&#123;Vector2d(3.1, 4.2), Vector2d(3.0, 4.0)&#125; Private and “Protected” Attributes in PythonConsider this scenario: someone wrote a class named Dog that uses a mood instance attribute internally, without exposing it. You need to subclass Dog as Beagle. If you create your own mood instance attribute without being aware of the name clash, you will clobber the mood attribute used by the methods inherited from Dog. This would be a pain to debug. To prevent this, if you name an instance attribute in the form __mood (two leading underscores andzero or at most one trailing underscore), Python stores the name in the instance __dict__ prefixedwith a leading underscore and the class name, so in the Dog class,__mood becomes_Dog__mood, and in Beagle it’s _Beagle__mood. This language feature goes by thelovely name of name mangling. The single underscore prefix has no special meaning to the Python interpreter when used in attributenames, but it’s a very strong convention among Python programmers that you should not access suchattributes from outside the class. Saving Space with the slots Class AttributeBy default, Python stores instance attributes in a per-instance dict named __dict__.If youare dealing with millions of instances with few attributes, the __slots__ class attribute can save alot of memory, by letting the interpreter store the instance attributes in a tuple instead of a dict. A __slots__ attribute inherited from a superclass has no effect. Python only takes into account__slots__ attributes defined in each class individually. By defining __slots__ in the class, you are telling the interpreter: “These are all the instanceattributes in this class.” Python then stores them in a tuple-like structure in each instance, avoiding thememory overhead of the per-instance_\dict__. This can make a huge difference in memory usage ifyour have millions of instances active at the same time. The __weakref__ attribute is necessary for an object to support weak references. That attribute ispresent by default in instances of user-defined classes. However, if the class defines __slots__, andyou need the instances to be targets of weak references, then you need to include __weakref__among the attributes named in __slots__. To summarize, __slots__ may provide significant memory savings if properly used, but there are afew caveats: You must remember to redeclare __slots__ in each subclass, because the inherited attribute is ignored by the interpreter. Instances will only be able to have the attributes listed in __slots__, unless you include ‘_\dict__‘ in __slots__ (but doing so may negate the memory savings). Instances cannot be targets of weak references unless you remember to include ‘__weakref__‘ in __slots__. Overriding Class AttributesA distinctive feature of Python is how class attributes can be used as default values for instanceattributes. 1234567891011121314151617&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; v1 = Vector2d(1.1, 2.2) #Set typecode to 'f' in the v1 instance.&gt;&gt;&gt; dumpd = bytes(v1)&gt;&gt;&gt; dumpdb'd\x9a\x99\x99\x99\x99\x99\xf1?\x9a\x99\x99\x99\x99\x99\x01@'&gt;&gt;&gt; len(dumpd) #17&gt;&gt;&gt; v1.typecode = 'f' #&gt;&gt;&gt; dumpf = bytes(v1)&gt;&gt;&gt; dumpfb'f\xcd\xcc\x8c?\xcd\xcc\x0c@'&gt;&gt;&gt; len(dumpf) #9# Vector2d.typecode is unchanged; only the v1 instance uses typecode 'f'.&gt;&gt;&gt; Vector2d.typecode #'d' If you want to change a class attribute you must set it on the class directly, not through aninstance. You could change the default typecode for all instances (that don’t have their owntypecode) by doing this: Vector2d.typecode = ‘f’ The ShortVector2d is a subclass of Vector2d, which only overwrites the default typecode123456789&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; class ShortVector2d(Vector2d): #... typecode = 'f'...&gt;&gt;&gt; sv = ShortVector2d(1/11, 1/27) #&gt;&gt;&gt; svShortVector2d(0.09090909090909091, 0.037037037037037035) #&gt;&gt;&gt; len(bytes(sv)) #9]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C08_Object References, Mutability, and Recycling_02]]></title>
    <url>%2F2018%2F05%2F25%2FC08-Object-References-Mutability-and-Recycling-02%2F</url>
    <content type="text"><![CDATA[Function Parameters as ReferencesA function may change any mutable object it receives1234567891011121314151617181920212223&gt;&gt;&gt; def f(a, b):... a += b... return a...&gt;&gt;&gt; x = 1&gt;&gt;&gt; y = 2&gt;&gt;&gt; f(x, y)3&gt;&gt;&gt; x, y(1, 2)&gt;&gt;&gt; a = [1, 2]&gt;&gt;&gt; b = [3, 4]&gt;&gt;&gt; f(a, b)[1, 2, 3, 4]&gt;&gt;&gt; a, b([1, 2, 3, 4], [3, 4])&gt;&gt;&gt; t = (10, 20)&gt;&gt;&gt; u = (30, 40)&gt;&gt;&gt; f(t, u)(10, 20, 30, 40)&gt;&gt;&gt; t, u((10, 20), (30, 40)) Mutable Types as Parameter Defaults: Bad IdeaIf a default value is a mutable object, and you change it, the change will affect every future call of thefunction.12345678910111213141516171819class HauntedBus: def __init__(self, passengers=[]):#This assignment makes self.passengers an aliasfor passengers, which isitself an alias for the default list, #when no passengers argument is given. if passengers is None: self.passengers=passengers else: self.passengers = list(passengers)# When the methods .remove() and .append() are used with self.passengers we are actually mutating the #default list, which is an attribute of the function object def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) eerie behavior of the HauntedBus12345678910111213141516171819202122232425262728&gt;&gt;&gt; bus1 = HauntedBus(['Alice', 'Bill'])&gt;&gt;&gt; bus1.passengers['Alice', 'Bill']&gt;&gt;&gt; bus1.pick('Charlie')&gt;&gt;&gt; bus1.drop('Alice')&gt;&gt;&gt; bus1.passengers['Bill', 'Charlie']&gt;&gt;&gt; bus2 = HauntedBus()&gt;&gt;&gt; bus2.pick('Carrie')&gt;&gt;&gt; bus2.passengers['Carrie']&gt;&gt;&gt; bus3 = HauntedBus()&gt;&gt;&gt; bus3.passengers['Carrie']&gt;&gt;&gt; bus3.pick('Dave')&gt;&gt;&gt; bus2.passengers['Carrie', 'Dave']&gt;&gt;&gt; bus2.passengers is bus3.passengers # bus2.passengers and bus3.passengers refer to the same listTrue&gt;&gt;&gt; bus1.passengers # bus1.passengers is a distinct list['Bill', 'Charlie']&gt;&gt;&gt; dir(HauntedBus.__init__) # doctest: +ELLIPSIS['__annotations__', '__call__', ..., '__defaults__', ...]&gt;&gt;&gt; HauntedBus.__init__.__defaults__(['Carrie', 'Dave'],)&gt;&gt;&gt; HauntedBus.__init__.__defaults__[0] is bus2.passengersTrue The issue with mutable defaults explains why None is often used as the default value for parameters that may receive mutable values. Defensive Programming with Mutable ParametersIf your function receives a dict and needs to modify it while processing it, should this side effect bevisible outside of the function or not? Actually it depends on the context. It’s really a matter of aligningthe expectation of the coder of the function and that of the caller. A simple class to show the perils of mutating received arguments 123456789101112131415161718192021222324252627282930313233343536class TwilightBus: def __init__(self, passengers=None): if passengers is None: self.passengers=[] else: #this assignment makes self.passengers an alias for # passengers, which is itself an alias for the # actual argument passed to __init__ self.passengers=passengers #When the methods .remove() and .append() are used with #self.passengers, we are actually mutating the original #list received as argument to the constructor. def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name)# Passengers disappear when dropped by a TwilightBus&gt;&gt;&gt; basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']&gt;&gt;&gt; bus = TwilightBus(basketball_team)&gt;&gt;&gt; bus.drop('Tina')&gt;&gt;&gt; bus.drop('Pat')&gt;&gt;&gt; basketball_team #The dropped passengers vanished from the basketball team!['Sue', 'Maya', 'Diana']# The fix is simple: in __init__, when the passengers parameter is provided, self.passengers should be # initialized with a copy of itdef __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) del and Garbage CollectionThe del statement deletes names, not objects.An object may be garbage collected as result of adel command, but only if the variable deleted holds the last reference to the object, or if the objectbecomes unreachable. Rebinding a variable may also cause the number of references to an object toreach zero, causing its destruction. There is a __del__ special method, but it does not cause the disposal of the instance, and shouldnot be called by your code. __del__ invoked by the Python interpreter when the instance is aboutto be destroyed to give it a chance to release external resources. In CPython, the primary algorithm for garbage collection is reference counting. Essentially, each objectkeeps count of how many references point to it. As soon as that refcount reaches zero, the object isimmediately destroyed: CPython calls the __del__ method on the object (if defined) and thenfrees the memory allocated to the object. In CPython 2.0, a generational garbage collection algorithmwas added to detect groups of objects involved in reference cycles—which may be unreachable evenwith outstanding references to them, when all the mutual references are contained within the group.Other implementations of Python have more sophisticated garbage collectors that do not rely onreference counting, which means the __del__ method may not be called immediately when thereare no more references to the object. Using weakref.finalize to register a callback function to be called when an object is destroyed Weak ReferencesWeak references to an object do not increase its reference count. The object that is the target of areference is called the referent. Therefore, we say that a weak reference does not prevent thereferent from being garbage collected. A weak reference is a callable that returns the referenced object or None if the referent is no more 1234567891011121314151617181920212223242526&gt;&gt;&gt; import weakref&gt;&gt;&gt; a_set = &#123;0, 1&#125;&gt;&gt;&gt; wref = weakref.ref(a_set)&gt;&gt;&gt; wref&lt;weakref at 0x100637598; to 'set' at 0x100636748&gt;#Invoking wref() returns the referenced object, &#123;0, 1&#125;. Because this is a console session, the result &#123;0, 1&#125; is # bound to the _ variable.&gt;&gt;&gt; wref()&#123;0, 1&#125;# a_set no longer refers to the &#123;0, 1&#125; set, so its reference count is decreased. But the _ variable still refers # to it.&gt;&gt;&gt; a_set = &#123;2, 3, 4&#125;&gt;&gt;&gt; wref()&#123;0, 1&#125;# When this expression is evaluated, &#123;0, 1&#125; lives, therefore wref() is not None. But _ is then bound to the # resulting value, False. Now there are no more strong references to &#123;0, 1&#125;.&gt;&gt;&gt; wref() is NoneFalse# Because the &#123;0, 1&#125; object is now gone, this last call to wref() returns None.&gt;&gt;&gt; wref() is NoneTrue The WeakValueDictionary SkitThe class WeakValueDictionary implements a mutable mapping where the values are weak referencesto objects. When a referred object is garbage collected elsewhere in the program, the corresponding keyis automatically removed from WeakValueDictionary. This is commonly used for caching. 123456789101112131415161718192021222324class Cheese: def __init__(self, kind): self.kind = kind def __repr__(self): return 'Cheese(%r)' % self.kind&gt;&gt;&gt; import weakref&gt;&gt;&gt; stock = weakref.WeakValueDictionary()&gt;&gt;&gt; catalog = [Cheese('Red Leicester'), Cheese('Tilsit'),... Cheese('Brie'), Cheese('Parmesan')]...# The stock mapsthe name of the cheese to aweak reference to the cheese instance in the catalog.&gt;&gt;&gt; for cheese in catalog: ... stock[cheese.kind] = cheese...&gt;&gt;&gt; sorted(stock.keys())['Brie', 'Parmesan', 'Red Leicester', 'Tilsit']&gt;&gt;&gt; del catalog&gt;&gt;&gt; sorted(stock.keys())['Parmesan']&gt;&gt;&gt; del cheese&gt;&gt;&gt; sorted(stock.keys())[]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[春天的一些照片]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%98%A5%E5%A4%A9%E7%9A%84%E4%B8%80%E4%BA%9B%E7%85%A7%E7%89%87%2F</url>
    <content type="text"><![CDATA[照片非出自本人，双休日这么好的日子，宅着，看看nba, 看看番。 2018年3月，同事去某个植物园，可能是上海植物园，陶堰情操，拍拍花草，感觉比以前拍的好看点。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>photo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态博客太好用！！！]]></title>
    <url>%2F2018%2F04%2F22%2F%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E5%A4%AA%E5%A5%BD%E7%94%A8%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
    <content type="text"><![CDATA[用hexo搭的这个博客， 太好搭了，就用了node.js和git, 不需要vps, 省钱。之前flask搭的那个博客，要先看完书，然后找了一个别人写好的框架，一步一步网上谷歌，废了九牛二虎之力才弄好， 弄完后眼睛那个酸。这个flask博客还是蛮好看的，羡慕别人怎么就可以写的出这么好的框架。 hexo博客优点：样式简洁美观上传文章超级方便 hexo n, hexo g, hexo s, hexo d 不过发现图片总是404错误，于是在vps上搭了个图床，第一次发现宝塔这个工具，真是强大方便。。。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>吐槽</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C08_Object References, Mutability, and Recycling_01]]></title>
    <url>%2F2018%2F04%2F22%2FC08-Object-References-Mutability-and-Recycling-01%2F</url>
    <content type="text"><![CDATA[The distinction between objects and their names. A name is not the object; a name is a separate thing. Variables Are Not BoxesIt’s better to think of Python variables as labels attached to objects Variables a and b hold references to the same list, not copies of the list12345&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; b[1, 2, 3, 4] With reference variables, it makes much more sense to say that the variable is assigned to an object,and not the other way around. After all, the object is created before the assignment. The righthand side of an assignment happens first(Variables are assigned to objects only after the objects are created) 1234567891011121314&gt;&gt;&gt; class Gizmo:... def __init__(self):... print('Gizmo id: %d' % id(self))...&gt;&gt;&gt; x = Gizmo()Gizmo id: 4301489152# a second Gizmo was actually instantiated before the multiplication was attempted.&gt;&gt;&gt; y = Gizmo() * 10 Gizmo id: 4301489432Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int' Identity, Equality, and Aliasescharles and lewis refer to the same object123456789&gt;&gt;&gt; charles = &#123;'name': 'Charles L. Dodgson', 'born': 1832&#125;&gt;&gt;&gt; lewis = charles # lewis is an alias for charles.&gt;&gt;&gt; lewis is charlesTrue&gt;&gt;&gt; id(charles), id(lewis) (4300473992, 4300473992)&gt;&gt;&gt; lewis['balance'] = 950&gt;&gt;&gt; charles&#123;'name': 'Charles L. Dodgson', 'balance': 950, 'born': 1832&#125; alex and charles compare equal, but alex is not charles #alex refers to an object that is a replica of the object assigned to charles.12345&gt;&gt;&gt; alex = &#123;'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950&#125;&gt;&gt;&gt; alex == charlesTrue&gt;&gt;&gt; alex is not charlesTrue In The Python Language Reference, “3.1. Objects, values and types” states: Every object has an identity, a type and a value. An object’s identity never changes onceit has been created; you may think of it as the object’s address in memory. The is operatorcompares the identity of two objects; the id() function returns an integer representingits identity. In CPython, id() returns the memory address of the object, but it may be something else in anotherPython interpreter. The key point is that the ID is guaranteed to be a unique numeric label, and it willnever change during the life of the object. Choosing Between == and isThe == operator compares the values of objects (the data they hold), while is compares theiridentities. checking whether a variable is bound to Nonex is None The is operator is faster than ==, because it cannot be overloaded, so Python does not have tofind and invoke special methods to evaluate it, and computing is as simple as comparing two integer IDs.In contrast, a == b is syntactic sugar for a.__eq__(b). The __eq__ method inherited fromobject compares object IDs, so it produces the same result as is. The Relative Immutability of TuplesTuples, like most Python collections—lists, dicts, sets, etc.—hold references toobjects.If the referenced items are mutable, they may change even if the tuple itself does not. Inother words, the immutability of tuples really refers to the physical contents of the tuple datastructure (i.e., the references it holds), and does not extend to the referenced objects.1234567891011121314&gt;&gt;&gt; t1 = (1, 2, [30, 40])&gt;&gt;&gt; t2 = (1, 2, [30, 40])&gt;&gt;&gt; t1 == t2True&gt;&gt;&gt; id(t1[-1])4302515784&gt;&gt;&gt; t1[-1].append(99)&gt;&gt;&gt; t1(1, 2, [30, 40, 99])&gt;&gt;&gt; id(t1[-1]) #The identity of t1[-1] has not changed, only its value.4302515784&gt;&gt;&gt; t1 == t2False Copies Are Shallow by Default12345678&gt;&gt;&gt; l1 = [3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 = list(l1) #list(l1) creates a copy of l1.&gt;&gt;&gt; l2[3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 == l1True&gt;&gt;&gt; l2 is l1 #The copies are equal. But refer to two different objectsFalse For lists and other mutable sequences, the shortcut l2 = l1[:] also makes a copy. Using the constructor or [:] produces a shallow copy (i.e., the outermost container is duplicated,but the copy is filled with references to the same items held by the original container). This savesmemory and causes no problems if all the items are immutable. But if there are mutable items, this maylead to unpleasant surprises. Making a shallow copy of a list containing another list 123456789101112131415161718192021l1 = [3, [66, 55, 44], (7, 8, 9)]l2 = list(l1) #l1.append(100) #l1[1].remove(55) #print('l1:', l1)print('l2:', l2)l2[1] += [33, 22] #l2[2] += (10, 11) #print('l1:', l1)print('l2:', l2)#ouput:l1: [3, [66, 44], (7, 8, 9), 100]l2: [3, [66, 44], (7, 8, 9)]#For a mutable object like the list referred by l2[1], the operator += changes the list in place. This change #is visible at l1[1], which is an alias for l2[1].l1: [3, [66, 44, 33, 22], (7, 8, 9), 100]# += on a tuple creates a new tuple and rebinds the variable l2[2] here. This is the same as doing l2[2] = # l2[2] + (10, 11). Now the tuples in the last position of l1 and l2 are no longer the same object. l2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)] Deep and Shallow Copies of Arbitrary ObjectsThe copy module provides the deepcopy and copy functions that return deep and shallowcopies of arbitrary objects.1234567891011121314class Bus: def __init__(self, passengers=None): if passengers is None: self.passengers=[] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) Effects of using copy versus deepcopy1234567891011121314151617&gt;&gt;&gt; import copy&gt;&gt;&gt; bus1 = Bus(['Alice', 'Bill', 'Claire', 'David'])&gt;&gt;&gt; bus2 = copy.copy(bus1)&gt;&gt;&gt; bus3 = copy.deepcopy(bus1)#Using copy and deepcopy, we create three distinct Bus instances.&gt;&gt;&gt; id(bus1), id(bus2), id(bus3)(4301498296, 4301499416, 4301499752)&gt;&gt;&gt; bus1.drop('Bill')&gt;&gt;&gt; bus2.passengers['Alice', 'Claire', 'David']&gt;&gt;&gt; id(bus1.passengers), id(bus2.passengers), id(bus3.passengers)(4302658568, 4302658568, 4302657800)&gt;&gt;&gt; bus3.passengers['Alice', 'Bill', 'Claire', 'David'] Cyclic references: b refers to a, and then is appended to a; deepcopy still manages to copy a 123456789&gt;&gt;&gt; a=[10,20]&gt;&gt;&gt; b=[a,30]&gt;&gt;&gt; a.append(b)&gt;&gt;&gt; a[10, 20, [[...], 30]]&gt;&gt;&gt; from copy import deepcopy&gt;&gt;&gt; c=deepcopy(a)&gt;&gt;&gt; c[10, 20, [[...], 30]]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Stacked Decorators_Parameterized Decorators]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Stacked-Decorators-Parameterized-Decorators%2F</url>
    <content type="text"><![CDATA[Stacked DecoratorsWhen two decorators @d1 and @d2 are applied to a function f in that order, the result isthe same as f = d1(d2(f)). Parameterized DecoratorsWhen parsing a decorator in source code, Python takes the decorated function and passes it as the firstargument to the decorator function. So how do you make a decorator accept other arguments? Theanswer is: make a decorator factory that takes those arguments and returns a decorator, which is thenapplied to the function to be decorated._registration_param.py_12345678910111213141516171819202122registry=set()def register(active=True): def decorate(func): print('running register (active=%s) -&gt; decorate(%s)')%(active, func) if active: registry.add(func) else: registry.discard(func) return func return decorate@register(active=False)def f1(): print('running f1()')@register(active=True)def f2(): print('running f2()')def f3(): print('running f3()') _Using the registration_param module_123456789101112131415&gt;&gt;&gt; from registration_param import *running register(active=False)-&gt;decorate(&lt;function f1 at 0x10073c1e0&gt;)running register(active=True)-&gt;decorate(&lt;function f2 at 0x10073c268&gt;)&gt;&gt;&gt; registry #&#123;&lt;function f2 at 0x10073c268&gt;&#125;&gt;&gt;&gt; register()(f3) #running register(active=True)-&gt;decorate(&lt;function f3 at 0x10073c158&gt;)&lt;function f3 at 0x10073c158&gt;&gt;&gt;&gt; registry #&#123;&lt;function f3 at 0x10073c158&gt;, &lt;function f2 at 0x10073c268&gt;&#125;&gt;&gt;&gt; register(active=False)(f2) #running register(active=False)-&gt;decorate(&lt;function f2 at 0x10073c268&gt;)&lt;function f2 at 0x10073c268&gt;&gt;&gt;&gt; registry #&#123;&lt;function f3 at 0x10073c158&gt;&#125; _clockdeco_param.py_12345678910111213141516171819202122232425262728293031323334import timeDEFAULT_FMT='[&#123;elasped: 0.8f&#125;s &#123;name&#125;(&#123;args&#125;) -&gt; &#123;result&#125;]'def clock(fmt=DEFAULT_FMT): def decorate(func): def clocked(*_args): t0=time.time() _result=func(*_args) elapsed=time.time()- t0 name=func.__name__ #_args holds the actual arguments of clocked args=', '.join(repr(arg) for arg in _args) result=repr(_result) #Using **locals() here allows any local variable of #clocked to be referenced in the fmt. print(fmt.format(**locals())) return _result return clocked return decorateif __name__=='__main__': @clock() def snooze(seconds): time.sleep(seconds) for i in range(3): snooze(.123)# output:$ python3 clockdeco_param.py[0.12412500s] snooze(0.123) -&gt; None[0.12411904s] snooze(0.123) -&gt; None[0.12410498s] snooze(0.123) -&gt; None _clockdeco_param_demo1.py_ 12345678910111213import timefrom clockdeco_param import clock@clock('&#123;name&#125;: &#123;elapsed&#125;s')def snooze(seconds): time.sleep(seconds)for i in range(3): snooze(.123)# output:$ python3 clockdeco_param_demo1.pysnooze: 0.12414693832397461ssnooze: 0.1241159439086914ssnooze: 0.12412118911743164s _clockdeco_param_demo2.py_12345678910111213import timefrom clockdeco_param import clock@clock('&#123;name&#125;(&#123;args&#125;) dt=&#123;elapsed:0.3f&#125;s')def snooze(seconds): time.sleep(seconds)for i in range(3): snooze(.123)# output:$ python3 clockdeco_param_demo2.pysnooze(0.123) dt=0.124ssnooze(0.123) dt=0.124ssnooze(0.123) dt=0.124s]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Implementing a Simple Decorator]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Implementing-a-Simple-Decorator%2F</url>
    <content type="text"><![CDATA[clockdeco.py: 12345678910import timedef clock(func): def clocked(*args): t0=time.perf_counter() result=func(*args) elapsed=time.perf_counter()- t0 name=func.__name__ arg_str=', '.join(repr(arg) for arg in args) print('[%0.8fs] %s(%s) -&gt; %r'%(elapsed, name, arg_str, result)) return clocked _clockdeco_demo.py:_ 123456789101112131415161718192021import timefrom clockdeco import clock@clockdef snooze(seconds): time.sleep(seconds)@clockdef factorial(n): return 1 if n ==1 else n * factorial(n-1)if __name__ == '__main__': print('*'*40, 'Calling snooze(.123)') snooze(.123) print('*'*40, 'Calling factorial(6)') print('6!= ', factorial(6))&gt;&gt;&gt; import clockdeco_demo&gt;&gt;&gt; clockdeco_demo.factorial.__name__'clocked' So factorial now actually holds a reference to the clocked function. The typical behavior of a decorator: it replaces the decorated function with a new function that accepts the same arguments and (usually) returns whatever the decorated function was supposed to return, while also doing some extra processing. functools.wrapsThe clock decorator has a few shortcomings: it does not support keyword arguments, and it masksthe __name__ and __doc__ of the decorated function. clockdeco2.py1234567891011121314151617181920import timeimport functoolsdef clock(func): @functools.wraps(func) def clocked(*args, **kwargs): t0=time.time() result=func(*args, **kwargs) elapsed=time.time() - t0 name=func.__name__ arg_lst=[] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs=['%s = %s' %(k,w) for k,w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str=', '.join(arg_lst) print('[%0.8fs] %s(%s) -&gt; %r'%(elapsed, name, arg_str, result)) return result return clocked functools.lru_cacheA very practical decorator is functools.lru_cache. It implements memoization: an optimizationtechnique that works by saving the results of previous invocations of an expensive function, avoidingrepeat computations on previously used arguments. The letters LRU stand for Least Recently Used,meaning that the growth of the cache is limited by discarding the entries that have not been read for awhile. _fibo_demo.py_1234567891011121314151617181920212223242526272829303132333435from clockdeco import clock@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n-2) + fibonacci(n-1)if __name__=='__main__': print(fibonacci(6))# $ python3 fibo_demo.py# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00007892s] fibonacci(2) -&gt; 1# [0.00000095s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00003815s] fibonacci(2) -&gt; 1# [0.00007391s] fibonacci(3) -&gt; 2# [0.00018883s] fibonacci(4) -&gt; 3# [0.00000000s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000119s] fibonacci(1) -&gt; 1# [0.00004911s] fibonacci(2) -&gt; 1# [0.00009704s] fibonacci(3) -&gt; 2# [0.00000000s] fibonacci(0) -&gt; 0# [0.00000000s] fibonacci(1) -&gt; 1# [0.00002694s] fibonacci(2) -&gt; 1# [0.00000095s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00005102s] fibonacci(2) -&gt; 1# [0.00008917s] fibonacci(3) -&gt; 2# [0.00015593s] fibonacci(4) -&gt; 3# [0.00029993s] fibonacci(5) -&gt; 5# [0.00052810s] fibonacci(6) -&gt; 8# 8 _fibo_demo_lru.py_12345678910111213141516import functools from clockdeco import clock@functools.lru_cache()@clockdef fibnacci(n): if n &lt; 2: return n return fibnacci(n-2)+ fibnacci(n-1)$ python3 fibo_demo_lru.py# [0.00000119s] fibonacci(0) -&gt; 0# [0.00000119s] fibonacci(1) -&gt; 1# [0.00010800s] fibonacci(2) -&gt; 1# [0.00000787s] fibonacci(3) -&gt; 2# [0.00016093s] fibonacci(4) -&gt; 3# [0.00001216s] fibonacci(5) -&gt; 5# [0.00025296s] fibonacci(6) -&gt; 8 lru_cache can be tuned by passing two optional arguments. Its full signature is: functools.lru_cache(maxsize=128, typed=False) The maxsize argument determines how many call results are stored. After the cache is full, older results are discarded to make room. For optimal performance, maxsize should be a power of 2. The typed argument, if set to True, stores results of different argument types separately, i.e., distinguishing between float and integer arguments that are normally considered equal, like 1 and 1.0. By the way, because lru_cache uses a dict to store the results, and the keys are made from the positional and keyword arguments used in the calls, all the arguments taken by the decorated function must be hashable. functools.singledispatchIf you decorate a plain function with @singledispatch, it becomes a generic function: a group offunctions to perform the same operation in different ways, depending on the type of the first argument. 123456789101112131415161718192021222324252627from functools import singledispatchfrom collections import abcimport numbersimport html@singledispatchdef htmlize(obj): content=html.escape(repr(obj)) return '&lt;pre&gt;&#123;&#125;&lt;/pre&gt;'.format(content)@htmlize.register(str)def _(text): content=html.escape(text).replace('\n','&lt;br&gt;\n') return '&lt;p&gt;&#123;0&#125;&lt;/p&gt;'.format(content)#numbers.Integral is a virtual superclass of int.@htmlize.register(numbers.Integral)def _(n): return '&lt;pre&gt;&#123;0&#125; (0x&#123;0:x&#125;)&lt;/pre&gt;'.format(n) #stack several register decorators to support different types #with the same function@htmlize.register(tuple)@htmlize.register(abc.MutableSequence)def _(seq): inner='&lt;/li&gt;\n&lt;li&gt;'.join(htmlize(item) for item in seq) return '&lt;ul&gt;\n&lt;li&gt;'+inner+'&lt;/li&gt;\n&lt;/ul&gt;']]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Function Decorators and Closures_02]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Function-Decorators-and-Closures-02%2F</url>
    <content type="text"><![CDATA[ClosuresA closure is a function with an extended scope that encompasses nonglobal variables referencedin the body of the function but not defined there. It does not matter whether the function is anonymousor not; what matters is that it can access nonglobal variables that are defined outside of its body.A class to calculate a running average12345678910class Averager(): def __init__(self): self.series=[] def __call__(self, new_value): self.series.append(new_value) total=sum(self.series) return total / len(self.series) A higher-order function to calculate a running average12345678910def make_averager(): series=[] def averager(new_value): series.append(new_value) total=sum(series) return total / len(series) return averager When invoked, make_averager returns an averager function object. Each time anaverager is called, it appends the passed argument to the series, and computes the currentaverage, as shown: 1234567&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 _inspecting the function created by make_average_ 1234&gt;&gt;&gt; avg.__code__.co_varnames('new_value', 'total')&gt;&gt;&gt; avg.__code__.co_freevars('series',) The binding for series is kept in the __closure__ attribute of the returned function avg. Each item inavg.__closure__corresponds to a name in avg.__closure__**.co_free vars. These items are cells, and they have an attribute called cell_contents** where the actual value can be found. A closure is a function that retains the bindings of the free variables that exist when the function isdefined, so that they can be used later when the function is invoked and the defining scope is no longeravailable. nonlocal Declaration123456789101112131415161718def make_averager(): count=0 total=0 def averager(new_value): count += 1 total += new_value return total / count return averager&gt;&gt;&gt; avg= make_averager()&gt;&gt;&gt; avg(10)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 6, in averagerUnboundLocalError: local variable 'count' referenced before assignment The problem is that the statement count += 1 actually means the same as count = count + 1,when count is a number or any immutable type. So we are actually assigning to count in the bodyof averager, and that makes it a local variable. The same problem affects the total variable. We did not have this problem in series because we never assigned to the series name; we onlycalled series.append and invoked sum and len on it. So we took advantage of the fact that lists are mutable. But with immutable types like numbers, strings, tuples, etc., all you can do is read, but never update. Ifyou try to rebind them, as in count = count + 1, then you are implicitly creating a local variablecount. It is no longer a free variable, and therefore it is not saved in the closure. To work around this, the nonlocal declaration was introduced in Python 3. It lets you flag a variable asa free variable even when it is assigned a new value within the function. If a new value is assigned to anonlocal variable, the binding stored in the closure is changed. Calculate a running average without keeping all history (fixed with the use of nonlocal) 123456789def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Function Decorators and Closures01]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Function-Decorators-and-Closures01%2F</url>
    <content type="text"><![CDATA[decoratorA decorator is a callable that takes another function as argument (the decorated function). Thedecorator may perform some processing with the decorated function, and returns it or replaces it withanother function or callable object. 1234567891011&gt;&gt;&gt; def deco(func):... def inner():... print('running inner')... return inner&gt;&gt;&gt; @deco... def target():... print('running target()')&gt;&gt;&gt; target()running inner&gt;&gt;&gt; target&lt;function deco.&lt;locals&gt;.inner at 0x05CDF348&gt; The first crucial fact about decorators is that they have the power to replace the decoratedfunction with a different one. The second crucial fact is that they are executed immediately when a moduleis loaded. This is explained next. When Python Executes Decorators123456789101112131415161718192021222324252627282930313233# _registration.py_registry=[]def register(func): print('running register(%s)'%func) registry.append(func) return func@registerdef f1(): print('running f1()')@registerdef f2():print('running f2()')def f3():print('running f3()')def main(): print('running main()') print('registry -&gt;', registry) f1() f2() f3()if __name__ == '__main__': main()#If registration.py is imported&gt;&gt;&gt; importregistrationrunning register(&lt;function f1 at 0x10063b1e0&gt;)running register(&lt;function f2 at 0x10063b268&gt;)&gt;&gt;&gt; registration.registry[&lt;function f1 at 0x10063b1e0&gt;, &lt;function f2 at 0x10063b268&gt;] decorators are executed as soon as the module is imported, but the decorated functions only runwhen they are explicitly invoked. This highlights the difference between what Pythonistas call importtime and runtime. Considering how decorators are commonly employed in real code, above example is unusual in two ways: The decorator function is defined in the same module as the decorated functions. A real decorator is usually defined in one module and applied to functions in other modules. The register decorator returns the same function passed as argument. In practice, most decorators define an inner function and return it. Decorator-Enhanced Strategy Pattern1234567891011121314151617181920212223242526272829303132promos=[]def promotion(promo_func): promos.append(promo_func) return promo_func@promotiondef fidelity(order): """5% discount for customers with 1000 or more fidelity points""" return order.total() *0.05 if order.customer.fidelity &gt;=1000 else 0@promotiondef bulk_item(order): """10% discount for each LineItem with 20 or more units""" discount=0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * 0.1 return discount@promotiondef large_order(order): """7% discount for orders with 10 or more distinct items""" distinct_items=&#123;item.product for item in order.cart&#125; if len(distinct_items) &gt;= 10: return order.total() * 0.07 return 0def best_promo(order): """Select best discount available""" return max(promo(order) for promo in promos) Most decorators do change the decorated function. They usually do it by defining an inner function andreturning it to replace the decorated function. Code that uses inner functions almost always depends onclosures to operate correctly. Variable Scope Rules12345678910111213&gt;&gt;&gt; b=6&gt;&gt;&gt; def f2(a):... print(a)... print(b)... b=9... &gt;&gt;&gt; f2(3)3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in f2UnboundLocalError: local variable 'b' referenced before assignment When Python compiles the body of the function, it decides that b is a local variable because it is assigned within the function. Python does not require you to declare variables, but assumes that a variable assigned in thebody of a function is local. This is much better than the behavior of JavaScript, which doesnot require variable declarations either, but if you do forget to declare that a variable is local (withvar), you may clobber a global variable without knowing. If we want the interpreter to treat b as a global variable in spite of the assignment within the function,we use the global declaration： 1234567891011121314&gt;&gt;&gt; def f3(a):... global b... print(a)... print(b)... b=9... &gt;&gt;&gt; f3(3)36&gt;&gt;&gt; b9&gt;&gt;&gt; f3(3)39]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_design pattern- Command]]></title>
    <url>%2F2018%2F04%2F22%2FC06-design-pattern-Command%2F</url>
    <content type="text"><![CDATA[Command is another design pattern that can be simplified by the use of functions passed as arguments. MacroCommand12345678class MacroCommand: def __init__(self, commands): self.commands=list(commands) def __call__(self): for command in self.commands: command() More advanced uses of the Command pattern—to support undo, for example—may require morethan a simple callback function. Even then, Python provides a couple of alternatives that deserveconsideration: A callable instance like MacroCommand can keep whatever state is necessary, and provide extra methods in addition to __call__. A closure can be used to hold the internal state of a function between calls.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_Strategy Design and Function-Oriented Strategy]]></title>
    <url>%2F2018%2F04%2F22%2FC06-Strategy-Design-and-Function-Oriented-Strategy%2F</url>
    <content type="text"><![CDATA[Strategy Designpromotion.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from abc import ABC, abstractmethodfrom collections import namedtupleCustomer=namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self,product,quantity,price): self.product=product self.quantity=quantity self.price=price def total(self): return self.price * self.quantityclass Order: def __init__(self, customer,cart, promotion=None): self.customer=customer self.cart=list(cart) self.promotion=promotion def total(self): if not hasattr(self,'__total'): self.__total=sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount=0 else: discount=self.promotion.discount(self) return self.total()- discount def __repr__(self): fmt='&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total, self.due())class Promotion(ABC): @abstractmethod def discount(self,order): ''' return discount as a positive dollar amount '''class FidelityPromo(Promotion): ''' 5% discount for customers with 1000 or more fidelity points ''' def discount(self, order): return order.total() *0.05 if order.customer.fidelity &gt;=1000 else 0class BulkItemPromo(Promotion): ''' 10% discount for each LineItem with 20 or more units ''' def discount(self, order): discount=0 for item in order.cart: if item.quantity &gt;= 20: discount+= item.total() * 0.1 return discountclass LargeOrderPromo(Promotion): ''' "7% discount for orders with 10 or more distinct items ''' def discount(self, order): distinct_items=&#123;item.products for item in order.cart&#125; if len(distinct_items) &gt; 10: return order.total() * 0.07 return 0 Sample usage of Order class with different promotions applied12345678910111213141516171819&gt;&gt;&gt; joe = Customer('John Doe', 0) &gt;&gt;&gt; ann = Customer('Ann Smith', 1100) &gt;&gt;&gt; cart = [LineItem('banana', 4, .5), ... LineItem('apple', 10, 1.5), ... LineItem('watermellon', 5, 5.0)] &gt;&gt;&gt; Order(joe, cart, FidelityPromo()) &lt;Order total: 42.00 due: 42.00&gt; &gt;&gt;&gt; Order(ann, cart, FidelityPromo()) &lt;Order total: 42.00 due: 39.90&gt; &gt;&gt;&gt; banana_cart = [LineItem('banana', 30, .5), ... LineItem('apple', 10, 1.5)] &gt;&gt;&gt; Order(joe, banana_cart, BulkItemPromo()) &lt;Order total: 30.00 due: 28.50&gt; &gt;&gt;&gt; long_order = [LineItem(str(item_code), 1, 1.0) ... for item_code in range(10)] &gt;&gt;&gt; Order(joe, long_order, LargeOrderPromo()) &lt;Order total: 10.00 due: 9.30&gt; &gt;&gt;&gt; Order(joe, cart, LargeOrderPromo()) &lt;Order total: 42.00 due: 42.00&gt; Function-Oriented Strategy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from collections import namedtupleCustomer = namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self, product, quantity, price): self.product = product self.quantity = quantity self.price = price def total(self): return self.price * self.quantityclass Order: # the Context def __init__(self, customer, cart, promotion=None): self.customer = customer self.cart = list(cart) self.promotion = promotion def total(self): if not hasattr(self, '__total'): self.__total = sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount = 0 else: discount = self.promotion(self) # &lt;1&gt; return self.total() - discount def __repr__(self): fmt = '&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total(), self.due())# &lt;2&gt;def fidelity_promo(order): # &lt;3&gt; """5% discount for customers with 1000 or more fidelity points""" return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0def bulk_item_promo(order): """10% discount for each LineItem with 20 or more units""" discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discountdef large_order_promo(order): """7% discount for orders with 10 or more distinct items""" distinct_items = &#123;item.product for item in order.cart&#125; if len(distinct_items) &gt;= 10: return order.total() * .07 return 0# END STRATEGY Sample usage of Order class with promotions as functions1234567&gt;&gt;&gt; joe = Customer('John Doe', 0) &gt;&gt;&gt; ann = Customer('Ann Smith', 1100) &gt;&gt;&gt; cart = [LineItem('banana', 4, .5), ... LineItem('apple', 10, 1.5), ... LineItem('watermellon', 5, 5.0)] &gt;&gt;&gt; Order(joe, cart, fidelity_promo) &lt;Order total: 42.00 due: 42.00&gt; best_promo1234promos=[fidelity_promo, bulk_item_promo, large_order_promo]def best_promo(order): return max(promo(order) for promo in promos) globals()globals()Return a dictionary representing the current global symbol table. This is always the dictionary of the current module (inside a function or method, this is the module where it is defined, not the module from which it is called). 123promos=[globals()[name] for name in globals() if name.endswith('_promo') and name != 'best_promo']def best_promo(order): return max(promo(order) for promo in promos) create a module and put all the strategy functions there, except for best_promoThe list of strategy functions is built by introspection of a separate module called promotions. 1234promos=[func for name, func in inspect.getmembsers(promotions, inspect.isfunction)]def best_promo(order): return max(promo(order) for promo in promos) The function inspect.getmembers returns the attributes of an object—in this case, the promotions module—optionally filtered by a predicate (a boolean function). We use inspect.isfunction to get only the functions from the module.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_functional programming]]></title>
    <url>%2F2018%2F04%2F22%2FC05-functional-programming%2F</url>
    <content type="text"><![CDATA[The operator Module123&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fact(n):... return reduce(lambda a,b : a*b, ranage(1,n+1)) To save you the trouble of writing trivial anonymous functions like lambda a, b:a*b, the operator module provides function equivalents for dozens of arithmetic operators.123&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; def fact(n):... return reduce(mul, range(1,n+1)) itemgetter and attrgetteritemgetter itemgetter(1) does the same as lambda fields: fields[1]: create a function that, given a collection, returns the item at index 1. Because itemgetter uses the [ ] operator, it supports not only sequences but also mappings andany class that implements __getitem__. 1234567891011121314151617&gt;&gt;&gt; metro_data = [... ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),... ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),... ]&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; for city in sorted(metro_data, key=itemgetter(1)):... print(city)('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))('Tokyo', 'JP', 36.933, (35.689722, 139.691667))# pass multiple index arguments to itemgetter, the function it builds will returntuples with the extracted values&gt;&gt;&gt; cc_name = itemgetter(1, 0)&gt;&gt;&gt; for city in metro_data:... print(cc_name(city))('JP', 'Tokyo')('IN', 'Delhi NCR') attrgetter attrgetter creates functions to extract object attributes by name. If you pass attrgetter severalattribute names as arguments, it also returns a tuple of values. In addition, if any argument name contains a . (dot), attrgetter navigates through nested objects to retrieve the attribute.123456789101112131415161718192021222324&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; LatLong = namedtuple('LatLong', 'lat long') #&gt;&gt;&gt; Metropolis = namedtuple('Metropolis', 'name cc pop coord') # &gt;&gt;&gt; metro_areas = [Metropolis(name, cc, pop, LatLong(lat, long)) #... for name, cc, pop, (lat, long) in metro_data]&gt;&gt;&gt; metro_areas[0]Metropolis(name='Tokyo', cc='JP', pop=36.933, coord=LatLong(lat=35.689722,long=139.691667))&gt;&gt;&gt; metro_areas[0].coord.lat #Reach into element metro_areas[0] to get its latitude.35.689722&gt;&gt;&gt; from operator import attrgetter&gt;&gt;&gt; name_lat = attrgetter('name', 'coord.lat') #Define an attrgetter to retrieve the name and the coord.lat nested attribute&gt;&gt;&gt;&gt;&gt;&gt; for city in sorted(metro_areas, key=attrgetter('coord.lat')): # Use attrgetter again to sort list of cities by latitude.... print(name_lat(city)) #...('Sao Paulo', -23.547778)('Mexico City', 19.433333)('Delhi NCR', 28.613889)('Tokyo', 35.689722)('New York-Newark', 40.808611) methodcallermethodcaller creates a function on the fly. The function it creates calls a method by name on theobject given as argument. 12345678&gt;&gt;&gt; from operator import methodcaller&gt;&gt;&gt; s = 'The time has come'&gt;&gt;&gt; upcase = methodcaller('upper')&gt;&gt;&gt; upcase(s)'THE TIME HAS COME'&gt;&gt;&gt; hiphenate = methodcaller('replace', ' ', '-')&gt;&gt;&gt; hiphenate(s)'The-time-has-come' functools.partialfunctools.partial is a higher-order function that allows partial application of a function. Given a function, a partial application produces a new callable with some of the arguments of the original function fixed. partial takes a callable as first argument, followed by an arbitrary number of positional and keywordarguments to bind. 1234567&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; triple=partial(mul, 3)&gt;&gt;&gt; triple(7)21&gt;&gt;&gt; list(map(triple, range(1,10)))[3, 6, 9, 12, 15, 18, 21, 24, 27] 1234567891011&gt;&gt;&gt; tag&lt;function tag at 0x03DFA300&gt;&gt;&gt;&gt; picture=partial(tag, 'img', cls='pic-frame')&gt;&gt;&gt; picturefunctools.partial(&lt;function tag at 0x03DFA300&gt;, 'img', cls='pic-frame')&gt;&gt;&gt; picture.func&lt;function tag at 0x03DFA300&gt;&gt;&gt;&gt; picture.args('img',)&gt;&gt;&gt; picture.keywords&#123;'cls': 'pic-frame'&#125; The functools.partialmethod function (new in Python 3.4) does the same job as partial, but isdesigned to work with methods.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_functions]]></title>
    <url>%2F2018%2F04%2F22%2FC05-functions%2F</url>
    <content type="text"><![CDATA[Higher-Order FunctionsReplacements for map, filter, and reduceFunctional languages commonly offer the map, filter, and reduce higher-order functions(sometimes with different names). A listcomp or a genexp does the job of map and filtercombined, but is more readable. 12345678&gt;&gt;&gt; list(map(fact, range(6)))[1, 1, 2, 6, 24, 120]&gt;&gt;&gt; [fact(n) for n in range(6)][1, 1, 2, 6, 24, 120]&gt;&gt;&gt; list(map(factorial, filter(lambda n: n % 2, range(6))))[1, 6, 120]&gt;&gt;&gt; [factorial(n) for n in range(6) if n % 2][1, 6, 120] map and filter return generators—a form of iterator—so their direct substitute is now a generator expression . The reduce function was demoted from a built-in in Python 2 to the functools module in Python 3.123456&gt;&gt;&gt; from functools import reduce #Starting with Python 3.0, reduce is not a built-in&gt;&gt;&gt; from operator import add&gt;&gt;&gt; reduce(add, range(100))4950&gt;&gt;&gt; sum(range(100)) Other reducing built-ins are all and any:all(iterable)Returns True if every element of the iterable is truthy; all([ ]) returns True.any(iterable)Returns True if any element of the iterable is truthy; any([ ]) returns False. Anonymous FunctionsThe best use of anonymous functions is in the context of an argument list. 12&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=lambda word: word[::-1]) Callable ObjectsThe call operator (i.e., ()) may be applied to other objects beyond user-defined functions. To determinewhether an object is callable, use the callable() built-in function. User-Defined Callable TypesArbitrary Python objects may also be made to behave like functions. Implementing a __call__instance method is all it takes. 123456789101112131415import randomclass BingoCage: def __init__(self, items): self._items=list(items) random.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError('pick from empty BingoCage') def __call__(self): return self.pick() A class implementing *__call__**is an easy way to create function-like objects that havesome internal state that must be kept across invocations. An example is a decorator. Decorators must befunctions, but it is sometimes convenient to be able to “remember” something between calls of thedecorator. Function Introspection123456&gt;&gt;&gt; dir(factorial)['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__'] __dict__Like the instances of a plain user-defined class, a function uses the__dict__attribute to store userattributes assigned to it. Listing attributes of functions that don’t exist in plain instances1234567&gt;&gt;&gt; class C: pass #&gt;&gt;&gt; obj = C() #&gt;&gt;&gt; def func(): pass #&gt;&gt;&gt; sorted(set(dir(func)) - set(dir(obj))) #['__annotations__', '__call__', '__closure__', '__code__', '__defaults__','__get__', '__globals__', '__kwdefaults__', '__name__', '__qualname__'] Positional and Keyword-Only ParametersUsing* and ** to “explode” iterables and mappings into separate arguments whenwe call a function. __defaults__ 123456789101112131415def clip(text, max_len=80): """Return text clipped at the last space before or after max_len """ end = None if len(text) &gt; max_len: space_before = text.rfind(' ', 0, max_len) if space_before &gt;= 0: end = space_before else: space_after = text.rfind(' ', max_len) if space_after &gt;= 0: end = space_after if end is None: # no spaces were found end = len(text) return text[:end].rstrip() The values of __defaults__, __code__.co_varnames, and__code__.co_argcount for the clip function:123456789&gt;&gt;&gt; from clip import clip&gt;&gt;&gt; clip.__defaults__(80,)&gt;&gt;&gt; clip.__code__ # doctest: +ELLIPSIS&lt;code object clip at 0x...&gt;&gt;&gt;&gt; clip.__code__.co_varnames(&apos;text&apos;, &apos;max_len&apos;, &apos;end&apos;, &apos;space_before&apos;, &apos;space_after&apos;)&gt;&gt;&gt; clip.__code__.co_argcount2 Extracting the function signature:123456789101112&gt;&gt;&gt; from clip import clip&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; sig = signature(clip)&gt;&gt;&gt; sig # doctest: +ELLIPSIS&lt;inspect.Signature object at 0x...&gt;&gt;&gt;&gt; str(sig)'(text, max_len=80)'&gt;&gt;&gt; for name, param in sig.parameters.items():... print(param.kind, ':', name, '=', param.default)...POSITIONAL_OR_KEYWORD : text = &lt;class 'inspect._empty'&gt;POSITIONAL_OR_KEYWORD : max_len = 80 The kind attribute holds one of five possible values from the _ParameterKind class:POSITIONAL_OR_KEYWORDA parameter that may be passed as a positional or as a keyword argument (most Python functionparameters are of this kind).VAR_POSITIONALA tuple of positional parameters.VAR_KEYWORDA dict of keyword parameters.KEYWORD_ONLYA keyword-only parameter (new in Python 3).POSITIONAL_ONLYA positional-only parameter; currently unsupported by Python function declaration syntax. inspect.Signature 12345678910111213141516171819def tag(name,*content, cls=None, **attrs): pass&gt;&gt;&gt; import inspect&gt;&gt;&gt; sig = inspect.signature(tag)&gt;&gt;&gt; my_tag = &#123;'name': 'img', 'title': 'Sunset Boulevard',... 'src': 'sunset.jpg', 'cls': 'framed'&#125;&gt;&gt;&gt; bound_args = sig.bind(**my_tag)&gt;&gt;&gt; bound_args&lt;inspect.BoundArguments object at 0x...&gt;&gt;&gt;&gt; for name, value in bound_args.arguments.items(): #Iterate over the items in bound_args.arguments, which is an OrderedDict, to display the names and # values of the arguments.... print(name, '=', value)...name = imgcls = framedattrs = &#123;'title': 'Sunset Boulevard', 'src': 'sunset.jpg'&#125;&gt;&gt;&gt; del my_tag['name']&gt;&gt;&gt; bound_args = sig.bind(**my_tag)Traceback (most recent call last): Function AnnotationsEach argument in the function declaration may have an annotation expression preceded by :. If there is a default value, the annotation goes between the argument name and the = sign. To annotate the return value, add -&gt; and another expression between the ) and the : at the tail of the function declaration. The expressions may be of any type. 12345678# clip_annot.pydef clip(text:str, max_len:'int &gt; 0' =80) -&gt;str: end=None if len(text) &gt; max_len: pass&gt;&gt;&gt; from clip_annot import clip&gt;&gt;&gt; clip.__annotations__&#123;'text': &lt;class 'str'&gt;, 'max_len': 'int &gt; 0', 'return': &lt;class 'str'&gt;&#125; Extracting annotations from the function signatureThe signature function returns a Signature object, which has a return_annotation attribute and aparameters dictionary mapping parameter names to Parameter objects. Each Parameter object has itsown annotation attribute. 12345678910&gt;&gt;&gt; from clip_annot import clip&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; sig = signature(clip)&gt;&gt;&gt; sig.return_annotation&lt;class 'str'&gt;&gt;&gt;&gt; for param in sig.parameters.values():... note = repr(param.annotation).ljust(13)... print(note, ':', param.name, '=', param.default)&lt;class 'str'&gt; : text = &lt;class 'inspect._empty'&gt;'int &gt; 0' : max_len = 80]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_bytes, bytesarray, struct module, memory views]]></title>
    <url>%2F2018%2F04%2F22%2FC04-bytes-bytesarray-struct-module-memory-views%2F</url>
    <content type="text"><![CDATA[The Unicode standard explicitly separates the identity of characters from specific byte representations: The identity of a character The actual bytes that represent a character depend on the encoding in use. Although binary sequences are really sequences of integers, their literal notation reflects the fact that ASCII text is often embedded in them. Therefore, three different displays are used, depending on each byte value: For bytes in the printable ASCII range—from space to ~—the ASCII character itself is used. For bytes corresponding to tab, newline, carriage return, and \, the escape sequences \t, \n, \r, and \ are used. For every other byte value, a hexadecimal escape sequence is used (e.g., \x00 is the null byte). Binary sequences have a class method that str doesn’t have, called fromhex, which builds a binary sequence by parsing pairs of hex digits optionally separated by spaces:12&gt;&gt;&gt; bytes.fromhex('31 4B CE A9')b'1K\xce\xa9' Building a binary sequence from a buffer-like object is a low-level operation that may involve type casting.12345&gt;&gt;&gt; import array&gt;&gt;&gt; numbers = array.array('h', [-2, -1, 0, 1, 2])&gt;&gt;&gt; octets = bytes(numbers)&gt;&gt;&gt; octetsb'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00' Structs and Memory ViewsThe struct module provides functions to parse packed bytes into a tuple of fields of different types and to perform the opposite conversion, from a tuple into packed bytes. Struct is used with bytes, bytearray, and memoryview objects. 123&gt;&gt;&gt; import struct&gt;&gt;&gt; fmt = '&lt;3s3sHH' #struct format: &lt; little-endian; 3s3s two sequences of 3 bytes; HH two 16-bitintegers. 123456789101112&gt;&gt;&gt; with open('filter.gif', 'rb') as fp:... img = memoryview(fp.read()) #...&gt;&gt;&gt; header = img[:10] # another memoryview by slicing the first one; no bytes are copied here&gt;&gt;&gt; bytes(header) #Convert to bytes for display only; 10 bytes are copied hereb'GIF89a+\x02\xe6\x00'&gt;&gt;&gt; struct.unpack(fmt, header) #Unpack memoryview into tuple of: type, version, width, and height(b'GIF', b'89a', 555, 230)&gt;&gt;&gt; del header #Delete references to release the memory associated with the memoryviewinstances&gt;&gt;&gt; del img Discover the Encoding of a Byte SequenceIf you omit the encoding argument when opening a file, the default is given by locale.getpreferredencoding() (‘cp1252’ on Windows). It is the default for opening text files and for sys.stdout/stdin/stderr when they are redirected to files. The encoding of sys.stdout/stdin/stderr is given by the PYTHONIOENCODING environment variable, if present, otherwise it is either inherited from the console or defined by locale.getpreferredencoding() if the output/input is redirected to/from a file. sys.getdefaultencoding() is used internally by Python to convert binary data to/from str; this happens less often in Python 3, but still happens. sys.getfilesystemencoding() is used to encode/decode filenames (not file contents). It is used when open() gets a str argument for the filename; if the filename is given as a bytes argument, it is passed unchanged to the OS API. The Python Unicode HOWTO says: “on Windows, Python uses the name mbcs to refer to whatever the currently configured encoding is.” The acronym MBCS stands for Multi Byte Character Set, which for Microsoft are the legacy variable-width encodings like gb2312 or Shift_JIS, but not UTF-8.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_set, hash tables]]></title>
    <url>%2F2018%2F04%2F22%2FC03-set-hash-tables%2F</url>
    <content type="text"><![CDATA[set, forzensetA set is a collection of unique objects. A basic use case is removing duplication: 12345&gt;&gt;&gt; l = ['spam', 'spam', 'eggs', 'spam']&gt;&gt;&gt; set(l)&#123;'eggs', 'spam'&#125;&gt;&gt;&gt; list(set(l))['eggs', 'spam'] There is no special syntax to represent frozenset literals—they must be created by calling the constructor. 12&gt;&gt;&gt; frozenset(range(10))frozenset(&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;) There’s no literal notation for the empty set, so we must remember to write set(). hashA hash table is a sparse array (i.e., an array that always has empty cells). In standard data structure texts,the cells in a hash table are often called “buckets.” In a dict hash table, there is a bucket for each item, andit contains two fields: a reference to the key and a reference to the value of the item. Because all bucketshave the same size, access to an individual bucket is done by offset. To put an item in a hash table, the first step is to calculate the hash value of the item key, which is donewith the hash() built-in function. Hashes and equalityThe hash() built-in function works directly with built-in types and falls back to calling __hash__ for user-defined types. If two objects compare equal, their hash values must also be equal, otherwise the hashtable algorithm does not work. To be effective as hash table indexes, hash values should scatter around the index space as much aspossible. This means that, ideally, objects that are similar but not equal should have hash values that differwidely. hash table algorithmTo fetch the value at my_dict[search_key], Python calls hash(search_key) to obtain the hash value ofsearch_key and uses the least significant bits of that number as an offset to look up a bucket in the hashtable (the number of bits used depends on the current size of the table). If the found bucket is empty,KeyError is raised.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re module_compile, finditer, group, match]]></title>
    <url>%2F2018%2F04%2F22%2Fre-module-compile-finditer-group-match%2F</url>
    <content type="text"><![CDATA[使用 re 模块有两种方式： 使用 re.compile 函数生成一个 Pattern 对象，然后使用 Pattern 对象的一系列方法对文本进行匹配查找； 直接使用 re.match, re.search 和 re.findall 等函数直接对文本匹配查找。 re 模块的一般使用步骤如下： 使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象 通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果（一个 Match 对象） 最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作 Python 的正则匹配默认是贪婪匹配。 compile12import repattern=re.compile(r'\w+') pattern的常用方法： match 方法 search 方法 findall 方法 finditer 方法 split 方法 sub 方法 subn 方法 matchmatch 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。123456789101112&gt;&gt;&gt; pattern=re.compile(r'\d+')&gt;&gt;&gt; m=pattern.match(''one12twothree34four', 3, 10') #从'1'的位置开始匹配，正好匹配&gt;&gt;&gt; print(m) # 返回一个 Match 对象&lt;_sre.SRE_Match object at 0x10a42aac0&gt;&gt;&gt;&gt; m.group(0) # 可省略 0'12'&gt;&gt;&gt; m.start(0) # 可省略 03&gt;&gt;&gt; m.end(0) # 可省略 05&gt;&gt;&gt; m.span(0) # 可省略 0(3, 5) 当匹配成功时返回一个 Match 对象，其中： group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)； start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0； end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0； span([group]) 方法返回 (start(group), end(group))。 123456&gt;&gt;&gt; pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I) # re.I 表示忽略大小写&gt;&gt;&gt; m = pattern.match('Hello World Wide Web')&gt;&gt;&gt; m.group(0) # 返回匹配成功的整个子串'Hello World'&gt;&gt;&gt; m.groups() # 等价于 (m.group(1), m.group(2), ...)('Hello', 'World') finditerfinditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器12for match in pattern.finditer(hello 234 ref'): print(match.group(), m1.span())]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_Generic Mapping Types, hashable]]></title>
    <url>%2F2018%2F04%2F22%2FC03-Generic-Mapping-Types-hashable%2F</url>
    <content type="text"><![CDATA[hashableAn object is hashable if it has a hash value which never changes during its lifetime (it needs a hash()method), and can be compared to other objects (it needs an eq() method). Hashable objects whichcompare equal must have the same hash value.The atomic immutable types (str, bytes, numeric types) are all hashable. A frozen set is always hashable,because its elements must be hashable by definition. A tuple is hashable only if all its items are hashable. User-defined types are hashable by default because their hash value is their id() and they all compare notequal. If an object implements a custom eq that takes into account its internal state, it may be hashable only if all its attributes are immutable. ways to build a dict1234567&gt;&gt;&gt; a = dict(one=1, two=2, three=3)&gt;&gt;&gt; b = &#123;'one': 1, 'two': 2, 'three': 3&#125;&gt;&gt;&gt; c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))&gt;&gt;&gt; d = dict([('two', 2), ('one', 1), ('three', 3)])&gt;&gt;&gt; e = dict(&#123;'three': 3, 'one': 1, 'two': 2&#125;)&gt;&gt;&gt; a == b == c == d == eTrue dict comprehension A dictcomp builds a dict instance by producing key:value pair from any iterable. dict.setdefault12345678910111213141516import sysimport reWORD_RE = re.compile('\w+')index = &#123;&#125;with open(sys.argv[1], encoding='utf-8') as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) # this is ugly; coded like this to make a point occurrences = index.get(word, []) occurrences.append(location) index[word] = occurrences for word in sorted(index, key=str.upper): print(word, index[word]) 123456789with open(sys.argv[1], encoding='utf-8') as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) index.setdefault(word, []).append(location) #Get the list of occurrences for word, or set it to [ ] if not found; setdefault returns the value, so it can be #updated without requiring a second search. collections.defaultdictA defaultdict is configured to create items on demand whenever a missing key is searched. When instantiating a defaultdict, you provide a callable that is used to produce a default value whenever__getitem__ is passed a nonexistent key argument. given an empty defaultdict created as dd = defaultdict(list), if ‘new-key’ is not in dd, the expressiondd[‘new-key’] does the following steps: Calls list() to create a new list. Inserts the list into dd using ‘new-key’ as key. Returns a reference to that list 123456789101112131415161718import sysimport reimport collectionsword_re=re.compile('\w+')index=collections.defaultdict(list) #Create a defaultdict with the list constructor as default_factory.If no default_factory is provided, the usual KeyError is raised for missing keys.with open(sys.argv[1],encoding='utf-8') as fp: for line_no, line in enumerate(fp,1): for match in word_re.finditer(line): word=match.group() column_no=match.start()+1 location=(line_no,column_no) index[word].append(location) #If word is not initially in the index, the default_factory is called to produce the missing value, which in this #case is an empty list that is then assigned to index[word] and returned, so the .append(location) #operation always succeeds. __missing__If you subclass dict and provide a __missing__ method, the standard dict.getitemwill call itwhenever a key is not found, instead of raising KeyError. The __missing__ method is just called by __getitem__ (i.e., forthe d[k] operator). The presence of a __missing__ method has no effect on the behavior of other methods that look up keys, such as get or__contains__ (which implements the in operator). This is why the default_factory of defaultdict worksonly with__getitem__, 12345678910111213141516class StrKeyDict0(dict): def __missing__(self,key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def get(self,key,default=None): try: return self[key] #The get method delegates to __getitem__ by using the self[key] notation; that gives the opportunity for our __missing__ to act. except KeyError: return default def __contains__(self,key): #we do not check for the key in the usual Pythonic way—k in my_dict—becausestr(key) in self would recursively call __contains__. We avoid this by explicitly looking up the key in self.keys(). return key in self.keys() or str(key) in self.keys() collections.OrderedDict collections.ChainMap collections.Counter collections.UserDictUserDict it’s preferable to subclass from UserDict rather than from dict is that the built-in has someimplementation shortcuts that end up forcing us to override methods that we can just inherit fromUserDict with no problems. Note that UserDict does not inherit from dict, but has an internal dict instance, called data, which holdsthe actual items. This avoids undesired recursion when coding special methods like __setitem__, andsimplifies the coding of __contains__.1234567891011class StrKeyDict(collections.UserDict): def __missing__(self,key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def __contains__(self, key): return str(key) in self.data def __setitem__(self, key, item): self.data[str(key)] = item UserDict subclasses MutableMapping, the remaining methods that make StrKeyDict a full-fledgedmapping are inherited from UserDict, MutableMapping, or Mapping. The latter have several usefulconcrete methods, in spite of being abstract base classes (ABCs). The following methods are worth noting: MutableMapping.update This powerful method can be called directly but is also used by __init__ to load the instance from other mappings, from iterables of (key, value) pairs, and keyword arguments. Because it uses self[key] = value to add items, it ends up calling our implementation of __setitem__. Mapping.get In StrKeyDict0 (Example 3-7), we had to code our own get to obtain results con‐sistent with getitem, but in Example 3-8 we inherited Mapping.get, which isimplemented exactly like StrKeyDict0.get (see Python source code). MappingProxyTypeThe types module provides a wrapper class called MappingProxyType, which, given a mapping, returns amappingproxy instance that is a read-only but dynamic view of the original mapping. This means thatupdates to the original mapping can be seen in the mappingproxy, but changes cannot be made throughit. 12345678910111213141516&gt;&gt;&gt; from types import MappingProxyType&gt;&gt;&gt; d = &#123;1: 'A'&#125;&gt;&gt;&gt; d_proxy = MappingProxyType(d)&gt;&gt;&gt; d_proxymappingproxy(&#123;1: 'A'&#125;)&gt;&gt;&gt; d_proxy[1] # Items in d can be seen through d_proxy.'A'&gt;&gt;&gt; d_proxy[2] = 'x' # Changes cannot be made through d_proxy.Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment&gt;&gt;&gt; d[2] = 'B'&gt;&gt;&gt; d_proxy # d_proxy is dynamic: any change in d is reflectedmappingproxy(&#123;1: 'A', 2: 'B'&#125;)&gt;&gt;&gt; d_proxy[2]'B']]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C_deque]]></title>
    <url>%2F2018%2F04%2F22%2FC-deque%2F</url>
    <content type="text"><![CDATA[Inserting and removing from the left of a list (the 0-index end) is costly because the entire list must be shifted. The class collections.deque is a thread-safe double-ended queue designed for fast inserting and removingfrom both ends. It is also the way to go if you need to keep a list of “last seen items” or something like that, because a deque can be bounded—i.e., created with a maximum length—and then, when it is full, itdiscards items from the opposite end when you append new ones.The append and popleft operations are atomic, so deque is safe to use as a LIFO queue in multithreadedapplications without the need for using locks.12345678910111213141516171819&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; dq = deque(range(10), maxlen=10)&gt;&gt;&gt; dqdeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.rotate(3)&gt;&gt;&gt; dqdeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)&gt;&gt;&gt; dq.rotate(-4)&gt;&gt;&gt; dqdeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)&gt;&gt;&gt; dq.appendleft(-1) #Appending to a deque that is full discards items from the other end&gt;&gt;&gt; dqdeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.extend([11, 22, 33])&gt;&gt;&gt; dqdeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)&gt;&gt;&gt; dq.extendleft([10, 20, 30, 40])&gt;&gt;&gt; dqdeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Array, Memory Views]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Array-Memory-Views%2F</url>
    <content type="text"><![CDATA[An array does not actually hold full-fledged float objects, but only the packed bytes representing theirmachine values—just like an array in the C language. On the other hand, if you are constantly adding andremoving items from the ends of a list as a FIFO or LIFO data structure, a deque (double-ended queue)works faster. array.arrayIf the list will only contain numbers, an array.array is more efficient than a list: it supports all mutablesequence operations (including .pop, .insert, and .extend), and additional methods for fast loading andsaving such as .frombytes and .tofile. When creating an array, you provide a typecode,A letter to determine the underlying C type used to store each item in the array. For example, b is thetypecode for signed char. If you create an array(‘b’), then each item will be stored in a single byte andinterpreted as an integer from –128 to 127. For large sequences of numbers, this saves a lot of memory.And Python will not let you put any number that does not match the type for the array.12345678910111213141516&gt;&gt;&gt; from array import array&gt;&gt;&gt; from random import random&gt;&gt;&gt; floats = array('d', (random() for i in range(10**7)))&gt;&gt;&gt; floats[-1]0.07802343889111107&gt;&gt;&gt; fp = open('floats.bin', 'wb')&gt;&gt;&gt; floats.tofile(fp)&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2 = array('d') &gt;&gt;&gt; fp = open('floats.bin', 'rb')&gt;&gt;&gt; floats2.fromfile(fp, 10**7)&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2[-1]0.07802343889111107&gt;&gt;&gt; floats2 == floatsTrue As of Python 3.4, the array type does not have an in-place sort method like list.sort(). If you need to sortan array, use the sorted function to rebuild it sorted:1a=array.array(typecode, sorted(a)) memoryviewA memoryview is essentially a generalized NumPy array structure in Python itself (without the math). Itallows you to share memory between data-structures (things like PIL images, SQLlite databases, NumPyarrays, etc.) without first copying. This is very important for large data sets. memoryview.cast method lets you change the way multiple bytes are read or written as units withoutmoving bits around (just like the C cast operator). memoryview.cast returns yet another memoryviewobject, always sharing the same memory]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_list.sort and the sorted Built-In Function, bisect]]></title>
    <url>%2F2018%2F04%2F22%2FC02-list-sort-and-the-sorted-Built-In-Function-bisect%2F</url>
    <content type="text"><![CDATA[list.sort and sortedThe list.sort method sorts a list in place—that is, without making a copy. It returns None to remind us that itchanges the target object, and does not create a new list. This is an important Python API convention:functions or methods that change an object in place should return None to make it clear to the caller thatthe object itself was changed, and no new object was created.The built-in function sorted creates a new list and returns it. In fact, itaccepts any iterable object as anargument, including immutable sequences and generators. Regardless of the type of iterable given to sorted, it always returns a newly created list Both list.sort and sorted take two optional, keyword-only arguments:reverse: If True, the items are returned in descending order (i.e., by reversing the comparison of the items). The default is False. key: A one-argument function that will be applied to each item to produce its sorting key. For example, when sorting a list of strings, key=str.lower can be used to perform a case-insensitive sort, and key=len will sort the strings by character length. The default is the identity function (i.e., the items themselves are compared).bisectThe bisect module offers two main functions—bisect and insort. bisect(haystack, needle) does a binary search for needle in haystack—which must be a sorted sequence.You could use the result of bisect(haystack, needle) as the index argument to haystack.insert(index, needle)—however, using insort does both steps, and is faster. bisect finds insertion points for items in a sorted sequencebisect_right returns an insertion point after the existing item, and bisect_left returns the position of theexisting item, so insertion would occur before it.1234567891011121314151617181920212223242526import bisectimport sysHAYSTACK=[1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]NEEDLES=[0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]ROW_FMT='&#123;0:2d&#125; @ &#123;1:2d&#125; &#123;2&#125;&#123;0:&lt;2d&#125;'def demo(bisect_fn): for needle in reversed(NEEDLES): positioin=bisect_fn(HAYSTACK, needle) #Use the chosen bisect function to get the insertion point. offset=positioin * ' |' print(ROW_FMT.format(needle, positioin, offset))if __name__=='__main__': if sys.argv[-1] =='left': bisect_fn=bisect.bisect_left else: bisect_fn=bisect.bisect print('DEMO:', bisect_fn.__name__) print('haystack -&gt;',' '.join('%2d'% n for n in HAYSTACK )) demo(bisect_fn) 1234567&gt;&gt;&gt; import bisect&gt;&gt;&gt; def grade(score, breakpoints=[60,70,80,90],grades='FDCBA'):... i=bisect.bisect(breakpoints, score)... return grades[i]... &gt;&gt;&gt; [grade(score) for score in [33, 99, 77, 70, 89, 90, 100]]['F', 'A', 'C', 'C', 'B', 'A', 'A'] Inserting with bisect.insortinsort(seq, item) inserts item into seq so as to keep seq in ascending order12345678910111213141516171819import bisectimport randomSIZE=7random.seed(1729)my_list=[]for i in range(SIZE): new_item = random.randrange(SIZE*2) bisect.insort(my_list, new_item) print('%2d -&gt;' % new_item, my_list)#10 -&gt; [10]#0 -&gt; [0, 10]#6 -&gt; [0, 6, 10]#8 -&gt; [0, 6, 8, 10]#7 -&gt; [0, 6, 7, 8, 10]#2 -&gt; [0, 2, 6, 7, 8, 10]#10 -&gt; [0, 2, 6, 7, 8, 10, 10]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Sequence Assignment +=, *=]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Sequence-Assignment%2F</url>
    <content type="text"><![CDATA[#The augmented assignment operators += and *= behave very differently depending on the first operand. The special method that makes += work is iadd (for “in-place addition”). However, if iadd is notimplemented, Python falls back to calling add.If a implements iadd, a += b will be called. In the case of mutable sequences (e.g., list, bytearray,array.array), a will be changed in place (i.e., the effect will be similar to a.extend(b)). However, when a doesnot implement iadd, the expression a += b has the same effect as a = a + b: the expression a + b isevaluated first, producing a new object, which is then bound to a. In general, for mutable sequences, it is a good bet that iadd is implemented and that += happens inplace. For immutable sequences, clearly there is no way for that to happen.1234567891011121314&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; id(l)4311953800&gt;&gt;&gt; l *= 2&gt;&gt;&gt; l[1, 2, 3, 1, 2, 3]&gt;&gt;&gt; id(l)4311953800&gt;&gt;&gt; t = (1, 2, 3)&gt;&gt;&gt; id(t)4312681568&gt;&gt;&gt; t *= 2&gt;&gt;&gt; id(t)4301348296]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Slice Objects]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Slice-Objects%2F</url>
    <content type="text"><![CDATA[The notation a :b :c is only valid within [ ] when used as the indexing or subscript operator, and it produces aslice object: slice(a, b, c).To evaluate the expression seq[start:stop:step], Python calls seq.getitem(slice(start, stop, step)). Assign to slices12345678910&gt;&gt;&gt; l=list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; l[2:5]=[20,30] #When the target of the assignment is a slice, the right side must be an iterable object, even if it has just one item.&gt;&gt;&gt; l[0, 1, 20, 30, 5, 6, 7, 8, 9]&gt;&gt;&gt; del l[5:7]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 8, 9] Building Lists of Lists1123456&gt;&gt;&gt; board = [['_'] * 3 for i in range(3)]&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; board[1][2] = 'X'&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']] Equivalent to:12345678910&gt;&gt;&gt; board = []&gt;&gt;&gt; for i in range(3):... row = ['_'] * 3 #... board.append(row)...&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; board[2][0] = 'X'&gt;&gt;&gt; board #[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']] 2123456&gt;&gt;&gt; weird_board = [['_'] * 3] * 3&gt;&gt;&gt; weird_board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; weird_board[1][2] = 'O'&gt;&gt;&gt; weird_board[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']] Equivalent to:1234row = ['_'] * 3board = []for i in range(3): board.append(row)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Tuple unpacking, namedtuple]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Tuple-unpacking-namedtuple%2F</url>
    <content type="text"><![CDATA[Tuple unpacking works with any iterable object. The only requirement is that the iterable yields exactly oneitem per variable in the receiving tuple, unless you use a star (*) to capture excess items. 123456789101112&gt;&gt;&gt; t=(20,8)&gt;&gt;&gt; divmod(t)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: divmod expected 2 arguments, got 1&gt;&gt;&gt; divmod(*t)(2, 4)&gt;&gt;&gt; import os&gt;&gt;&gt; _,filename=os.path.split('/hom/lubi/.ssh/if.pub')&gt;&gt;&gt; filename'if.pub' use * to grab excess items123456&gt;&gt;&gt; a,b,*rest=range(5)&gt;&gt;&gt; a,b,rest(0, 1, [2, 3, 4])&gt;&gt;&gt; *head,b,c=range(5)&gt;&gt;&gt; head,b,c([0, 1, 2], 3, 4) namedtuple Two parameters are required to create a named tuple: a class name and a list of field names, which can be given as an iterable of strings or as a single spacedelimited string. Data must be passed as positional arguments to the constructor (in contrast, the tuple constructor takes a single iterable). You can access the fields by name or position. 12345678910&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; City = namedtuple('City', 'name country population coordinates')&gt;&gt;&gt; tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667))&gt;&gt;&gt; tokyoCity(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722,139.691667))&gt;&gt;&gt; tokyo.population36.933&gt;&gt;&gt; tokyo[1]'JP' _fields is a tuple with the field names of the class. _make() allow you to instantiate a named tuple from an iterable; City(*delhi_data) would do the same. _asdict() returns a collections.OrderedDict built from the named tuple instance. 12345678910&gt;&gt;&gt; City._fields('name', 'country', 'population', 'coordinates')&gt;&gt;&gt; LatLong = namedtuple('LatLong', 'lat long')&gt;&gt;&gt; delhi_data = ('Delhi NCR', 'IN', 21.935, LatLong(28.613889, 77.208889))&gt;&gt;&gt; delhi = City._make(delhi_data)&gt;&gt;&gt; delhi._asdict()OrderedDict([('name', 'Delhi NCR'), ('country', 'IN'), ('population',21.935), ('coordinates', LatLong(lat=28.613889, long=77.208889))])&gt;&gt;&gt; for key, value in delhi._asdict().items(): print(key + ':', value)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Generate Expressions]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Generate-Expressions%2F</url>
    <content type="text"><![CDATA[To initialize tuples, arrays, and other types of sequences, you could also start from a listcomp, but a genexpsaves memory because it yields items one by one using the iterator protocol instead of building a whole listjust to feed another constructor.12345678910&gt;&gt;&gt; symbols = '$¢£¥€¤'&gt;&gt;&gt; tuple(ord(symbol) for symbol in symbols)(36, 162, 163, 165, 8364, 164)#If the generator expression is the single argument in a function call, there is no need to duplicate the #enclosing parentheses.&gt;&gt;&gt; import array&gt;&gt;&gt; array.array('I', (ord(symbol) for symbol in symbols))array('I', [36, 162, 163, 165, 8364, 164])#The array constructor takes two arguments, so the parentheses around the generator expression are mandatory.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_listcomp_speed.py]]></title>
    <url>%2F2018%2F04%2F22%2FC02-listcomp-speed-py%2F</url>
    <content type="text"><![CDATA[Container sequences and Flat sequencesContainer sequences:list, tuple, and collections.deque can hold items of different types. Flat sequences:str, bytes, bytearray, memoryview, and array.array hold items of one type. Container sequences hold references to the objects they contain, which may be of any type, while flatsequences physically store the value of each item within its own memory space, and not as distinctobjects. Mutable sequences and Immutable sequencesMutable sequences:list, bytearray, array.array, collections.deque, and memoryview Immutable sequences:tuple, str, and bytes map and filter were faster than the equivalent listcomps123456789101112131415161718import timeitTIMES = 10000SETUP = """symbols = '$¢£¥€¤'def non_ascii(c): return c &gt; 127"""def clock(label, cmd): res = timeit.repeat(cmd, setup=SETUP, number=TIMES) print(label, *('&#123;:.3f&#125;'.format(x) for x in res))clock('listcomp :', '[ord(s) for s in symbols if ord(s) &gt; 127]')clock('listcomp + func :', '[ord(s) for s in symbols if non_ascii(ord(s))]')clock('filter + lambda :', 'list(filter(lambda c: c &gt; 127, map(ord, symbols)))')clock('filter + func :', 'list(filter(non_ascii, map(ord, symbols)))')]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_repr__,__str__,__bool__]]></title>
    <url>%2F2018%2F04%2F22%2FC01-repr-str-bool%2F</url>
    <content type="text"><![CDATA[repr, strThe repr special method is called by the repr built-in to get the string representation of the object forinspection.The interactive console and debugger call repr on the results of the expressions evaluated. Contrast repr with str, which is called by the str() constructor and implicitly used by the printfunction. str should return a string suitable for display to end users.If you only implement one ofthese special methods, choose repr, because when no custom str is available, Python will callrepr as a fallback. boolBy default, instances of user-defined classes are considered truthy, unless either__bool__ or lenis implemented. Basically, bool(x) calls x.bool() and uses the result. If bool is not implemented,Python tries to invoke x.len(), and if that returns zero, bool returns False. Otherwise bool returns True.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_How Special Methods Are Used]]></title>
    <url>%2F2018%2F04%2F22%2FC01-How-Special-Methods-Are-Used%2F</url>
    <content type="text"><![CDATA[First thing to know about special methods is that they are meant to be called by thePython interpreter,and not by you. You don’t write my_object.len(). You writelen(my_object) and, if my_object is an instance of a user-defined class, then Python calls the len instance method you implemented. Special method call is implicit. For example, the statement for i in x: actually causes the invocation of iter(x), which in turn may call x.iter() if that is available.Normally, your code should not have many direct calls to special methods. Unless you are doing a lotof metaprogramming, you should be implementing special methods more often than invoking themexplicitly. The only special method that is frequently called by user code directly is init, to invokethe initializer of the superclass in your own init implementation. If you need to invoke a special method, it is usually better to call the related built-in function (e.g., len,iter, str, etc). These built-ins call the corresponding special method, but often provide other services and—for built-in types—are faster than method calls.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fluent python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C29_mysqldump]]></title>
    <url>%2F2018%2F04%2F22%2FC29-mysqldump%2F</url>
    <content type="text"><![CDATA[备份 使用命令行实用程序mysqldump 转储所有数据库内容到某个外部文件。在进行常规备份前这个实用程序应该正常运行，以便能正确地备份转储文件。 可用命令行实用程序mysqlhotcopy 从一个数据库复制所有数据（并非所有数据库引擎都支持这个实用程序）。 可以使用MySQL的BACKUP TABLE 或SELECT INTO OUTFILE 转储所有数据到某个外部文件。这两条语句都接受将要创建的系统文件名，此系统文件必须不存在，否则会出错。数据可以用RESTORE TABLE 来复原。 为了保证所有数据被写到磁盘（包括索引数据），可能需要在进行备份前使用FLUSH TABLES 语句。 维护ANALYZE TABLE ，用来检查表键是否正确1ANALYZE TABLE orders; CHECK TABLE 用来针对许多问题对表进行检查。在MyISAM 表上还对索引进行检查。CHECK TABLE 支持一系列的用于MyISAM 表的方式。CHANGED 检查自最后一次检查以来改动过的表。EXTENDED 执行最彻底的检查，FAST 只检查未正常关闭的表，MEDIUM 检查所有被删除的链接并进行键检验，QUICK 只进行快速扫描。 如果MyISAM 表访问产生不正确和不一致的结果，可能需要用REPAIR TABLE 来修复相应的表。这条语句不应该经常使用，如果需要经常使用，可能会有更大的问题要解决。 如果从一个表中删除大量数据，应该使用OPTIMIZE TABLE 来收回所用的空间，从而优化表的性能 启动问题 mysqld –safe-mode 装载减去某些最佳配置的服务器； –verbose 显示全文本消息（为获得更详细的帮助消息与–help 联合使用）； –version 显示版本信息然后退出。 日志MySQL维护管理员依赖的一系列日志文件。主要的日志文件有以下几种。 错误日志。它包含启动和关闭问题以及任意关键错误的细节。此日志通常名为hostname.err ，位于data 目录中。此日志名可用–log-error 命令行选项更改。 查询日志。它记录所有MySQL活动，在诊断问题时非常有用。此日志文件可能会很快地变得非常大，因此不应该长期使用它。此日志通常名为hostname.log ，位于data 目录中。此名字可以用–log 命令行选项更改。 二进制日志。它记录更新过数据（或者可能更新过数据）的所有语句。此日志通常名为hostname-bin ，位于data 目录内。此名字可以用–log-bin 命令行选项更改。注意，这个日志文件是MySQL 5中添加的，以前的MySQL版本中使用的是更新日志。 缓慢查询日志。顾名思义，此日志记录执行缓慢的任何查询。这个日志在确定数据库何处需要优化很有用。此日志通常名为hostname-slow.log ，位于data 目录中。此名字可以用–log-slow-queries 命令行选项更改。 在使用日志时，可用FLUSH LOGS 语句来刷新和重新开始所有日志文件。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C28_用户管理]]></title>
    <url>%2F2018%2F04%2F22%2FC28-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[MySQL用户账号和信息存储在名为mysql 的MySQL数据库中。一般不需要直接访问mysql 数据库和表，但有时需要直接访问。需要直接访问它的时机之一是在需要获得所有用户账号列表时。12USE mysql;SELECT user FROM user; 创建用户账号一般来说CREATE USER 是最清楚和最简单的句子。此外，也可以通过直接插入行到user 表来增加用户，不过为安全起见，一般不建议这样做。MySQL用来存储用户账号信息的表（以及表模式等）极为重要，对它们的任何毁坏都可能严重地伤害到MySQL服务器。因此，相对于直接处理来说，最好是用标记和函数来处理这些表。123CREATE USER ben IDENTIFIED BY 'password';-- IDENTIFIED BY 指定的口令为纯文本，MySQL将在保存到user 表之前对其进行加密。为了作为散列值指-- 定口令，使用IDENTIFIED BY PASSWORD 。 RENAME USER1RENAME USER ben TO bforra; DROP USER1DROP USER bforra; 设置访问权限12SHOW GRANTS FOR bforra;--为看到赋予用户账号的权限 为设置权限，使用GRANT 语句。GRANT 要求你至少给出以下信息： 要授予的权限； 被授予访问权限的数据库或表； 用户名。每个GRANT 添加（或更新）用户的一个权限。MySQL读取所有授权，并根据它们确定权限。GRANT 的反操作为REVOKE 123456789GRANT SELECT ON crashcourse.* TO bforra;-- 此GRANT 允许用户在crashcourse.* （crashcourse 数据库的所有表）上使用SELECT 。通过只授予-- SELECT 访问权限，用户bforta 对crashcourse 数据库中的所有数据具有只读访问权限。GRANT SELECT, INSERT ON crashcourse.* TO beforta;-- 可通过列出各权限并用逗号分隔，将多条GRANT 语句串在一起REVOKE SELECT ON crashcourse.* FROM bfoora; -- 被撤销的访问权限必须存在,否则会报错。 GRANT 和REVOKE 可在几个层次上控制访问权限： 整个服务器，使用GRANT ALL 和REVOKE ALL ； 整个数据库，使用ON database.* ； 特定的表，使用ON database.table ； 特定的列； 特定的存储过程。###更改password新口令必须传递到Password() 函数进行加密。 1234SET PASSWORD FOR befora= PASSWORD('new password')SET PASSWORD = PASSWORD('new password')-- 设置自己的口令]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C27_CHARACTER SET, COLLATE]]></title>
    <url>%2F2018%2F04%2F22%2FC27-CHARACTER-SET-COLLATE%2F</url>
    <content type="text"><![CDATA[给表指定字符集和校对123456CREATE TABLE mytable( columnn1 INT, columnn2 VARCHAR(10)) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 如果指定CHARACTER SET 和COLLATE 两者，则使用这些值。 如果只指定CHARACTER SET ，则使用此字符集及其默认的校对（如SHOW CHARACTER SET 的结果中所示）。 如果既不指定CHARACTER SET ，也不指定COLLATE ，则使用数据库默认。 每个列设置1234567891011CREATE TABLE mytable( columnn1 INT, columnn2 VARCHAR(10), column3 VARCHAR(10) CHARACTER SET latin1 COLLATE latin1_general_ci) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci;SELECT * FROM customersORDER BY lastname, firstname COLLATE latin1_general_cs;-- 此SELECT 使用COLLATE 指定一个备用的校对顺序（在这个例子中，为区分大小写的校对）。这显然将-- 会影响到结果排序的次序。 COLLATE 还可以用于GROUP BY 、HAVING 、聚集函数、别名等。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C26_COMMIT ROLLBACK]]></title>
    <url>%2F2018%2F04%2F22%2FC26-COMMIT-ROLLBACK%2F</url>
    <content type="text"><![CDATA[MyISAM 和InnoDB 是两种最常使用的引擎。前者不支持明确的事务处理管理，而后者支持。 事务处理（transactionprocessing）可以用来维护数据库的完整性，它保证成批的MySQL操作要么完全执行，要么完全不执行。 事务（transaction） 指一组SQL语句； 回退（rollback） 指撤销指定SQL语句的过程； 提交（commit） 指将未存储的SQL语句结果写入数据库表； 保留点（savepoint） 指事务处理中设置的临时占位符（place-holder），你可以对它发布回退（与回退整个事务处理不同）。 标识事务的开始1START TRANSANCTION 使用ROLLBACKROLLBACK 只能在一个事务处理内使用（在执行一条START TRANSACTION 命令之后）。事务处理用来管理INSERT 、UPDATE 和DELETE 语句。你不能回退SELECT 语句。（这样做也没有什么意义。）你不能回退CREATE 或DROP 操作。事务处理块中可以使用这两条语句，但如果你执行回退，它们不会被撤销。123456789SELECT * FROM ordertotals;START TRANSACTION;DELETE FROM ordertotals;SELECT * FROM ordertotals;ROLLBACK;SELECT * FROM ordertotals-- 首先执行一条SELECT 以显示该表不为空。然后开始一个事务处理，用一条DELETE 语句删除ordertotals -- 中的所有行。另一条SELECT 语句验证ordertotals 确实为空。这时用一条ROLLBACK 语句回退START -- TRANSACTION 之后的所有语句，最后一条SELECT 语句显示该表不为空。 使用COMMIT一般的MySQL语句都是直接针对数据库表执行和编写的。这就是所谓的隐含提交（implicitcommit），即提交（写或保存）操作是自动进行的。 但是，在事务处理块中，提交不会隐含地进行。为进行明确的提交，使用COMMIT 语句。1234567-- 从系统中完全删除订单20010 。因为涉及更新两个数据库表orders 和orderItems ，所以使用事务处理-- 块来保证订单不被部分删除。最后的COMMIT 语句仅在不出错时写出更改。如果第一条DELETE 起作-- 用，但第二条失败，则DELETE 不会提交（实际上，它是被自动撤销的）。START TRANSACTION;DELETE FROM orderitems WHERE order_num = 20010;DELETE FROM orders WHERE order_num = 20010;COMMIT; 使用保留点简单的ROLLBACK 和COMMIT 语句就可以写入或撤销整个事务处理。但是，只是对简单的事务处理才能这样做，更复杂的事务处理可能需要部分提交或回退。 为了支持回退部分事务处理，必须能在事务处理块中合适的位置放置占位符。这样，如果需要回退，可以回退到某个占位符。123SAVEPOINT delete1;ROLLBACK TO delete1; 保留点在事务处理完成（执行一条ROLLBACK 或COMMIT ）后自动释放。自MySQL 5以来，也可以用RELEASE SAVEPOINT 明确地释放保留点。 更改默认的提交行为默认的MySQL行为是自动提交所有更改。换句话说，任何时候你执行一条MySQL语句，该语句实际上都是针对表执行的，而且所做的更改立即生效。为指示MySQL不自动提交更改:1SET autocommit = 0;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C25_TRIGGER]]></title>
    <url>%2F2018%2F04%2F22%2FC25-TRIGGER%2F</url>
    <content type="text"><![CDATA[触发器是MySQL响应以下任意语句而自动执行的一条MySQL语句（或位于BEGIN 和END 语句之间的一组语句）： DELETE； INSERT； UPDATE。 其他MySQL语句不支持触发器。只有表才支持触发器，视图不支持（临时表也不支持）。每个表每个事件每次只允许一个触发器。因此，每个表最多支持6个触发器（每条INSERT 、UPDATE 和DELETE 的之前和之后）。单一触发器不能与多个事件或多个表关联，所以，如果你需要一个对INSERT 和UPDATE 操作执行的触发器，则应该定义两个触发器。 触发器名必须在每个表中唯一，但不是在每个数据库中唯一。这表示同一数据库中的两个表可具有相同名字的触发器。这在其他每个数据库触发器名必须唯一的DBMS中是不允许的，最好是在数据库范围内使用唯一的触发器名。 如果BEFORE 触发器失败，则MySQL将不执行请求的操作。此外，如果BEFORE 触发器或语句本身失败，MySQL将不执行AFTER 触发器（如果有的话）。 在创建触发器时，需要给出4条信息： 唯一的触发器名； 触发器关联的表； 触发器应该响应的活动（DELETE 、INSERT 或UPDATE ）； 触发器何时执行（处理之前或之后）。 12345#对每个成功的插入，显示Product added 消息。CREATE TRIGGER newproduct AFTER INSERT ON productsFOR EACH ROW SELECT 'Product added';#删除触发器DROP TRIGGER newproduct; INSERT触发器 INSERT 触发器在INSERT 语句执行之前或之后执行。需要知道以下几点： 在INSERT 触发器代码内，可引用一个名为NEW 的虚拟表，访问被插入的行； 在BEFORE INSERT 触发器中，NEW 中的值也可以被更新（允许更改被插入的值）； 对于AUTO_INCREMENT 列，NEW 在INSERT 执行之前包含0 ，在INSERT 执行之后包含新的自动生成值。12345#MySQL生成一个新订单号并保存到order_num 中。触发器从NEW.order_num 取得这个值并返回它。此#触发器必须按照AFTER INSERT 执行，因为在BEFORE INSERT 语句执行之前，新order_num 还没有生成。#对于orders 的每次插入使用这个触发器将总是返回新的订单号。CREATE TRIGGER neworder AFTER INSERT ON ordersFOR EACH ROW SELECT NEW.order_num; DELETE触发器 DELETE 触发器在DELETE 语句执行之前或之后执行。需要知道以下两点： 在DELETE 触发器代码内，你可以引用一个名为OLD 的虚拟表，访问被删除的行； OLD 中的值全都是只读的，不能更新。123456CREATE TRIGGER deleteorder BEFORE DELETE ON ordersFOR EACH ROWBEGIN #使用BEGIN END 块的好处是触发器能容纳多条SQL语句 INSERT INTO archive_orders(order_num, order_date, cust_id) VALUES(OLD.order_num, OLD.order_date, OLD.cust_id);END; UPDATE触发器 UPDATE 触发器在UPDATE 语句执行之前或之后执行。需要知道以下几点： 在UPDATE 触发器代码中，你可以引用一个名为OLD 的虚拟表访问以前（UPDATE 语句前）的值，引用一个名为NEW 的虚拟表访问新更新的值； 在BEFORE UPDATE 触发器中，NEW 中的值可能也被更新（允许更改将要用于UPDATE 语句中的值）； OLD 中的值全都是只读的，不能更新。 12CREATE TRIGGER updatevendor BEFORE UPDATE ON vendorsFOR EACH ROW SET NEW.vend_state = Upper(NEW.vend_state); 与其他DBMS相比，MySQL5中支持的触发器相当初级。未来的MySQL版本中有一些改进和增强触发器支持的计划。 创建触发器可能需要特殊的安全访问权限，但是，触发器的执行是自动的。如果INSERT 、UPDATE 或DELETE 语句能够执行，则相关的触发器也能执行。 应该用触发器来保证数据的一致性（大小写、格式等）。在触发器中执行这种类型的处理的优点是它总是进行这种处理，而且是透明地进行，与客户机应用无关。 触发器的一种非常有意义的使用是创建审计跟踪。使用触发器，把更改（如果需要，甚至还有之前和之后的状态）记录到另一个表非常容易。 遗憾的是，MySQL触发器中不支持CALL 语句。这表示不能从触发器内调用存储过程。所需的存储过程代码需要复制到触发器内。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C29__getitem__, __setitem__]]></title>
    <url>%2F2018%2F04%2F22%2FC29-getitem-setitem%2F</url>
    <content type="text"><![CDATA[Indexing and SlicingIf defined in a class (or inherited by it), the getitem method is called automatically for instance-indexingoperations. When an instance X appears in an indexing expression like X[i], Python calls the getitemmethod inherited by the instance, passing X to the first argument and the index in brackets to the secondargument.123456789101112&gt;&gt;&gt; class Indexer:... def __getitem__(self, index):... return index ** 2...&gt;&gt;&gt; X = Indexer()&gt;&gt;&gt; X[2] # X[i] calls X.__getitem__(i)4&gt;&gt;&gt; for i in range(5):... print(X[i], end=' ') # Runs __getitem__(X, i) each time...0 1 4 9 16 slicingIn addition to indexing, getitem is also called for slice expressions. Slicing bounds are bundled up intoa slice object and passed to the list’s implementation of indexing. In fact, you can always pass a sliceobject manually—slice syntax is mostly syntactic sugar for indexing with a slice object.1234567891011121314151617181920212223242526272829&gt;&gt;&gt; L = [5, 6, 7, 8, 9]&gt;&gt;&gt; L[slice(2, 4)] # Slice with slice objects[7, 8]&gt;&gt;&gt; L[slice(1, None)][6, 7, 8, 9]&gt;&gt;&gt; L[slice(None, −1)][5, 6, 7, 8]&gt;&gt;&gt; L[slice(None, None, 2)][5, 7, 9]&gt;&gt;&gt; class Indexer:... data = [5, 6, 7, 8, 9]... def __getitem__(self, index): # Called for index or slice... print('getitem:', index)... return self.data[index] # Perform index or slice...&gt;&gt;&gt; X = Indexer()&gt;&gt;&gt; X[0] # Indexing sends __getitem__ an integergetitem: 05&gt;&gt;&gt; X[2:4] # Slicing sends __getitem__ a slice objectgetitem: slice(2, 4, None)[7, 8]&gt;&gt;&gt; X[1:]getitem: slice(1, None, None)[6, 7, 8, 9]&gt;&gt;&gt; X[:-1]getitem: slice(None, −1, None)[5, 6, 7, 8] Index IterationThe for statement works by repeatedly indexing a sequence from zero to higher indexes, until an out-of-bounds exception is detected. Because of that, getitem also turns out to be one way to overloaditeration in Python—if this method is defined, for loops call the class’s getitem each time through,with successively higher offsets.12345678910111213&gt;&gt;&gt; class stepper:... def __getitem__(self, i):... return self.data[i]...&gt;&gt;&gt; X = stepper() # X is a stepper object&gt;&gt;&gt; X.data = 'Spam'&gt;&gt;&gt;&gt;&gt;&gt; X[1] # Indexing calls __getitem__'p'&gt;&gt;&gt; for item in X: # for loops call __getitem__... print(item, end=' ') # for indexes items 0..N...S p a m Any class that supports for loops automatically supports all iteration contexts in Python. For example, thein membership test, list comprehensions, the map built-in, list and tuple assignments, and typeconstructors will also call getitem automatically, if it’s defined.123456789101112131415161718&gt;&gt;&gt; 'p' in X # All call __getitem__ tooTrue&gt;&gt;&gt; [c for c in X] # List comprehension['S', 'p', 'a', 'm']&gt;&gt;&gt; list(map(str.upper, X)) # map calls (use list() in 3.0)['S', 'P', 'A', 'M']&gt;&gt;&gt; (a, b, c, d) = X # Sequence assignments&gt;&gt;&gt; a, c, d('S', 'a', 'm')&gt;&gt;&gt; list(X), tuple(X), ''.join(X)(['S', 'p', 'a', 'm'], ('S', 'p', 'a', 'm'), 'Spam')&gt;&gt;&gt; X&lt;__main__.stepper object at 0x00A8D5D0&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>learning python 5th</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_named tuple, random.choice]]></title>
    <url>%2F2018%2F04%2F14%2FC01-named-tuple-random-choice%2F</url>
    <content type="text"><![CDATA[namedtuplePython中的tuples（元组）是经常用来表示简单的数据结构，但它只能通过下标来访问其中的数据，这导致代码难于阅读和维护。Python的collections模块包含一个namedtuple()函数，用来创建一个tuple的子类，其可以通过属性名称访问tuple中的元素。1234567891011121314&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; NetworkAddress = namedtuple('NetworkAddress',['hostname','port'])&gt;&gt;&gt; a = NetworkAddress('www.python.org',80)&gt;&gt;&gt; a.hostname'www.python.org'&gt;&gt;&gt; a.port80&gt;&gt;&gt; host, port = a&gt;&gt;&gt; len(a)2&gt;&gt;&gt; type(a)&lt;class '__main__.NetworkAddress'&gt;&gt;&gt;&gt; isinstance(a, tuple)True 支持所有普通tuple的操作，而且增加了通过使用属性名访问其中数据的功能。但使用namedtuple访问属性值时，不如通过类那样高效。 1234567class Stock(object): def __init__(self,name,shares,price): self.name = name self.shares = shares self.price = price#可定义为Stock=namedtuple('Stock',['name', 'shares', 'price']) 作为字典的替代，因为字典存储需要更多的内存空间。 如果你需要构建一个非常大的包含字典的数据结构，那么使用命名元组会更加高效。 但是需要注意的是，不像字典那样，一个命名元组是不可更改的。 需要改变属性的值，可以使用命名元组实例的 _replace() 方法 123456789101112&gt;&gt;&gt; s = Stock('ACME', 100, 123.45)&gt;&gt;&gt; sStock(name='ACME', shares=100, price=123.45)&gt;&gt;&gt; s.shares = 75Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attributes._replace(shares=75)&gt;&gt;&gt; sStock(name='ACME', shares=75, price=123.45)&gt;&gt;&gt; randomrandom.choice从一个序列中随机的抽取一个元素123456&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values)2&gt;&gt;&gt; random.choice(values)3 random.sample提取出N个不同元素的样本123456&gt;&gt;&gt; random.sample(values,4)[4, 5, 2, 1]&gt;&gt;&gt; random.sample(values,3)[4, 1, 2]&gt;&gt;&gt; random.sample(values,3)[5, 4, 2] random.shuffle123456&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 3, 5, 1, 2]&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 5, 2, 3, 1] random.randint生成随机整数123456&gt;&gt;&gt; random.randint(0,10)6&gt;&gt;&gt; random.randint(0,10)0&gt;&gt;&gt; random.randint(0,10)10 random.random()生成0到1范围内均匀分布的浮点数 random.getrandbits()获取N位随机位(二进制)的整数12&gt;&gt;&gt; random.getrandbits(22)343509 random.seed()random 模块使用 Mersenne Twister 算法来计算生成随机数。这是一个确定性算法， 但是你可以通过random.seed() 函数修改初始化种子123random.seed() # Seed based on system time or os.urandom()random.seed(12345) # Seed based on integer givenrandom.seed(b'bytedata') # Seed based on byte data]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>learning python 5th</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C24_CURSOR]]></title>
    <url>%2F2018%2F04%2F14%2FC24-CURSOR%2F</url>
    <content type="text"><![CDATA[需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在MySQL服务器上的数据库查询，它不是一条SELECT 语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。 MySQL游标只能用于存储过程（和函数）。使用游标涉及几个明确的步骤: 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的SELECT 语句。 一旦声明后，必须打开游标以供使用。这个过程用前面定义的SELECT 语句把数据实际检索出来。 对于填有数据的游标，根据需要取出（检索）各行。 在结束游标使用时，必须关闭游标。 DECLARE CURSOR12345678#这个存储过程处理完成后，游标就消失（因为它局限于存储过程）。CREATE PROCEDURE processorders()BEGIN DECLARE ordernumbers CURSOR # DECLARE 语句用来定义和命名游标 FOR SELECT ordernum FROM orders;END; 打开和关闭游标CLOSE 释放游标使用的所有内部内存和资源，因此在每个游标不再需要时都应该关闭。1234567891011121314CREATE PROCEDURE processorders()BEGIN -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Open the cursor OPEN ordernumbers; -- Close the cursor CLOSE ordernumbers;END; FETCH游标数据在一个游标被打开后，可以使用FETCH 语句分别访问它的每一行。FETCH 指定检索什么数据（所需的列），检索出来的数据存储在什么地方。它还向前移动游标中的内部行指针，使下一条FETCH 语句检索下一行（不重复读取同一行）。 123456789101112131415161718192021222324252627282930313233CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler -- 当SQLSTATE '02000' 出现时，SET done=1 。SQLSTATE '02000' 是一个未找到条件，当REPEAT 由于 -- 没有更多的行供循环而不能继续时，出现这个条件。 --- 句柄必须在游标之后定义 DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END 下面例子中，我们增加了另一个名为t 的变量（存储每个订单的合计）。此存储过程还在运行中创建了一个新表（如果它不存在的话），名为ordertotals 。这个表将保存存储过程生成的结果。FETCH 像以前一样取每个order_num ，然后用CALL 执行另一个存储过程（我们在前一章中创建）来计算每个订单的带税的合计（结果存储到t ）。最后，用INSERT 保存每个订单的订单号和合计。123456789101112131415161718192021222324252627282930313233343536373839404142CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL(8,2); -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Create a table to store the results CREATE TABLE IF NOT EXISTS ordertotals (order_num INT, total DECIMAL(8,2)); -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- Get the total for this order CALL ordertotal(o, 1, t); -- Insert order and total into ordertotals INSERT INTO ordertotals(order_num, total) VALUES(o, t); -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C23_存储过程]]></title>
    <url>%2F2018%2F04%2F14%2FC23-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MySQL称存储过程的执行为调用，因此MySQL执行存储过程的语句为CALL 。CALL 接受存储过程的名字以及需要传递给它的任意参数。1234CALL productpricing ( @pricelow, @pricehigh, @priceaverage);#执行名为productpricing 的存储过程，它计算并返回产品的最低、最高和平均价格 CREATR PROCEDURE 创建存储过程1234567CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END;#此存储过程名为productpricing ，用CREATE PROCEDURE productpricing() 语句定义。如果存储过程接受#参数，它们将在() 中列举出来。 更改命令行分隔符DELIMITER// 告诉命令行实用程序使用// 作为新的语句结束分隔符1234567DELIMITER // CREATE PROCEDURE productpricing() BEGIN SELECT Avg(prod_price) AS priceaverage FROM products; END // 删除存储过程1234DROP PROCEDURE productpricing;DROP PROCEDURE productpricing IF EXISTS;# 使用参数关键字OUT 指出相应的参数用来从存储过程传出一个值（返回给调用者）。MySQL支持IN （传递给存储过程）、OUT （从存储过程传出，如这里所用）和INOUT （对存储过程传入和传出）类型的参数。存储过程的代码位于BEGIN 和END 语句内，一系列SELECT 语句，用来检索值，然后保存到相应的变量（通过指定INTO 关键字）。 所有MySQL变量都必须以@ 开始。 OUT 参数123456789101112131415161718192021222324252627CREATE PROCEDURE productpricing( OUT pl DECIMAL(8,2), OUT ph DECIMAL(8,2), OUT pa DECIMAL(8,2))BEGIN SELECT Min(prod_price) INTO pl FROM products; SELECT Max(prod_price) INTO ph FROM products; SELECT Avg(prod_price) INTO pa FROM products;END;#调用productpricing， 指定三个变量,#在调用时，这条语句并不显示任何数据。它返回以后可以显示（或在其他处理中使用）的变量。CALL productpricing(@pricelow,@pricehigh,@priceaverage);#显示检索出的产品平均价格，可如下进行：SELECT @pricehigh, @pricelow, @priceaverage; IN, OUT 参数12345678910111213CREATE PROCEDURE ordertotal(IN onumber INT,OUT ototal DECIMAL(8,2))BEGINSELECT Sum(item_price * quantity)FROM orderitemsWHERE order_num = onumberINTO ototal;END;#调用ordertotal, 第一个参数为订单号，第二个参数为包含计算出来的合计的变量名CALL ordertotal(20005, @total); 建立智能存储过程`sql #注释 – – Name: ordertotal– Parameters: onumber = order number– taxable = 0 if not taxable, 1 if taxable– ototal = order total variable CREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2)) COMMENT ‘Obtain order total, optionally adding tax’ #将在SHOW PROCEDURE STATUS 的结果中显示 BEGIN # DECLARE 语句定义了两个局部变量 DECLARE total DECIMAL(8,2); DECLARE taxrate INT DEFAULT 6; SELECT Sum(item_price * quantity) FROM orderitems WHERE order_num = onumber INTO total; IF taxable THEN SELECT total + (total/100*taxrate) INTO total; END IF; SELECT total INTO ototal; END; CALL ordertotal(20005, 1, @total); 获得包括何时、由谁创建等详细信息的存储过程列表，使用SHOW PROCEDURE STATUS#使用LIKE 指定一个过滤模式SHOW PROCEDURE STATUS LIKE ‘ordertotal’;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C22_View]]></title>
    <url>%2F2018%2F04%2F14%2FC22-View%2F</url>
    <content type="text"><![CDATA[视图是虚拟的表。与包含数据的表不一样，视图只包含使用时动态检索数据的查询 在视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行SELECT 操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据。 重要的是知道视图仅仅是用来查看存储在别处的数据的一种设施。视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。视图的一些常见应用: 重用SQL语句。 简化复杂的SQL操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节。 使用表的组成部分而不是整个表。 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 视图的规则和限制: 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字）。 对于可以创建的视图数目没有限制。 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图。 ORDER BY 可以用在视图中，但如果从该视图检索数据SELECT 中也含有ORDER BY ，那么该视图中的 ORDER BY 将被覆盖。 视图不能索引，也不能有关联的触发器或默认值。 视图可以和表一起使用。例如，编写一条联结表和视图的SELECT 语句。 CREATE VIEW1234567891011121314151617181920212223242526CREATE VIEW productcustomers ASSELECT cust_name, cust_contact, prod_idFROM customers, orders, orderitemsWHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num;CREATE VIEW vendorlocations ASSELECT Concat(RTrim(vend_name), ' (', RTrim(vend_country), ')') AS vend_titleFROM vendorsORDER BY vend_name;#过滤CREATE VIEW customeremaillist ASSELECT cust_id, cust_name, cust_emailFROM customersWHERE cust_email IS NOT NULL;# 计算字段CREATE VIEW orderitemsexpanded ASSELECT order_num, prod_id, quantity, item_price, quantity*item_price AS expanded_priceFROM orderitems; 更新视图视图是可更新的（即，可以对它们使用INSERT 、UPDATE 和DELETE ）。更新一个视图将更新其基表（视图本身没有数据）。如果你对视图增加或删除行，实际上是对其基表增加或删除行。 并非所有视图都是可更新的。基本上可以说，如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。这实际上意味着，如果视图定义中有以下操作，则不能进行视图的更新： 分组（使用GROUP BY 和HAVING ）； 联结； 子查询； 并； 聚集函数（Min() 、Count() 、Sum() 等）； DISTINCT ； 导出（计算）列 一般，应该将视图用于检索（SELECT 语句）而不用于更新（INSERT 、UPDATE 和DELETE ）。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_ALTER TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-ALTER-TABLE%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223#给表添加一个列ALTER TABLE vendorsADD vend_phone CHAR(20);#删除一个列ALTER TABLE VendorsDROP COLUMN vend_phone;#定义外键ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_ordersFOREIGN KEY (order_num) REFERENCES orders (order_num);ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_products FOREIGN KEY (prod_id)REFERENCES products (prod_id);ALTER TABLE ordersADD CONSTRAINT fk_orders_customers FOREIGN KEY (cust_id)REFERENCES customers (cust_id);ALTER TABLE productsADD CONSTRAINT fk_products_vendorsFOREIGN KEY (vend_id) REFERENCES vendors (vend_id); 复杂的表结构更改一般需要手动删除过程，它涉及以下步骤： 用新的列布局创建一个新表； 使用INSERT SELECT 语句（关于这条语句的详细介绍，请参阅第19章）从旧表复制数据到新表。如果有必要，可使用转换函数和计算字段； 检验包含所需数据的新表； 重命名旧表（如果确定，可以删除它）； 用旧表原来的名字重命名新表； 根据需要，重新创建触发器、存储过程、索引和外键。 DROP TABLE12#删除表没有确认，也不能撤销DROP TABLE customers2; RENAME TABLE123RENAME TABLE backup_customers TO customers, backup_vendors TO vendors, backup_products TO products;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_CREATE TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-CREATE-TABLE%2F</url>
    <content type="text"><![CDATA[PRIMARY KEY如果主键使用单个列，则它的值必须唯一。如果使用多个列，则这些列的组合值必须唯一。 123456789101112PRIMARY KEY (vend_id) #用单个列作为主键#创建由多个列组成的主键,应该以逗号分隔的列表给出各列名CREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL , item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB; AUTO_INCREMENT每个表只允许一个AUTO_INCREMENT 列，而且它必须被索引（如，通过使它成为主键）。1234567cust_id int NOT NULL AUTO_INCREMENT,#AUTO_INCREMENT 告诉MySQL，本列每当增加一行时自动增量。每次执行一个INSERT 操作时，MySQL#自动对该列增量，给该列赋予下一个可用的值。这样给每个行分配一个唯一的cust_id ，从而可以用作主#键值。SELECT last_insert_id();#返回最后一个AUTO_INCREMENT 值 DEFAULT如果在插入行时没有给出值，MySQL允许指定此时使用的默认值。默认值用CREATE TABLE 语句的列定义中的DEFAULT关键字指定。 1quantity int NOT NULL DEFAULT 1, ENGINE引擎类型可以混用。混用引擎类型有一个大缺陷。外键（用于强制实施引用完整性）不能跨引擎，即使用一个引擎的表不能引用具有使用不同引擎的表的外键。 InnoDB 是一个可靠的事务处理引擎，它不支持全文本搜索； MEMORY 在功能等同于MyISAM ，但由于数据存储在内存中，速度很快（特别适合于临时 表 MyISAM 是一个性能极高的引擎，它支持全文本搜索，但不支持事务处理。 sqlCREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL DEFAULT 1, item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_UPDATE_DELETE]]></title>
    <url>%2F2018%2F04%2F14%2FC20-UPDATE-DELETE%2F</url>
    <content type="text"><![CDATA[12345678UPDATE customersSET cust_email = 'elmer@fudd.com', cust_name = 'The Fudds',WHERE cust_id = 10005;UPDATE customersSET cust_email = NULL #NULL 用来去除cust_email 列中的值。WHERE cust_id = 10005; IGNORE如果用UPDATE 语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个UPDATE 操作被取消（错误发生前更新的所有行被恢复到它们原来的值）。为即使是发生错误，也继续进行更新，可使用IGNORE 关键字 1UPDATE IGNORE customers ... DELETE12DELETE FROM customersWHERE cust_id = 10006; Attention 除非确实打算更新和删除每一行，否则绝对不要使用不带WHERE 子句的UPDATE 或DELETE 语句。 保证每个表都有主键（如果忘记这个内容，请参阅第15章），尽可能像WHERE 子句那样使用它（可以指 定各主键、多个值或值的范围）。 在对UPDATE 或DELETE 语句使用WHERE 子句前，应该先用SELECT 进行测试，保证它过滤的是正确的记 录，以防编写的WHERE 子句不正确。 使用强制实施引用完整性的数据库（关于这个内容，请参阅第15章），这样MySQL将不允许删除具有与 其他表相关联的数据的行。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_INSERT]]></title>
    <url>%2F2018%2F04%2F14%2FC19-INSERT%2F</url>
    <content type="text"><![CDATA[INSERT 是用来插入（或添加）行到数据库表的。插入可以用几种方式使用： 插入完整的行； 插入行的一部分； 插入多行； 插入某些查询的结果。 12345678910111213141516INSERT INTO CustomersVALUES(NULL, 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#插入一个新客户到customers 表。存储到每个表列中的数据在VALUES 子句中给出，对每个列必须提供一#个值。如果某个列没有值（如上面的cust_contact 和cust_email 列），应该使用NULL 值（假定表允许对#该列指定空值）。各个列必须以它们在表定义中出现的次序填充。第一列cust_id 也为NULL 。这是因为#每次插入一个新行时，该列由MySQL自动增量。你不想给出一个值（这是MySQL的工作），又不能省略#此列（如前所述，必须给出每个列），所以指定一个NULL 值（它被MySQL忽略，MySQL在这里插入下#一个可用的cust_id 值）。 12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country, cust_contact, cust_email)VALUES('Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#在插入行时，MySQL将用VALUES 列表中的相应值填入列表中的对应项。VALUES 中的第一个值对应于第#一个指定的列名。第二个值对应于第二个列名，如此等等。#因为提供了列名，VALUES 必须以其指定的次序匹配指定的列名，不一定按各个列出现在实际表中的次#序。其优点是，即使表的结构改变，此INSERT 语句仍然能正确工作。你会发现cust_id 的NULL 值是不必#要的，cust_id 列并没有出现在列表中，所以不需要任何值。 如果数据检索是最重要的（通常是这样），则你可以通过在INSERT 和INTO 之间添加关键字LOW_PRIORITY ，指示MySQL降低INSERT 语句的优先级, 也适用于UPDATE 和DELETE 语句1INSERT LOW_PRIORITY INTO insert 多条12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)VALUES( 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA' ), ( 'M. Martian', '42 Galaxy Way', 'New York', 'NY', '11213', 'USA' ); INSERT SELECT12345678910111213141516171819INSERT INTO customers(cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)SELECT cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_countryFROM custnew; 这个例子使用INSERT SELECT 从custnew 中将所有数据导入customers 。SELECT 语句从custnew 检索出要插入的值，而不是列出它们。SELECT 中列出的每个列对应于customers 表名后所跟的列表中的每个列。这条语句将插入多少行有赖于custnew 表中有多少行。 为简单起见，这个例子在INSERT 和SELECT 语句中使用了相同的列名。但是，不一定要求列名匹配。事实上，MySQL甚至不关心SELECT 返回的列名。它使用的是列的位置，因此SELECT 中的第一列（不管其列名）将用来填充表列中指定的第一个列，第二列将用来填充表列中指定的第二个列，如此等等。这对于从使用不同列名的表中导入数据是非常有用的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_布尔方式 搜索_boolean mode]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%B8%83%E5%B0%94%E6%96%B9%E5%BC%8F-%E6%90%9C%E7%B4%A2-boolean-mode%2F</url>
    <content type="text"><![CDATA[###MySQL支持全文本搜索的另外一种形式，称为布尔方式 （boolean mode）。以布尔方式，可以提供关于如下内容的细节： 要匹配的词； 要排斥的词（如果某行包含这个词，则不返回该行，即使它包含其他指定的词也是如此）； 排列提示（指定某些词比其他词更重要，更重要的词等级 表达式分组； 另外一些内容。 即使没有定义FULLTEXT 索引，也可以使用它。但这是一种非常缓慢的操作 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('heavy-rope*' IN BOOLEAN MODE);#匹配包含heavy 但不包含任意以rope 开始的词的行, -rope* 明确地指示MySQL排除包含rope* （任何以#rope 开始的词，包括ropes ）的行 + 包含，词必须存在 - 排除，词必须不出现 > 包含，而且增加等级值 &lt; 包含，且减少等级值 () 把词组成子表达式（允许这些子表达式作为一个组被包含、排除、排列等） ~ 取消一个词的排序值 * 词尾的通配符 ‘ ‘ 定义一个短语（与单个词的列表不一样，它匹配整个短语以便包含或排除这个短语） 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('+rabbit +bait'' IN BOOLEAN MODE);# 搜索匹配包含词rabbit 和bait 的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit bait' IN BOOLEAN MODE);#没有指定操作符，这个搜索匹配包含rabbit 和bait 中的至少一个词的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against(''rabbit bait'' IN BOOLEAN MODE);#搜索匹配短语rabbit bait 而不是匹配两个词rabbit 和bait 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('&gt;rabbit &lt;carrot' IN BOOLEAN MODE);#匹配rabbit 和carrot ，增加前者的等级，降低后者的等级 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('+safe +(&lt;combination)' IN BOOLEANMODE);#搜索匹配词safe 和combination ，降低后者的等级]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_全文本搜索]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%85%A8%E6%96%87%E6%9C%AC%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[启用一般在创建表时启用全文本搜索。CREATE TABLE 语句接受FULLTEXT 子句，它给出被索引列的一个逗号分隔的列表。123456789CREATE TABLE productnotes( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL, note_text text NULL , PRIMARY KEY(note_id), FULLTEXT(note_text)) ENGINE=MyISAM; 为了进行全文本搜索，MySQL根据子句FULLTEXT(note_text) 的指示对它进行索引。这里的FULLTEXT 索引单个列，如果需要也可以指定多个列。在定义之后，MySQL自动维护该索引。在增加、更新或删除行时，索引随之自动更新。 Match(), Against()在索引之后，使用两个函数Match() 和Against() 执行全文本搜索，其中Match() 指定被搜索的列，Against() 指定要使用的搜索表达式 Match(note_text) 指示MySQL针对指定的列进行搜索，Against(‘rabbit’) 指定词rabbit 作为搜索文本。由于有两行包含词rabbit ，这两个行被返回。123ELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit');]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_组合查询]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[多数SQL查询都只包含从一个或多个表中返回数据的单条SELECT 语句。MySQL也允许执行多个查询（多条SELECT 语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compoundquery） UNION可用UNION 操作符来组合数条SQL查询。利用UNION ，可给出多条SELECT 语句，将它们的结果组合成单个结果集。 UNION 必须由两条或两条以上的SELECT 语句组成，语句之间用关键字UNION 分隔（因此，如果组合4条SELECT 语句，将要使用3个UNION 关键字）。 UNION 中的每个查询必须包含相同的列、表达式或聚集函数（不过各个列不需要以相同的次序列出）。 列数据类型必须兼容：类型不必完全相同，但必须是DBMS可以隐含地转换的类型（例如，不同的数值类型或不同的日期类型）。 123456789101112SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5UNIONSELECT vend_id, prod_id, prod_priceFROM productsWHERE vend_id IN (1001,1002);SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5 OR vend_id IN (1001,1002); 包含或取消重复的行UNION 从查询结果集中自动去除了重复的行, 这是UNION 的默认行为，但是如果需要，可以改变它。事实上，如果想返回所有匹配行，可使用UNION ALL 而不是UNION 。如果确实需要每个条件的匹配行全部出现（包括重复行），则必须使用UNION ALL 而不是WHERE 。 排序SELECT 语句的输出用ORDER BY 子句排序。在用UNION 组合查询时，只能使用一条ORDER BY 子句，它必须出现在最后一条SELECT 语句之后。对于结果集，不存在用一种方式排序一部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条ORDER BY 子句。虽然ORDER BY 子句似乎只是最后一条SELECT 语句的组成部分，但实际上MySQL将用它来排序所有SELECT 语句返回的所有结果。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_带聚集函数的联结]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E5%B8%A6%E8%81%9A%E9%9B%86%E5%87%BD%E6%95%B0%E7%9A%84%E8%81%94%E7%BB%93%2F</url>
    <content type="text"><![CDATA[聚集函数用来汇总数据。虽然至今为止聚集函数的所有例子只是从单个表汇总数据，但这些函数也可以与联结一起使用。123456789#检索所有客户及每个客户所下的订单数，下面使用了COUNT() 函数的代码可完成此工作。此SELECT 语句#使用INNER JOIN 将customers 和orders 表互相关联。GROUP BY 子句按客户分组数据，因此，函数调用#COUNT(orders.order_num) 对每个客户的订单计数，将它作为num_ord 返回SELECT customers.cust_name, customers.cust_id, COUNT(orders.order_num) AS num_ordFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_idGROUP BY customers.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_INNER JOIN, LEFT OUTER JOIN]]></title>
    <url>%2F2018%2F04%2F14%2FC16-INNER-JOIN-LEFT-OUTER-JOIN%2F</url>
    <content type="text"><![CDATA[自联结某物品（其ID为DTNTR ）存在问题，因此想知道生产该物品的供应商生产的其他物品是否也存在这些问题。此查询要求首先找到生产ID为DTNTR 的物品的供应商，然后找出这个供应商生产的其他物品1234567891011121314SELECT prod_id, prod_nameFROM productsWHERE vend_id = (SELECT vend_id FROM products WHERE prod_id = 'DTNTR');#内部的SELECT语句做了一个简单的检索，返回生产ID为DTNTR 的物品供应商的vend_id 。该ID用于外部#查询的WHERE 子句中，以便检索出这个供应商生产的所有物品#使用联结的相同查询SELECT p1.prod_id, p1.prod_nameFROM products AS p1, products AS p2WHERE p1.vend_id = p2.vend_id AND p2.prod_id = 'DTNTR'; 自然联结123456SELECT c.*, o.order_num, o.order_date, oi.prod_id, oi.quantity, OI.item_priceFROM customers AS c, orders AS o, orderitems AS oiWHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'FB'; 外部联结联结包含了那些在相关表中没有关联行的行。这种类型的联结称为外部联结。 在使用OUTER JOIN 语法时，必须使用RIGHT 或LEFT 关键字指定包括其所有行的表（RIGHT 指出的是OUTER JOIN 右边的表，而LEFT 指出的是OUTER JOIN 左边的表）。 存在两种基本的外部联结形式：左外部联结和右外部联结。它们之间的唯一差别是所关联的表的顺序不同。换句话说，左外部联结可通过颠倒FROM 或WHERE 子句中表的顺序转换为右外部联结。因此，两种类型的外部联结可互换使用，而究竟使用哪一种纯粹是根据方便而定。 12345678910#一个简单的内部联结。它检索所有客户及其订单SELECT customers.cust_id, orders.order_numFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id;#检索所有客户，包括那些没有订单的客户， 使用LEFT OUTER JOIN 从FROM 子句的左边表#（customers 表）中选择所有行SELECT customers.cust_id, orders.order_numFROM customers LEFT OUTER JOIN orders ON customers.cust_id = orders.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_ocr.py_PIL-convert, point用法]]></title>
    <url>%2F2018%2F04%2F14%2FC07-ocr-py-PIL-convert-point%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对于彩色图像，不管其图像格式是PNG，还是BMP，或者JPG，在PIL中，使用Image模块的open()函数打开后，返回的图像对象的模式都是“RGB”。而对于灰度图像，不管其图像格式是PNG，还是BMP，或者JPG，打开后，其模式为“L”。对于PNG、BMP和JPG彩色图像格式之间的互相转换都可以通过Image模块的open()和save()函数来完成。具体说就是，在打开这些图像时，PIL会将它们解码为三通道的“RGB”图像。用户可以基于这个“RGB”图像，对其进行处理。处理完毕，使用函数save()，可以将处理结果保存成PNG、BMP和JPG中任何格式。这样也就完成了几种格式之间的转换。同理，其他格式的彩色图像也可以通过这种方式完成转换。当然，对于不同格式的灰度图像，也可通过类似途径完成，只是PIL解码后是模式为“L”的图像。 Image模块的convert()函数，用于不同模式图像之间的转换。PIL中有九种不同模式。分别为1，L，P，RGB，RGBA，CMYK，YCbCr，I，F。convert()函数有三种形式的定义，它们定义形式如下： im.convert(mode) ⇒ image im.convert(“P”, **options) ⇒ image im.convert(mode, matrix) ⇒ image使用不同的参数，将当前的图像转换为新的模式，并产生新的图像作为返回值。 模式“1”12345678910111213141516171819&gt;&gt;&gt;from PIL import Image &gt;&gt;&gt; lena =Image.open("D:\\Code\\Python\\test\\img\\lena.jpg") &gt;&gt;&gt; lena.mode 'RGB' &gt;&gt;&gt; lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt; lena_1 = lena.convert("1") &gt;&gt;&gt; lena_1.mode '1' &gt;&gt;&gt; lena_1.size (512, 512) &gt;&gt;&gt;lena_1.getpixel((0,0)) 255 &gt;&gt;&gt; lena_1.getpixel((10,10)) 255 &gt;&gt;&gt;lena_1.getpixel((10,120)) 0&gt;&gt;&gt;lena_1.getpixel((130,120)) 255 模式“L”模式“L”为灰色图像，它的每个像素用8个bit表示，0表示黑，255表示白，其他数字表示不同的灰度。在PIL中，从模式“RGB”转换为“L”模式是按照下面的公式转换的：L = R 299/1000 + G 587/1000+ B * 114/1000 123456789lena_L =lena.convert("L") &gt;&gt;&gt; lena_L.mode 'L' &gt;&gt;&gt; lena_L.size (512, 512) &gt;&gt;&gt;lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt;lena_L.getpixel((0,0)) 132 Pointpoint()方法通过一个函数或者查询表对图像中的像素点进行处理 定义1im.point(table)⇒ imageim.point(function) ⇒ image返回给定查找表对应的图像像素值的拷贝。变量table为图像的每个通道设置256个值。如果使用变量function，其对应函数应该有一个参数。这个函数将对每个像素值使用一次，结果表格将应用于图像的所有通道。 12345678910&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 = Image.open("D:\\Code\\Python\\test\\img\\test01.jpg") &gt;&gt;&gt;im_point_fun = im01.point(lambda i:i*1.2+10)&gt;&gt;&gt;im_point_fun.show()#图像im_point_fun比原图im01亮度增加了很多；因为lambda表达式中对原图的每个像素点的值都做了增#加操作。 定义2im.point(table,mode) ⇒ imageim.point(function, mode) ⇒ image与定义1一样，但是它会为输出图像指定一个新的模式。这个方法可以一步将模式为“L”和“P”的图像转换为模式为“1”的图像 1234567891011121314151617181920212223&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 =Image.open("D:\\Code\\Python\\test\\img\\test01.jpg")&gt;&gt;&gt;r,g,b = im01.split()&gt;&gt;&gt;r.mode'L'&gt;&gt;&gt; im= r.point(lambda x:x*1.3+5, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))19#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:1, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))1#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:x*0, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt; im.getpixel((0,0))0#图像im为全黑图；]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_form.py_PIL]]></title>
    <url>%2F2018%2F04%2F14%2FC07-form-py-PIL%2F</url>
    <content type="text"><![CDATA[PIL (Python Image Library) 是 Python 平台处理图片的事实标准，兼具强大的功能和简洁的 API。PIL 的更新速度很慢，而且存在一些难以配置的问题，不推荐使用；而 Pillow 库则是 PIL 的一个分支，维护和开发活跃，Pillow 兼容 PIL 的绝大多数语法，推荐使用。 新建一个 Image 类的实例PIL 的主要功能定义在 Image 类当中，而 Image 类定义在同名的 Image 模块当中。使用 PIL 的功能，一般都是从新建一个 Image 类的实例开始。新建 Image 类的实例有多种方法。你可以用 Image 模块的 open() 函数打开已有的图片档案，也可以处理其它的实例，或者从零开始构建一个实例。 12345678from PIL import ImagesourceFileName = "source.png"avatar = Image.open(sourceFileName)# open() 方法打开了 source.png 这个图像，构建了名为 avatar 的实例。如果打开失败，则会抛出IOError 异常。#使用 show() 方法来查看实例。注意，PIL 会将实例暂存为一个临时文件，而后打开它。avatar.show() 查看实例的属性Image 类的实例有 5 个属性，分别是： format: 以 string 返回图片档案的格式（JPG, PNG, BMP, None, etc.）；如果不是从打开文件得到的实例，则返回 None。 mode: 以 string 返回图片的模式（RGB, CMYK, etc.）；完整的列表参见 官方说明·图片模式列表 size: 以二元 tuple 返回图片档案的尺寸 (width, height) palette: 仅当 mode 为 P 时有效，返回 ImagePalette 示例 info: 以字典形式返回示例的信息 12print avatar.format, avatar.size, avatar.mode#图片的格式 PNG、图片的大小 (400, 400) 和图片的模式 RGB 图片 IO - 转换图片格式Image 模块提供了 open() 函数打开图片档案，Image 类则提供了 save() 方法将图片实例保存为图片档案。 save() 函数可以以特定的图片格式保存图片档案。比如 save(‘target.jpg’, ‘JPG’) 将会以 JPG 格式将图片示例保存为 target.jpg。不过，大多数时候也可以省略图片格式。此时，save() 方法会根据文件扩展名来选择相应的图片格式。 1234567891011import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: f, e = os.path.splitext(infile) outfile = f + ".jpg" if infile != outfile: try: Image.open(infile).save(outfile) except IOError: print "cannot convert", infile 制作缩略图Image 类的 thumbnail() 方法可以用来制作缩略图。它接受一个二元数组作为缩略图的尺寸，然后将示例缩小到指定尺寸。 12345678910111213import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: outfile = os.path.splitext(infile)[0] + ".thumbnail" if infile != outfile: try: im = Image.open(infile) x, y = im.size #用 im.size 获取原图档的尺寸 im.thumbnail((x//2, y//2)) im.save(outfile, "JPEG") except IOError: print "cannot create thumbnail for", infile 剪裁图档按照 horizon 和 vertic 两个变量切割当前目录下所有图片（包括子目录）。 123456789101112131415161718192021222324import Image as imgimport osimgTypes = ['.png','.jpg','.bmp']horizon = 8vertic = 1for root, dirs, files in os.walk('.'): for currentFile in files: crtFile = root + '\\' + currentFile if crtFile[crtFile.rindex('.'):].lower() in imgTypes: crtIm = img.open(crtFile) crtW, crtH = crtIm.size hStep = crtW // horizon vStep = crtH // vertic for i in range(vertic): for j in range(horizon): crtOutFileName = crtFile[:crtFile.rindex('.')] + \ '_' + str(i) + '_' + str(j)\ + crtFile[crtFile.rindex('.'):].lower() box = (j * hStep, i * vStep, (j + 1) * hStep, (i + 1) * vStep) cropped = crtIm.crop(box) cropped.save(crtOutFileName) 变形与粘贴transpose() 方法可以将图片左右颠倒、上下颠倒、旋转 90°、旋转 180° 或旋转 270°。paste() 方法则可以将一个 Image 示例粘贴到另一个 Image 示例上。 尝试将一张图片的左半部分截取下来，左右颠倒之后旋转 180°；将图片的右半边不作更改粘贴到左半部分；最后将修改过的左半部分粘贴到右半部分。 123456789101112131415161718192021222324252627282930313233from PIL import ImageimageFName = 'source.png'def iamge_transpose(image): ''' Input: a Image instance Output: a transposed Image instance Function: * switches the left and the right part of a Image instance * for the left part of the original instance, flips left and right\ and then make it upside down. ''' xsize, ysize = image.size xsizeLeft = xsize // 2 # while xsizeRight = xsize - xsizeLeft boxLeft = (0, 0, xsizeLeft, ysize) boxRight = (xsizeLeft, 0, xsize, ysize) boxLeftNew = (0, 0, xsize - xsizeLeft, ysize) boxRightNew = (xsize - xsizeLeft, 0, xsize, ysize) partLeft = image.crop(boxLeft).transpose(Image.FLIP_LEFT_RIGHT).\ transpose(Image.ROTATE_180) partRight = image.crop(boxRight) image.paste(partRight, boxLeftNew) image.paste(partLeft, boxRightNew) return imageavatar = Image.open(imageFName)avatar = iamge_transpose(avatar)avatar.show() 以 xsize 和 ysize 接收图片的宽和高，然后以 xsizeLeft 计算得到左半边图片的大小。需要注意的是，我们构建了四个元组，并命名为盒子。这个盒子用直角坐标的值在 image 的画布上框定了一个区域。注意，Image 模块以图片的左上角为直角坐标原点，向右为 x 轴正方向，向下为 y 轴正方向。元组中的前两个数，代表区域左上角的坐标值；后两个数代表区域右下角的坐标值。 接下来的代码相当易懂。我们先用 crop() 方法将原图 boxLeft 的区域（也就是原图的左半边）切下来，然后用 transpose() 方法先后进行左右颠倒和旋转 180° 的工作，并最周公将它保存在 partLeft 这个实例中。而 partRight 的操作更为简单。 函数的最后，我们用 paste() 方法，将前两步得到的 partLeft 和 partRight 分别粘贴到指定的区域；并最终返回 image 示例。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_login.py_glob]]></title>
    <url>%2F2018%2F04%2F14%2FC06-login-py-glob%2F</url>
    <content type="text"><![CDATA[基本用法 glob.glob（pathname), 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。 glob.iglob(pathname), 获取一个可编历对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而glob.iglob一次只获取一个匹配路径。 列出子目录中的文件，必须把子目录包含在模式中。 12345678import glob# get all py filesfiles = glob.glob('*.py')print files# Output# ['arg.py', 'g.py', 'shut.py', 'test.py'] 123456789101112131415import itertools as it, globdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements print filename# output#=========## test.txt# arg.py# g.py# shut.py# test.py 12345678910111213141516import itertools as it, glob, osdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements realpath = os.path.realpath(filename) print realpath# output#=========## C:\xxx\pyfunc\test.txt# C:\xxx\pyfunc\arg.py# C:\xxx\pyfunc\g.py# C:\xxx\pyfunc\shut.py# C:\xxx\pyfunc\test.py]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_search2.py_csv]]></title>
    <url>%2F2018%2F04%2F14%2FC05-search2-py-csv%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样：12345678Symbol,Price,Date,Time,Change,Volume"AA",39.48,"6/11/2007","9:36am",-0.18,181800"AIG",71.38,"6/11/2007","9:36am",-0.15,195500"AXP",62.58,"6/11/2007","9:36am",-0.46,935000"BA",98.31,"6/11/2007","9:36am",+0.12,104800"C",53.08,"6/11/2007","9:36am",-0.25,360900"CAT",78.29,"6/11/2007","9:36am",-0.23,225400 读取csv数据row[0] 访问Symbol， row[4] 访问Change1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f,delimiter='\t') headers = next(f_csv) for row in f_csv: # Process row ... row.Symbol 和 row.Change 代替下标访问123456789101112131415161718from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) #可能有一个包含非法标识符的列头行 Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ...#或者 row['Symbol']， row['Change']import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... csv.DictReader, csv.DictWriter用法12345678910111213141516171819202122232425FIELDS = ['Name', 'Sex', 'E-mail', 'Blog'] # DictWriter csv_file = open('test.csv', 'wb') writer = csv.DictWriter(csv_file, fieldnames=FIELDS) # write header writer.writerow(dict(zip(FIELDS, FIELDS))) d = &#123;&#125; d['Name'] = 'Qi' d['Sex'] = 'Male' d['E-mail'] = 'redice@163.com' d['Blog'] = 'http://www.redicecn.com' writer.writerow(d) csv_file.close() # DictReader # A easier way for skipping the header # Usually we need a extra flag variables for d in csv.DictReader(open('test.csv', 'rb')): print d # Output: # &#123;'Blog': 'http://www.redicecn.com', 'E-mail': 'redice@163.com', 'Name': 'Qi', 'Sex': 'Male'&#125; 写入csv数据123456789101112131415161718192021222324252627#创建writer对象headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)#创建DictWriter对象headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) csv数据进行类型转换123456789field_types = [ ('Price', float), ('Change', float), ('Volume', int) ]with open('stocks.csv') as f: for row in csv.DictReader(f): row.update((key, conversion(row[key])) for key, conversion in field_types) print(row)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_process_test.py_multiprocessing]]></title>
    <url>%2F2018%2F04%2F14%2FC04-process-test-py-multiprocessing%2F</url>
    <content type="text"><![CDATA[multiprocessing是Python的标准模块，它既可以用来编写多进程，也可以用来编写多线程。如果是多线程的话，用multiprocessing.dummy即可。 如果每个子进程执行需要消耗的时间非常短（执行+1操作等），这不必使用多进程，因为进程的启动关闭也会耗费资源。 当然使用多进程往往是用来处理CPU密集型（科学计算）的需求，如果是IO密集型（文件读取，爬虫等）则可以使用多线程去处理。 创建管理进程模块： Process（用于创建进程模块） Pool（用于创建管理进程池） Queue（用于进程通信，资源共享） Value，Array（用于进程通信，资源共享） Pipe（用于管道通信） Manager（用于资源共享） 同步子进程模块： Condition Event Lock RLock Semaphore]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_threaded_crawler.py_threading]]></title>
    <url>%2F2018%2F04%2F14%2FC04-threaded-crawler-py-threading%2F</url>
    <content type="text"><![CDATA[创建threading.Thread的子类来包装一个线程对象 threading.Thread类的使用： 1，在自己的线程类的init里调用threading.Thread.init(self, name = threadname) Threadname为线程的名字2， run()，通常需要重写，编写代码实现做需要的功能。 3，getName()，获得线程对象名称 4，setName()，设置线程对象名称 5，start()，启动线程 6，join([timeout])，等待另一线程结束后再运行。 7，setDaemon(bool)，设置子线程是否随主线程一起结束，必须在start()之前调用。默认为False。 8，isDaemon()，判断线程是否随主线程一起结束。 9，isAlive()，检查线程是否在运行中。 12345678910111213141516171819202122232425262728293031323334353637import threadingimport timeclass timer(threading.Thread): #The timer class is derived from the class threading.Thread def __init__(self, num, interval): threading.Thread.__init__(self) self.thread_num = num self.interval = interval self.thread_stop = False def run(self): #Overwrite run() method, put what you want the thread do here while not self.thread_stop: print 'Thread Object(%d), Time:%s\n' %(self.thread_num, time.ctime()) time.sleep(self.interval) def stop(self): self.thread_stop = True def test(): thread1 = timer(1, 1) thread2 = timer(2, 2) thread1.start() thread2.start() time.sleep(10) thread1.stop() thread2.stop() return if __name__ == '__main__': test()假设两个线程对象t1和t2都要对num=0进行增1运算，t1和t2都各对num修改10次，num的最终的结果应该为20。但是由于是多线程访问，有可能出现下面情况：在num=0时，t1取得num=0。系统此时把t1调度为”sleeping”状态，把t2转换为”running”状态，t2页获得num=0。然后t2对得到的值进行加1并赋给num，使得num=1。然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给num。这样，明明t1和t2都完成了1次加1工作，但结果仍然是num=1。上面的case描述了多线程情况下最常见的问题之一：数据共享。当多个线程都要去修改某一个共享数据的时候，我们需要对数据访问进行同步。 简单的同步 最简单的同步机制就是“锁”。锁对象由threading.RLock类创建。线程可以使用锁的acquire()方法获得锁，这样锁就进入“locked”状态。每次只有一个线程可以获得锁。如果当另一个线程试图获得这个锁的时候，就会被系统变为“blocked”状态，直到那个拥有锁的线程调用锁的release()方法来释放锁，这样锁就会进入“unlocked”状态。“blocked”状态的线程就会收到一个通知，并有权利获得锁。如果多个线程处于“blocked”状态，所有线程都会先解除“blocked”状态，然后系统选择一个线程来获得锁，其他的线程继续沉默（“blocked”）。 Python的threading module是在建立在thread module基础之上的一个module，在threading module中，暴露了许多thread module中的属性。在thread module中，python提供了用户级的线程同步工具“Lock”对象。而在threading module中，python又提供了Lock对象的变种: RLock对象。RLock对象内部维护着一个Lock对象，它是一种可重入的对象。对于Lock对象而言，如果一个线程连续两次进行acquire操作，那么由于第一次acquire之后没有release，第二次acquire将挂起线程。这会导致Lock对象永远不会release，使得线程死锁。RLock对象允许一个线程多次对其进行acquire操作，因为在其内部通过一个counter变量维护着线程acquire的次数。而且每一次的acquire操作必须有一个release操作与之对应，在所有的release操作完成之后，别的线程才能申请该RLock对象。 12345678910111213141516171819202122import threadingimport timemylock=threading.RLock()num=0f=file('test_result.txt','w')class dog(theading.Thread): def __init__(self,name): theading.Thread,__init__(self) self.name=name def run(self): global num while num&lt;=5: time.sleep(0.5) mylock.acquire() print "Th(%s) locked, number: %d\n" %(self.name, num) f.write(self.name+" "+str(num)+'\n') print "Th(%s) released, number: %d\n" %(self.name, num) mylock.release() num += 1 条件同步 锁只能提供最基本的同步。假如只在发生某些事件时才访问一个“临界区”，这时需要使用条件变量 Condition。Condition对象是对Lock对象的包装，在创建Condition对象时，其构造函数需要一个Lock对象作为参数，如果没有这个Lock对象参数，Condition将在内部自行创建一个Rlock对象。在Condition对象上，当然也可以调用acquire和release操作，因为内部的Lock对象本身就支持这些操作。但是Condition的价值在于其提供的wait和notify的语义。 条件变量是如何工作的呢？首先一个线程成功获得一个条件变量后，调用此条件变量的wait()方法会 导致这个线程释放这个锁，并进入“blocked”状态，直到另一个线程调用同一个条件变量的notify()方法来唤醒那个进入“blocked”状态的线程。如果调用这个条件变量的notifyAll()方法的话就会唤醒所有的在等待的线程。 如果程序或者线程永远处于“blocked”状态的话，就会发生死锁。所以如果使用了锁、条件变量等同步机制的话，一定要注意仔细检查，防止死锁情况的发生。对于可能产生异常的临界区要使用异常处理机制中的finally子句来保证释放锁。等待一个条件变量的线程必须用notify()方法显式的唤醒，否则就永远沉默。保证每一个wait()方法调用都有一个相对应的notify()调用，当然也可以调用notifyAll()方法以防万一。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import threadingimport time con = threading.Condition()x=0 class Producer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x&gt;0: con.wait() else: for i in range(5): x += 1 print "producing... "+str(x) con.notify() print x con.release() class Consumer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x==0: print "consumer wait" con.wait() else: for i in range(5): x -= 1 print "consuming... "+str(x) con.notify() print x con.release() def test(): print "start consumer\n" th1 = Consumer("consumer") print "start producer\n" th2 = Producer("producer") th1.start() th2.start() th1.join() th2.join() if __name__ == '__main__': test() 同步队列 Python中的Queue对象也提供了对线程同步的支持。使用Queue对象可以实现多个生产者和多个消费 者形成的FIFO的队列。 生产者将数据依次存入队列，消费者依次从队列中取出数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import threadingimport timeimport Queueimport random #producerclass Producer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): print "%s is producing %d to the queue!\n" %(self.getName(), i) self.data.put(i) time.sleep(random.randrange(10)/5) print "%s finished!\n" %(self.getName()) #consumerclass Consumer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): something = self.data.get() print "%s is consuming. %d in the queue is consumed!\n" %(self.getName(), something) time.sleep(random.randrange(10)) print "%s finished!\n" %(self.getName()) def test(): queue = Queue.Queue() th1 = Producer(queue) th2 = Consumer(queue) th1.start() th2.start() th1.join() th2.join() print "all threads terminate!\n" if __name__ == '__main__': test() join的用法123456789101112131415161718192021222324252627282930313233343536import threadingimport timedef context(tJoin): print 'in threadContext.' tJoin.start() # 将阻塞tContext直到threadJoin终止。 tJoin.join() # tJoin终止后继续执行。 print 'out threadContext.'def join(): print 'in threadJoin.' time.sleep(1) print 'out threadJoin.'tJoin = threading.Thread(target=join)tContext = threading.Thread(target=context, args=(tJoin,))tContext.start()#结果：in threadContext.in threadJoin.out threadJoin.out threadContext.&gt; tContext = threading.Thread(target=context, args=(tJoin,))&gt; tContext.start()# tJoin = threading.Thread(target=join)执行后，只是创建了一个线程对象tJoin，但并未启动该线程,这两# 句执行后，创建了另一个线程对象tContext并启动该线程（打印in threadContext.），同时将tJoin线程# 对象作为参数传给context函数，在context函数中，启动了tJoin这个线程，同时该线程又调用了join()函# 数（tJoin.join()），那tContext线程将等待tJoin这线程执行完成后，才能继续tContext线程后面的，所以# 先执行join()函数,tJoin线程执行结束后，继续执行tContext线程，于是打印输出了out threadContext.，# 于是就看到我们上面看到的输出结果，并且无论执行多少次，结果都是这个顺序。但如果将context()函# 数中tJoin.join()这句注释掉，再执行该程序，打印输出的结果顺序就不定了，因为此时这两线程就是并# 发执行的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_disk_cached.py_shutil]]></title>
    <url>%2F2018%2F04%2F14%2FC03-disk-cached-py-shutil%2F</url>
    <content type="text"><![CDATA[os模块提供了对目录或者文件的新建/删除/查看文件属性，还提供了对文件以及目录的路径操作。比如说：绝对路径，父目录…… 但是，os文件的操作还应该包含移动 复制 打包 压缩 解压等操作，这些os模块都没有提供，shutil则就是对os中文件操作的补充， shutil是shell utility的缩写。 shutil 模块shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 os 模块os.sep 可以取代操作系统特定的路径分隔符。windows下为 ‘\‘os.name 字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是 ‘posix’os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径os.getenv() 获取一个环境变量，如果没有返回noneos.putenv(key, value) 设置一个环境变量值os.listdir(path) 返回指定目录下的所有文件和目录名os.remove(path) 函数用来删除一个文件os.system(command) 函数用来运行shell命令os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 ‘\r\n’，Linux使用 ‘\n’ 而Mac使用 ‘\r’os.path.split(path) 函数返回一个路径的目录名和文件名os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录os.path.exists() 函数用来检验给出的路径是否真地存在os.curdir 返回当前目录 (‘.’)os.mkdir(path) 创建一个目录os.makedirs(path) 递归的创建目录os.chdir(dirname) 改变工作目录到dirnameos.path.getsize(name) 获得文件大小，如果name是目录返回0Los.path.abspath(name) 获得绝对路径os.path.normpath(path) 规范path字符串形式os.path.splitext() 分离文件名与扩展名os.path.join(path,name) 连接目录与文件名或目录os.path.basename(path) 返回文件名os.path.dirname(path) 返回文件路径os.walk(top,topdown=True,onerror=None) 遍历迭代目录os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。os.renames(old, new) 递归重命名文件夹或者文件。像rename()]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_mongo_cache.py_pymongo]]></title>
    <url>%2F2018%2F04%2F14%2FC03-mongo-cache-py-pymongo%2F</url>
    <content type="text"><![CDATA[基本用法 123456789101112131415161718192021222324252627282930313233import pymongoclient = pymongo.MongoClient(host='localhost', port=27017)db = client.cache# 指定为test 数据库collection = db.webpage#MongoDB 的每个数据库又包含了许多集合 Collection，也就类似与关系型数据库中的表condition = &#123;'name': 'Kevin'&#125;student = collection.find_one(condition)student['age'] = 25result = collection.update(condition, student)或者result = collection.update(condition, &#123;'$set': student&#125;)#只更新 student 字典内存在的字段，如果其原先还有其他字段则不会更新，也不会删除。而如果不用 $set 的话则会把之前的数据全部用 student 字典替换，如果原本存在其他的字段则会被删除。condition = &#123;'age': &#123;'$gt': 20&#125;&#125;result = collection.update_one(condition, &#123;'$inc': &#123;'age': 1&#125;&#125;)print(result)print(result.matched_count, result.modified_count)#指定查询条件为年龄大于 20，然后更新条件为 &#123;'$inc': &#123;'age': 1&#125;&#125;，也就是年龄加 1，执行之后会将第一条符合条件的数据年龄加 1# find_and_modify用法class MongoQueue: def pop(self): """Get an outstanding URL from the queue and set its status to processing. If the queue is empty a KeyError exception is raised. """ record = self.db.crawl_queue.find_and_modify( query=&#123;'status': self.OUTSTANDING&#125;, update=&#123;'$set': &#123;'status': self.PROCESSING, 'timestamp': datetime.now()&#125;&#125; ) if record: return record['_id'] else: self.repair() raise KeyError() mongodb存储二进制数据 BSON是一种类json的一种二进制形式的存储格式，简称Binary JSON，它和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。{“hello”:”world”} 这是一个BSON的例子，其中”hello”是key name，它一般是cstring类型，字节表示是cstring::= (byte) “/x00” ,其中表示零个或多个byte字节，/x00表示结束符;后面的”world”是value值，它的类型一般是、string,double,array,binarydata等类型。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pymongoimport bson.binaryfrom pymongo import MongoClientfrom cStringIO import StringIOdef insertFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image filename = 'F:/测试数据/hehe.jpg'.decode('utf-8') with open (filename,'rb') as myimage: content = StringIO(myimage.read()) coll.save(dict( content= bson.binary.Binary(content.getvalue()), filename = 'hehe.jpg' )) def getFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image data = coll.find_one(&#123;'filename':'hehe.jpg'&#125;) out = open('F:/测试数据/test4.jpg'.decode('utf-8'),'wb') out.write(data['content']) out.close()getFile()#Here is an example of how to save some data to MongoDB and then load it:&gt;&gt;&gt; url = 'http://example.webscraping.com/view/United-Kingdom-239'&gt;&gt;&gt; html = '...'&gt;&gt;&gt; db = client.cache &gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)ObjectId('5518c0644e0c87444c12a577')&gt;&gt;&gt; db.webpage.find_one(url=url)&#123;u'_id': ObjectId('5518c0644e0c87444c12a577'), u'html': u'...', u'url': u'http://example.webscraping.com/view/United-Kingdom-239'&#125;#A problem with the preceding example is that if we now insert another document #with the same URL, MongoDB will happily insert it for us, as follows:&gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)&gt;&gt;&gt; db.webpage.find(url=url).count()2#Now we have multiple records for the same URL when we are only interested in #storing the latest data. To prevent duplicates, we can set the ID to the URL and #perform upsert, which means updating the existing record if it exists; otherwise, #insert a new one, as shown here:&gt;&gt;&gt; self.db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': html&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': ''&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.find_one(&#123;'_id': url&#125;)&#123;u'_id': u'http://example.webscraping.com/view/United-Kingdom-239', u'html': u'...'&#125;#A timestamp index was created in the constructor. This is a handy MongoDB feature that will #automatically delete records in a specified number of seconds after the given timestamp. This means#that we do not need to manually check whether a record is still valid, as in the DiskCache class.&gt;&gt;&gt;expires=timedelta(days=30)&gt;&gt;&gt;self.db.webpage.create_index('timestamp',expireAfterSeconds=expires.total_seconds()) zlib压缩与解压缩1234567891011121314151617181920212223242526272829303132import zlibdef compress(infile, dst, level=9): infile = open(infile, 'rb') dst = open(dst, 'wb') compress = zlib.compressobj(level) data = infile.read(1024) while data: dst.write(compress.compress(data)) data = infile.read(1024) dst.write(compress.flush())def decompress(infile, dst): infile = open(infile, 'rb') dst = open(dst, 'wb') decompress = zlib.decompressobj() data = infile.read(1024) while data: dst.write(decompress.decompress(data)) data = infile.read(1024) dst.write(decompress.flush()) if __name__ == "__main__": infile = "1.txt" dst = "1.zlib.txt" compress(infile, dst) infile = "1.zlib.txt" dst = "2.txt" decompress(infile, dst) print "done~"compressobj返回一个压缩对象，用来压缩不能一下子读入内存的数据流。 level 从9到-1表示压缩等级，其中1最快但压缩度最小，9最慢但压缩度最大，0不压缩，默认是-1大约相当于与等级6，是一个压缩速度和压缩度适中的level。 picklepickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle模块只能在python中使用，python中几乎所有的数据类型（列表，字典，集合，类等）都可以用pickle来序列化 pickle.dump(obj, file[, protocol]) 序列化对象，并将结果数据流写入到文件对象中。参数protocol是序列化模式，默认值为0，表示以文本的形式序列化。protocol的值还可以是1或2，表示以二进制的形式序列化。 dumps()函数执行和dump() 函数相同的序列化，但是与dump不同的dumps并不将转换后的字符串写入文件，而是将所得到的转换后的数据以字符串的形式返回。 pickle.load(file) 反序列化对象。将文件中的数据解析为一个Python对象。 loads()函数执行和load()函数一样的反序列化。 loads接受一个字符串参数，将字符串解码成为python的数据类型，函数loads和dumps进行的是互逆的操作。12345678910111213141516171819202122232425import cPickle #序列化到文件obj = 123,"abcdedf",["ac",123],&#123;"key":"value","key1":"value1"&#125;print obj#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)#r 读写权限 r b 读写到二进制文件f = open(r"d:\a.txt","r ")cPickle.dump(obj,f)f.close()f = open(r"d:\a.txt")print cPickle.load(f)#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;) #序列化到内存（字符串格式保存），然后对象可以以任何方式处理如通过网络传输obj1 = cPickle.dumps(obj)print type(obj1)#输出：&lt;type 'str'&gt;print obj1#输出：python专用的存储格式obj2 = cPickle.loads(obj1)print type(obj2)#输出：&lt;type 'tuple'&gt;print obj2#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_downloader.py_datetime]]></title>
    <url>%2F2018%2F04%2F14%2FC03-downloader-py-datetime%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718from datetime import datetime&gt;&gt;&gt; today=datetime.today()&gt;&gt;&gt; today.date()datetime.date(2018, 4, 2)&lt;!-- more --&gt;&gt;&gt;&gt; today.time()datetime.time(4, 52, 20, 254000)&gt;&gt;&gt; next_month=today.replace(month=today.month+1)&gt;&gt;&gt; next_monthdatetime.datetime(2018, 5, 2, 4, 52, 20, 254000)&gt;&gt;&gt; next_month.strftime('%Y-%m-%d %H:%M:%S')'2018-05-02 04:52:20'&gt;&gt;&gt; import time&gt;&gt;&gt; t=time.mktime(today.timetuple()) #将一个datetime对象转成时间戳，很遗憾的是python并没直接提供这个方法，但是提供了一个timetuple()方法，它返回一个time.struct_time对象，通过它我们可以构造出时间戳&gt;&gt;&gt; t1522669940.0]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask 博客配置]]></title>
    <url>%2F2018%2F04%2F14%2Fflask-%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[服务器，域名，DNSvps: vultr debian7 domain: Freenom ，具体操作可见此处(需科学上网) DNS: cloudfare, 具体操作可见此处(需科学上网)，这篇文章介绍的是cloudXNS, 也适用于cloudfare 服务器环境设置 工具 git bash 或者 putty，git bash是windows上安装git时顺带安装的bash模拟器，后续还用到git,所以直接安装git 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#此处ip是vps的ip地址， 22是ssh连接的默认端口号， 如果没有修改过默认端口，这个-p可以省略不写了ssh -p 22 root@ip#输入vps的密码后就可以登录了，此时建议换一个密码[root@vultr:~]passwd#新建一个普通用户,名为user, ‘adduser’也可以新建用户， 但是创建的用户是三无用户，无家目录/无bash/无密码，所以此处用的是'useradd',记性不好，这两个命令常常搞错。。[root@vultr:~]# user#创建密码[root@vultr:~]passwd user#此时新用户创建完成， 还需把它加入sudoers列表中，让它可以使用root的命令[root@vultr:~]apt-get install sudo #debian7 上没有默认安装sudo命令[root@vultr:~]visudo...root ALL=(ALL) ALLuser ALL=(ALL) ALL #新增加这一行就Ok了#切换到user用户[root@vultr:~]su - user[user@vultr:~]$#这个博客是基于flask,python3.5写的，所以还得安装python3.5, debian7默认已安装的是python2.7[user@vultr:~]$sudo apt-get update[user@vultr:~]$sudo dpkg -l python* #可以看到最新的python是到3.1#安装依赖[user@vultr:~]$sudo apt-get install -y build-essential libncurses5-dev libncursesw5-dev libreadline6-dev libdb5.1-dev libgdbm-dev libsqlite3-dev libssl-dev libbz2-dev libexpat1-dev liblzma-dev zlib1g-dev#下载安装包[user@vultr:~]$wget --no-check-certificate https://www.python.org/ftp/python/3.5.1/Python-3.5.1.tgz#编译安装[user@vultr:~]$tar xzvf Python-3.5.1.tgz[user@vultr:~]$cd Python-3.5.1.tgz[user@vultr:~]$./configure --prefix=/usr/local/python3 #创建 Makefile 这个文件，prefix是安装路径[user@vultr:~]$make all #all会编译所有子模块，如sqlite3等[user@vultr:~]$sudo make install#成功安装后应该可以看到一下信息Installing collected packages: setuptools, pipSuccessfully installed pip-7.1.2 setuptools-18.2#按照提示升级pip3[user@vultr:~]$pip3 install --upgrade pip#为当前用户user添加路径[user@vultr:~]$vim ~/.bashrc添加 export PATH=$PATH:/usr/local/python3/bin[user@vultr:~]$source ~/.bashrc #将当前bashrc设置读入目前的bash环境中[user@vultr:~]$python3 --verison #当前python3的版本#为python3, pip3, virtualenv设置软链接, 这里没有把原来的python命令删除[user@vultr:~]$sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/virtualenv /usr/bin/virtualenv #/usr/bin 所以一般用户用的指令放的地方 安装数据库redis123456789101112131415161718#redis安装，方法一$wget http://download.redis.io/releases/redis-stable.tar.gz$ xzf redis-stable.tar.gz$cd redis-stable$make$make test #run the test after build$sudo make install #redis安装， 方法二#其实直接可以用apt-get安装啦$sudo apt-get install redis-server#安装完成后， redis会自动启动$ps -aux | grep redis #检查redis进程$netstat -nlt | grep 6379 #检查redis网络监听端口$sudo /etc/init.d/redis-server status # /etc/init.d/ #系统服务启动的接口放在这个目录下 安装postgresql123456789101112131415#添加postgrelsql apt repository$sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ wheezy-pgdg main" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'#导入密匙$wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -#安装$sudo apt-get update$sudo apt-get install postgresql postgresql-contrib#默认情况下安装PostgreSQL数据库服务器后，它将创建一个用户'postgres'，角色为'postgres'。 它还创建一个名称为“postgres”的系统帐户#登陆$sudo su - postgres$psql 上传文件到vps一般来说有两种方式， 一种是scp, 一种git clone 123456789101112#从本地复制目录到远程服务器， -r 递归复制整个目录$scp -r local_folder remote_username@remote_ip:remote_folder #此处remote_folder为dir=/home/&#123;user&#125;/blog，local_folder为your_blogname#如果是git clone的话，先注册个git hub 账号，然后创建一个repository,把本地文件push到这个库里$git init $git add .$git commit -m 'upload'$git configure --global$git remote add origin https://github...$git push -u origin master#感觉用git clone比较快点，而且可以随时随地上传，比较方便 安装虚拟环境把文件上传到vps后，就需要安装python3的虚拟环境了 12345678#在当前目录下，比如dir=/home/&#123;user&#125;/blog$ virtualenv venv #此时dir下生成一个venv文件夹$ source ./venv/bin/activate #激活虚拟环境（关闭虚拟环境 deactivate）#安装库$ pip install -r requirements.txt redis配置根据config.py设置的redis 密码，将/etc/redis/redis.conf 中设置成相应的密码 123$ sudo cp /etc/redis/redis.conf /usr/local/etc/redis.conf$ sudo vim /usr/local/etc/redis.conf #找到requirepass这一行，取消注释，并加上自己的password$ sudo redis-server /usr/local/etc/redis.conf #按照配置路径启动 postgresql配置根据config.py设置 ，SQLALCHEMY_DATABASE_URI = ‘postgresql://user:password@localhost/blog’，一个数据库名为’blog’, 用户是’user’，密码是’password’ 12345678$sudo -u postgres psqlpostgres= CREATE DATABASE blog;postgres= CREATE USER user WITH PASSWORD 'password';postgres= ALTER ROLE user SET client_encoding TO 'utf8';postgres= ALTER ROLE user SET default_transaction_isolation TO 'read committed';postgres= ALTER ROLE user SET timezone TO 'UTC';postgres= GRANT ALL PRIVILEGES ON DATABASE blog TO user;postgres= \q nginx安装和配置123456789101112131415161718192021222324252627#安装最新稳定版$echo deb http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list #更新库$echo deb-src http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list$wget http://nginx.org/keys/nginx_signing.key &amp;&amp; apt-key add nginx_signing.key #升级key$apt-get install nginx#配置文件$ sudo vim /etc/nginx/site-avaliable/blog添加以下内容server &#123; listen 80; server_name whistlestop.ml; # 域名， 用ip也行 location / &#123; proxy_pass http://127.0.0.1:8000; # gunicorn.py里的端口号 proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; $ cp /etc/nginx/sites-avaliable/blog /etc/nginx/sites-enabled/blog$ rm /etc/nginx/sites-enable/default #把原来的default文件删除#重启nginx$ sudo service nginx restart 启动博客1gunicorn --config gunicorn.py manager:app 后续要做的事 数据库备份 markdown预览功能 整个vps如何维护]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
</search>

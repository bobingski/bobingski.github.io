<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[C01_named tuple, random.choice]]></title>
    <url>%2F2018%2F04%2F14%2FC01-named-tuple-random-choice%2F</url>
    <content type="text"><![CDATA[namedtuplePython中的tuples（元组）是经常用来表示简单的数据结构，但它只能通过下标来访问其中的数据，这导致代码难于阅读和维护。Python的collections模块包含一个namedtuple()函数，用来创建一个tuple的子类，其可以通过属性名称访问tuple中的元素。1234567891011121314&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; NetworkAddress = namedtuple('NetworkAddress',['hostname','port'])&gt;&gt;&gt; a = NetworkAddress('www.python.org',80)&gt;&gt;&gt; a.hostname'www.python.org'&gt;&gt;&gt; a.port80&gt;&gt;&gt; host, port = a&gt;&gt;&gt; len(a)2&gt;&gt;&gt; type(a)&lt;class '__main__.NetworkAddress'&gt;&gt;&gt;&gt; isinstance(a, tuple)True 支持所有普通tuple的操作，而且增加了通过使用属性名访问其中数据的功能。但使用namedtuple访问属性值时，不如通过类那样高效。 1234567class Stock(object): def __init__(self,name,shares,price): self.name = name self.shares = shares self.price = price#可定义为Stock=namedtuple('Stock',['name', 'shares', 'price']) 作为字典的替代，因为字典存储需要更多的内存空间。 如果你需要构建一个非常大的包含字典的数据结构，那么使用命名元组会更加高效。 但是需要注意的是，不像字典那样，一个命名元组是不可更改的。 需要改变属性的值，可以使用命名元组实例的 _replace() 方法 123456789101112&gt;&gt;&gt; s = Stock('ACME', 100, 123.45)&gt;&gt;&gt; sStock(name='ACME', shares=100, price=123.45)&gt;&gt;&gt; s.shares = 75Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attributes._replace(shares=75)&gt;&gt;&gt; sStock(name='ACME', shares=75, price=123.45)&gt;&gt;&gt; randomrandom.choice从一个序列中随机的抽取一个元素123456&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values)2&gt;&gt;&gt; random.choice(values)3 random.sample提取出N个不同元素的样本123456&gt;&gt;&gt; random.sample(values,4)[4, 5, 2, 1]&gt;&gt;&gt; random.sample(values,3)[4, 1, 2]&gt;&gt;&gt; random.sample(values,3)[5, 4, 2] random.shuffle123456&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 3, 5, 1, 2]&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 5, 2, 3, 1] random.randint生成随机整数123456&gt;&gt;&gt; random.randint(0,10)6&gt;&gt;&gt; random.randint(0,10)0&gt;&gt;&gt; random.randint(0,10)10 random.random()生成0到1范围内均匀分布的浮点数 random.getrandbits()获取N位随机位(二进制)的整数12&gt;&gt;&gt; random.getrandbits(22)343509 random.seed()random 模块使用 Mersenne Twister 算法来计算生成随机数。这是一个确定性算法， 但是你可以通过random.seed() 函数修改初始化种子123random.seed() # Seed based on system time or os.urandom()random.seed(12345) # Seed based on integer givenrandom.seed(b'bytedata') # Seed based on byte data]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>learning python 5th</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C24_CURSOR]]></title>
    <url>%2F2018%2F04%2F14%2FC24-CURSOR%2F</url>
    <content type="text"><![CDATA[需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在MySQL服务器上的数据库查询，它不是一条SELECT 语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。 MySQL游标只能用于存储过程（和函数）。使用游标涉及几个明确的步骤: 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的SELECT 语句。 一旦声明后，必须打开游标以供使用。这个过程用前面定义的SELECT 语句把数据实际检索出来。 对于填有数据的游标，根据需要取出（检索）各行。 在结束游标使用时，必须关闭游标。 DECLARE CURSOR12345678#这个存储过程处理完成后，游标就消失（因为它局限于存储过程）。CREATE PROCEDURE processorders()BEGIN DECLARE ordernumbers CURSOR # DECLARE 语句用来定义和命名游标 FOR SELECT ordernum FROM orders;END; 打开和关闭游标CLOSE 释放游标使用的所有内部内存和资源，因此在每个游标不再需要时都应该关闭。1234567891011121314CREATE PROCEDURE processorders()BEGIN -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Open the cursor OPEN ordernumbers; -- Close the cursor CLOSE ordernumbers;END; FETCH游标数据在一个游标被打开后，可以使用FETCH 语句分别访问它的每一行。FETCH 指定检索什么数据（所需的列），检索出来的数据存储在什么地方。它还向前移动游标中的内部行指针，使下一条FETCH 语句检索下一行（不重复读取同一行）。 123456789101112131415161718192021222324252627282930313233CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler -- 当SQLSTATE '02000' 出现时，SET done=1 。SQLSTATE '02000' 是一个未找到条件，当REPEAT 由于 -- 没有更多的行供循环而不能继续时，出现这个条件。 --- 句柄必须在游标之后定义 DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END 下面例子中，我们增加了另一个名为t 的变量（存储每个订单的合计）。此存储过程还在运行中创建了一个新表（如果它不存在的话），名为ordertotals 。这个表将保存存储过程生成的结果。FETCH 像以前一样取每个order_num ，然后用CALL 执行另一个存储过程（我们在前一章中创建）来计算每个订单的带税的合计（结果存储到t ）。最后，用INSERT 保存每个订单的订单号和合计。123456789101112131415161718192021222324252627282930313233343536373839404142CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL(8,2); -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Create a table to store the results CREATE TABLE IF NOT EXISTS ordertotals (order_num INT, total DECIMAL(8,2)); -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- Get the total for this order CALL ordertotal(o, 1, t); -- Insert order and total into ordertotals INSERT INTO ordertotals(order_num, total) VALUES(o, t); -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C23_存储过程]]></title>
    <url>%2F2018%2F04%2F14%2FC23-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MySQL称存储过程的执行为调用，因此MySQL执行存储过程的语句为CALL 。CALL 接受存储过程的名字以及需要传递给它的任意参数。1234CALL productpricing ( @pricelow, @pricehigh, @priceaverage);#执行名为productpricing 的存储过程，它计算并返回产品的最低、最高和平均价格 CREATR PROCEDURE 创建存储过程1234567CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END;#此存储过程名为productpricing ，用CREATE PROCEDURE productpricing() 语句定义。如果存储过程接受#参数，它们将在() 中列举出来。 更改命令行分隔符DELIMITER// 告诉命令行实用程序使用// 作为新的语句结束分隔符1234567DELIMITER // CREATE PROCEDURE productpricing() BEGIN SELECT Avg(prod_price) AS priceaverage FROM products; END // 删除存储过程1234DROP PROCEDURE productpricing;DROP PROCEDURE productpricing IF EXISTS;# 使用参数关键字OUT 指出相应的参数用来从存储过程传出一个值（返回给调用者）。MySQL支持IN （传递给存储过程）、OUT （从存储过程传出，如这里所用）和INOUT （对存储过程传入和传出）类型的参数。存储过程的代码位于BEGIN 和END 语句内，一系列SELECT 语句，用来检索值，然后保存到相应的变量（通过指定INTO 关键字）。 所有MySQL变量都必须以@ 开始。 OUT 参数123456789101112131415161718192021222324252627CREATE PROCEDURE productpricing( OUT pl DECIMAL(8,2), OUT ph DECIMAL(8,2), OUT pa DECIMAL(8,2))BEGIN SELECT Min(prod_price) INTO pl FROM products; SELECT Max(prod_price) INTO ph FROM products; SELECT Avg(prod_price) INTO pa FROM products;END;#调用productpricing， 指定三个变量,#在调用时，这条语句并不显示任何数据。它返回以后可以显示（或在其他处理中使用）的变量。CALL productpricing(@pricelow,@pricehigh,@priceaverage);#显示检索出的产品平均价格，可如下进行：SELECT @pricehigh, @pricelow, @priceaverage; IN, OUT 参数12345678910111213CREATE PROCEDURE ordertotal(IN onumber INT,OUT ototal DECIMAL(8,2))BEGINSELECT Sum(item_price * quantity)FROM orderitemsWHERE order_num = onumberINTO ototal;END;#调用ordertotal, 第一个参数为订单号，第二个参数为包含计算出来的合计的变量名CALL ordertotal(20005, @total); 建立智能存储过程`sql #注释 – – Name: ordertotal– Parameters: onumber = order number– taxable = 0 if not taxable, 1 if taxable– ototal = order total variable CREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2)) COMMENT ‘Obtain order total, optionally adding tax’ #将在SHOW PROCEDURE STATUS 的结果中显示 BEGIN # DECLARE 语句定义了两个局部变量 DECLARE total DECIMAL(8,2); DECLARE taxrate INT DEFAULT 6; SELECT Sum(item_price * quantity) FROM orderitems WHERE order_num = onumber INTO total; IF taxable THEN SELECT total + (total/100*taxrate) INTO total; END IF; SELECT total INTO ototal; END; CALL ordertotal(20005, 1, @total); 获得包括何时、由谁创建等详细信息的存储过程列表，使用SHOW PROCEDURE STATUS#使用LIKE 指定一个过滤模式SHOW PROCEDURE STATUS LIKE ‘ordertotal’;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C22_View]]></title>
    <url>%2F2018%2F04%2F14%2FC22-View%2F</url>
    <content type="text"><![CDATA[视图是虚拟的表。与包含数据的表不一样，视图只包含使用时动态检索数据的查询 在视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行SELECT 操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据。 重要的是知道视图仅仅是用来查看存储在别处的数据的一种设施。视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。视图的一些常见应用: 重用SQL语句。 简化复杂的SQL操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节。 使用表的组成部分而不是整个表。 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 视图的规则和限制: 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字）。 对于可以创建的视图数目没有限制。 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图。 ORDER BY 可以用在视图中，但如果从该视图检索数据SELECT 中也含有ORDER BY ，那么该视图中的 ORDER BY 将被覆盖。 视图不能索引，也不能有关联的触发器或默认值。 视图可以和表一起使用。例如，编写一条联结表和视图的SELECT 语句。 CREATE VIEW1234567891011121314151617181920212223242526CREATE VIEW productcustomers ASSELECT cust_name, cust_contact, prod_idFROM customers, orders, orderitemsWHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num;CREATE VIEW vendorlocations ASSELECT Concat(RTrim(vend_name), ' (', RTrim(vend_country), ')') AS vend_titleFROM vendorsORDER BY vend_name;#过滤CREATE VIEW customeremaillist ASSELECT cust_id, cust_name, cust_emailFROM customersWHERE cust_email IS NOT NULL;# 计算字段CREATE VIEW orderitemsexpanded ASSELECT order_num, prod_id, quantity, item_price, quantity*item_price AS expanded_priceFROM orderitems; 更新视图视图是可更新的（即，可以对它们使用INSERT 、UPDATE 和DELETE ）。更新一个视图将更新其基表（视图本身没有数据）。如果你对视图增加或删除行，实际上是对其基表增加或删除行。 并非所有视图都是可更新的。基本上可以说，如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。这实际上意味着，如果视图定义中有以下操作，则不能进行视图的更新： 分组（使用GROUP BY 和HAVING ）； 联结； 子查询； 并； 聚集函数（Min() 、Count() 、Sum() 等）； DISTINCT ； 导出（计算）列 一般，应该将视图用于检索（SELECT 语句）而不用于更新（INSERT 、UPDATE 和DELETE ）。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_ALTER TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-ALTER-TABLE%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223#给表添加一个列ALTER TABLE vendorsADD vend_phone CHAR(20);#删除一个列ALTER TABLE VendorsDROP COLUMN vend_phone;#定义外键ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_ordersFOREIGN KEY (order_num) REFERENCES orders (order_num);ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_products FOREIGN KEY (prod_id)REFERENCES products (prod_id);ALTER TABLE ordersADD CONSTRAINT fk_orders_customers FOREIGN KEY (cust_id)REFERENCES customers (cust_id);ALTER TABLE productsADD CONSTRAINT fk_products_vendorsFOREIGN KEY (vend_id) REFERENCES vendors (vend_id); 复杂的表结构更改一般需要手动删除过程，它涉及以下步骤： 用新的列布局创建一个新表； 使用INSERT SELECT 语句（关于这条语句的详细介绍，请参阅第19章）从旧表复制数据到新表。如果有必要，可使用转换函数和计算字段； 检验包含所需数据的新表； 重命名旧表（如果确定，可以删除它）； 用旧表原来的名字重命名新表； 根据需要，重新创建触发器、存储过程、索引和外键。 DROP TABLE12#删除表没有确认，也不能撤销DROP TABLE customers2; RENAME TABLE123RENAME TABLE backup_customers TO customers, backup_vendors TO vendors, backup_products TO products;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_CREATE TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-CREATE-TABLE%2F</url>
    <content type="text"><![CDATA[PRIMARY KEY如果主键使用单个列，则它的值必须唯一。如果使用多个列，则这些列的组合值必须唯一。 123456789101112PRIMARY KEY (vend_id) #用单个列作为主键#创建由多个列组成的主键,应该以逗号分隔的列表给出各列名CREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL , item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB; AUTO_INCREMENT每个表只允许一个AUTO_INCREMENT 列，而且它必须被索引（如，通过使它成为主键）。1234567cust_id int NOT NULL AUTO_INCREMENT,#AUTO_INCREMENT 告诉MySQL，本列每当增加一行时自动增量。每次执行一个INSERT 操作时，MySQL#自动对该列增量，给该列赋予下一个可用的值。这样给每个行分配一个唯一的cust_id ，从而可以用作主#键值。SELECT last_insert_id();#返回最后一个AUTO_INCREMENT 值 DEFAULT如果在插入行时没有给出值，MySQL允许指定此时使用的默认值。默认值用CREATE TABLE 语句的列定义中的DEFAULT关键字指定。 1quantity int NOT NULL DEFAULT 1, ENGINE引擎类型可以混用。混用引擎类型有一个大缺陷。外键（用于强制实施引用完整性）不能跨引擎，即使用一个引擎的表不能引用具有使用不同引擎的表的外键。 InnoDB 是一个可靠的事务处理引擎，它不支持全文本搜索； MEMORY 在功能等同于MyISAM ，但由于数据存储在内存中，速度很快（特别适合于临时 表 MyISAM 是一个性能极高的引擎，它支持全文本搜索，但不支持事务处理。 sqlCREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL DEFAULT 1, item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_UPDATE_DELETE]]></title>
    <url>%2F2018%2F04%2F14%2FC20-UPDATE-DELETE%2F</url>
    <content type="text"><![CDATA[12345678UPDATE customersSET cust_email = 'elmer@fudd.com', cust_name = 'The Fudds',WHERE cust_id = 10005;UPDATE customersSET cust_email = NULL #NULL 用来去除cust_email 列中的值。WHERE cust_id = 10005; IGNORE如果用UPDATE 语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个UPDATE 操作被取消（错误发生前更新的所有行被恢复到它们原来的值）。为即使是发生错误，也继续进行更新，可使用IGNORE 关键字 1UPDATE IGNORE customers ... DELETE12DELETE FROM customersWHERE cust_id = 10006; Attention 除非确实打算更新和删除每一行，否则绝对不要使用不带WHERE 子句的UPDATE 或DELETE 语句。 保证每个表都有主键（如果忘记这个内容，请参阅第15章），尽可能像WHERE 子句那样使用它（可以指 定各主键、多个值或值的范围）。 在对UPDATE 或DELETE 语句使用WHERE 子句前，应该先用SELECT 进行测试，保证它过滤的是正确的记 录，以防编写的WHERE 子句不正确。 使用强制实施引用完整性的数据库（关于这个内容，请参阅第15章），这样MySQL将不允许删除具有与 其他表相关联的数据的行。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_INSERT]]></title>
    <url>%2F2018%2F04%2F14%2FC19-INSERT%2F</url>
    <content type="text"><![CDATA[INSERT 是用来插入（或添加）行到数据库表的。插入可以用几种方式使用： 插入完整的行； 插入行的一部分； 插入多行； 插入某些查询的结果。 12345678910111213141516INSERT INTO CustomersVALUES(NULL, 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#插入一个新客户到customers 表。存储到每个表列中的数据在VALUES 子句中给出，对每个列必须提供一#个值。如果某个列没有值（如上面的cust_contact 和cust_email 列），应该使用NULL 值（假定表允许对#该列指定空值）。各个列必须以它们在表定义中出现的次序填充。第一列cust_id 也为NULL 。这是因为#每次插入一个新行时，该列由MySQL自动增量。你不想给出一个值（这是MySQL的工作），又不能省略#此列（如前所述，必须给出每个列），所以指定一个NULL 值（它被MySQL忽略，MySQL在这里插入下#一个可用的cust_id 值）。 12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country, cust_contact, cust_email)VALUES('Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#在插入行时，MySQL将用VALUES 列表中的相应值填入列表中的对应项。VALUES 中的第一个值对应于第#一个指定的列名。第二个值对应于第二个列名，如此等等。#因为提供了列名，VALUES 必须以其指定的次序匹配指定的列名，不一定按各个列出现在实际表中的次#序。其优点是，即使表的结构改变，此INSERT 语句仍然能正确工作。你会发现cust_id 的NULL 值是不必#要的，cust_id 列并没有出现在列表中，所以不需要任何值。 如果数据检索是最重要的（通常是这样），则你可以通过在INSERT 和INTO 之间添加关键字LOW_PRIORITY ，指示MySQL降低INSERT 语句的优先级, 也适用于UPDATE 和DELETE 语句1INSERT LOW_PRIORITY INTO insert 多条12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)VALUES( 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA' ), ( 'M. Martian', '42 Galaxy Way', 'New York', 'NY', '11213', 'USA' ); INSERT SELECT12345678910111213141516171819INSERT INTO customers(cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)SELECT cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_countryFROM custnew; 这个例子使用INSERT SELECT 从custnew 中将所有数据导入customers 。SELECT 语句从custnew 检索出要插入的值，而不是列出它们。SELECT 中列出的每个列对应于customers 表名后所跟的列表中的每个列。这条语句将插入多少行有赖于custnew 表中有多少行。 为简单起见，这个例子在INSERT 和SELECT 语句中使用了相同的列名。但是，不一定要求列名匹配。事实上，MySQL甚至不关心SELECT 返回的列名。它使用的是列的位置，因此SELECT 中的第一列（不管其列名）将用来填充表列中指定的第一个列，第二列将用来填充表列中指定的第二个列，如此等等。这对于从使用不同列名的表中导入数据是非常有用的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_布尔方式 搜索_boolean mode]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%B8%83%E5%B0%94%E6%96%B9%E5%BC%8F-%E6%90%9C%E7%B4%A2-boolean-mode%2F</url>
    <content type="text"><![CDATA[###MySQL支持全文本搜索的另外一种形式，称为布尔方式 （boolean mode）。以布尔方式，可以提供关于如下内容的细节： 要匹配的词； 要排斥的词（如果某行包含这个词，则不返回该行，即使它包含其他指定的词也是如此）； 排列提示（指定某些词比其他词更重要，更重要的词等级 表达式分组； 另外一些内容。 即使没有定义FULLTEXT 索引，也可以使用它。但这是一种非常缓慢的操作 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('heavy-rope*' IN BOOLEAN MODE);#匹配包含heavy 但不包含任意以rope 开始的词的行, -rope* 明确地指示MySQL排除包含rope* （任何以#rope 开始的词，包括ropes ）的行 + 包含，词必须存在 - 排除，词必须不出现 > 包含，而且增加等级值 &lt; 包含，且减少等级值 () 把词组成子表达式（允许这些子表达式作为一个组被包含、排除、排列等） ~ 取消一个词的排序值 * 词尾的通配符 ‘ ‘ 定义一个短语（与单个词的列表不一样，它匹配整个短语以便包含或排除这个短语） 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('+rabbit +bait'' IN BOOLEAN MODE);# 搜索匹配包含词rabbit 和bait 的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit bait' IN BOOLEAN MODE);#没有指定操作符，这个搜索匹配包含rabbit 和bait 中的至少一个词的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against(''rabbit bait'' IN BOOLEAN MODE);#搜索匹配短语rabbit bait 而不是匹配两个词rabbit 和bait 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('&gt;rabbit &lt;carrot' IN BOOLEAN MODE);#匹配rabbit 和carrot ，增加前者的等级，降低后者的等级 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('+safe +(&lt;combination)' IN BOOLEANMODE);#搜索匹配词safe 和combination ，降低后者的等级]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_全文本搜索]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%85%A8%E6%96%87%E6%9C%AC%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[启用一般在创建表时启用全文本搜索。CREATE TABLE 语句接受FULLTEXT 子句，它给出被索引列的一个逗号分隔的列表。123456789CREATE TABLE productnotes( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL, note_text text NULL , PRIMARY KEY(note_id), FULLTEXT(note_text)) ENGINE=MyISAM; 为了进行全文本搜索，MySQL根据子句FULLTEXT(note_text) 的指示对它进行索引。这里的FULLTEXT 索引单个列，如果需要也可以指定多个列。在定义之后，MySQL自动维护该索引。在增加、更新或删除行时，索引随之自动更新。 Match(), Against()在索引之后，使用两个函数Match() 和Against() 执行全文本搜索，其中Match() 指定被搜索的列，Against() 指定要使用的搜索表达式 Match(note_text) 指示MySQL针对指定的列进行搜索，Against(‘rabbit’) 指定词rabbit 作为搜索文本。由于有两行包含词rabbit ，这两个行被返回。123ELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit');]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_组合查询]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[多数SQL查询都只包含从一个或多个表中返回数据的单条SELECT 语句。MySQL也允许执行多个查询（多条SELECT 语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compoundquery） UNION可用UNION 操作符来组合数条SQL查询。利用UNION ，可给出多条SELECT 语句，将它们的结果组合成单个结果集。 UNION 必须由两条或两条以上的SELECT 语句组成，语句之间用关键字UNION 分隔（因此，如果组合4条SELECT 语句，将要使用3个UNION 关键字）。 UNION 中的每个查询必须包含相同的列、表达式或聚集函数（不过各个列不需要以相同的次序列出）。 列数据类型必须兼容：类型不必完全相同，但必须是DBMS可以隐含地转换的类型（例如，不同的数值类型或不同的日期类型）。 123456789101112SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5UNIONSELECT vend_id, prod_id, prod_priceFROM productsWHERE vend_id IN (1001,1002);SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5 OR vend_id IN (1001,1002); 包含或取消重复的行UNION 从查询结果集中自动去除了重复的行, 这是UNION 的默认行为，但是如果需要，可以改变它。事实上，如果想返回所有匹配行，可使用UNION ALL 而不是UNION 。如果确实需要每个条件的匹配行全部出现（包括重复行），则必须使用UNION ALL 而不是WHERE 。 排序SELECT 语句的输出用ORDER BY 子句排序。在用UNION 组合查询时，只能使用一条ORDER BY 子句，它必须出现在最后一条SELECT 语句之后。对于结果集，不存在用一种方式排序一部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条ORDER BY 子句。虽然ORDER BY 子句似乎只是最后一条SELECT 语句的组成部分，但实际上MySQL将用它来排序所有SELECT 语句返回的所有结果。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_带聚集函数的联结]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E5%B8%A6%E8%81%9A%E9%9B%86%E5%87%BD%E6%95%B0%E7%9A%84%E8%81%94%E7%BB%93%2F</url>
    <content type="text"><![CDATA[聚集函数用来汇总数据。虽然至今为止聚集函数的所有例子只是从单个表汇总数据，但这些函数也可以与联结一起使用。123456789#检索所有客户及每个客户所下的订单数，下面使用了COUNT() 函数的代码可完成此工作。此SELECT 语句#使用INNER JOIN 将customers 和orders 表互相关联。GROUP BY 子句按客户分组数据，因此，函数调用#COUNT(orders.order_num) 对每个客户的订单计数，将它作为num_ord 返回SELECT customers.cust_name, customers.cust_id, COUNT(orders.order_num) AS num_ordFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_idGROUP BY customers.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_INNER JOIN, LEFT OUTER JOIN]]></title>
    <url>%2F2018%2F04%2F14%2FC16-INNER-JOIN-LEFT-OUTER-JOIN%2F</url>
    <content type="text"><![CDATA[自联结某物品（其ID为DTNTR ）存在问题，因此想知道生产该物品的供应商生产的其他物品是否也存在这些问题。此查询要求首先找到生产ID为DTNTR 的物品的供应商，然后找出这个供应商生产的其他物品1234567891011121314SELECT prod_id, prod_nameFROM productsWHERE vend_id = (SELECT vend_id FROM products WHERE prod_id = 'DTNTR');#内部的SELECT语句做了一个简单的检索，返回生产ID为DTNTR 的物品供应商的vend_id 。该ID用于外部#查询的WHERE 子句中，以便检索出这个供应商生产的所有物品#使用联结的相同查询SELECT p1.prod_id, p1.prod_nameFROM products AS p1, products AS p2WHERE p1.vend_id = p2.vend_id AND p2.prod_id = 'DTNTR'; 自然联结123456SELECT c.*, o.order_num, o.order_date, oi.prod_id, oi.quantity, OI.item_priceFROM customers AS c, orders AS o, orderitems AS oiWHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'FB'; 外部联结联结包含了那些在相关表中没有关联行的行。这种类型的联结称为外部联结。 在使用OUTER JOIN 语法时，必须使用RIGHT 或LEFT 关键字指定包括其所有行的表（RIGHT 指出的是OUTER JOIN 右边的表，而LEFT 指出的是OUTER JOIN 左边的表）。 存在两种基本的外部联结形式：左外部联结和右外部联结。它们之间的唯一差别是所关联的表的顺序不同。换句话说，左外部联结可通过颠倒FROM 或WHERE 子句中表的顺序转换为右外部联结。因此，两种类型的外部联结可互换使用，而究竟使用哪一种纯粹是根据方便而定。 12345678910#一个简单的内部联结。它检索所有客户及其订单SELECT customers.cust_id, orders.order_numFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id;#检索所有客户，包括那些没有订单的客户， 使用LEFT OUTER JOIN 从FROM 子句的左边表#（customers 表）中选择所有行SELECT customers.cust_id, orders.order_numFROM customers LEFT OUTER JOIN orders ON customers.cust_id = orders.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_ocr.py_PIL-convert, point用法]]></title>
    <url>%2F2018%2F04%2F14%2FC07-ocr-py-PIL-convert-point%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对于彩色图像，不管其图像格式是PNG，还是BMP，或者JPG，在PIL中，使用Image模块的open()函数打开后，返回的图像对象的模式都是“RGB”。而对于灰度图像，不管其图像格式是PNG，还是BMP，或者JPG，打开后，其模式为“L”。对于PNG、BMP和JPG彩色图像格式之间的互相转换都可以通过Image模块的open()和save()函数来完成。具体说就是，在打开这些图像时，PIL会将它们解码为三通道的“RGB”图像。用户可以基于这个“RGB”图像，对其进行处理。处理完毕，使用函数save()，可以将处理结果保存成PNG、BMP和JPG中任何格式。这样也就完成了几种格式之间的转换。同理，其他格式的彩色图像也可以通过这种方式完成转换。当然，对于不同格式的灰度图像，也可通过类似途径完成，只是PIL解码后是模式为“L”的图像。 Image模块的convert()函数，用于不同模式图像之间的转换。PIL中有九种不同模式。分别为1，L，P，RGB，RGBA，CMYK，YCbCr，I，F。convert()函数有三种形式的定义，它们定义形式如下： im.convert(mode) ⇒ image im.convert(“P”, **options) ⇒ image im.convert(mode, matrix) ⇒ image使用不同的参数，将当前的图像转换为新的模式，并产生新的图像作为返回值。 模式“1”12345678910111213141516171819&gt;&gt;&gt;from PIL import Image &gt;&gt;&gt; lena =Image.open("D:\\Code\\Python\\test\\img\\lena.jpg") &gt;&gt;&gt; lena.mode 'RGB' &gt;&gt;&gt; lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt; lena_1 = lena.convert("1") &gt;&gt;&gt; lena_1.mode '1' &gt;&gt;&gt; lena_1.size (512, 512) &gt;&gt;&gt;lena_1.getpixel((0,0)) 255 &gt;&gt;&gt; lena_1.getpixel((10,10)) 255 &gt;&gt;&gt;lena_1.getpixel((10,120)) 0&gt;&gt;&gt;lena_1.getpixel((130,120)) 255 模式“L”模式“L”为灰色图像，它的每个像素用8个bit表示，0表示黑，255表示白，其他数字表示不同的灰度。在PIL中，从模式“RGB”转换为“L”模式是按照下面的公式转换的：L = R 299/1000 + G 587/1000+ B * 114/1000 123456789lena_L =lena.convert("L") &gt;&gt;&gt; lena_L.mode 'L' &gt;&gt;&gt; lena_L.size (512, 512) &gt;&gt;&gt;lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt;lena_L.getpixel((0,0)) 132 Pointpoint()方法通过一个函数或者查询表对图像中的像素点进行处理 定义1im.point(table)⇒ imageim.point(function) ⇒ image返回给定查找表对应的图像像素值的拷贝。变量table为图像的每个通道设置256个值。如果使用变量function，其对应函数应该有一个参数。这个函数将对每个像素值使用一次，结果表格将应用于图像的所有通道。 12345678910&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 = Image.open("D:\\Code\\Python\\test\\img\\test01.jpg") &gt;&gt;&gt;im_point_fun = im01.point(lambda i:i*1.2+10)&gt;&gt;&gt;im_point_fun.show()#图像im_point_fun比原图im01亮度增加了很多；因为lambda表达式中对原图的每个像素点的值都做了增#加操作。 定义2im.point(table,mode) ⇒ imageim.point(function, mode) ⇒ image与定义1一样，但是它会为输出图像指定一个新的模式。这个方法可以一步将模式为“L”和“P”的图像转换为模式为“1”的图像 1234567891011121314151617181920212223&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 =Image.open("D:\\Code\\Python\\test\\img\\test01.jpg")&gt;&gt;&gt;r,g,b = im01.split()&gt;&gt;&gt;r.mode'L'&gt;&gt;&gt; im= r.point(lambda x:x*1.3+5, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))19#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:1, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))1#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:x*0, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt; im.getpixel((0,0))0#图像im为全黑图；]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_form.py_PIL]]></title>
    <url>%2F2018%2F04%2F14%2FC07-form-py-PIL%2F</url>
    <content type="text"><![CDATA[PIL (Python Image Library) 是 Python 平台处理图片的事实标准，兼具强大的功能和简洁的 API。PIL 的更新速度很慢，而且存在一些难以配置的问题，不推荐使用；而 Pillow 库则是 PIL 的一个分支，维护和开发活跃，Pillow 兼容 PIL 的绝大多数语法，推荐使用。 新建一个 Image 类的实例PIL 的主要功能定义在 Image 类当中，而 Image 类定义在同名的 Image 模块当中。使用 PIL 的功能，一般都是从新建一个 Image 类的实例开始。新建 Image 类的实例有多种方法。你可以用 Image 模块的 open() 函数打开已有的图片档案，也可以处理其它的实例，或者从零开始构建一个实例。 12345678from PIL import ImagesourceFileName = "source.png"avatar = Image.open(sourceFileName)# open() 方法打开了 source.png 这个图像，构建了名为 avatar 的实例。如果打开失败，则会抛出IOError 异常。#使用 show() 方法来查看实例。注意，PIL 会将实例暂存为一个临时文件，而后打开它。avatar.show() 查看实例的属性Image 类的实例有 5 个属性，分别是： format: 以 string 返回图片档案的格式（JPG, PNG, BMP, None, etc.）；如果不是从打开文件得到的实例，则返回 None。 mode: 以 string 返回图片的模式（RGB, CMYK, etc.）；完整的列表参见 官方说明·图片模式列表 size: 以二元 tuple 返回图片档案的尺寸 (width, height) palette: 仅当 mode 为 P 时有效，返回 ImagePalette 示例 info: 以字典形式返回示例的信息 12print avatar.format, avatar.size, avatar.mode#图片的格式 PNG、图片的大小 (400, 400) 和图片的模式 RGB 图片 IO - 转换图片格式Image 模块提供了 open() 函数打开图片档案，Image 类则提供了 save() 方法将图片实例保存为图片档案。 save() 函数可以以特定的图片格式保存图片档案。比如 save(‘target.jpg’, ‘JPG’) 将会以 JPG 格式将图片示例保存为 target.jpg。不过，大多数时候也可以省略图片格式。此时，save() 方法会根据文件扩展名来选择相应的图片格式。 1234567891011import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: f, e = os.path.splitext(infile) outfile = f + ".jpg" if infile != outfile: try: Image.open(infile).save(outfile) except IOError: print "cannot convert", infile 制作缩略图Image 类的 thumbnail() 方法可以用来制作缩略图。它接受一个二元数组作为缩略图的尺寸，然后将示例缩小到指定尺寸。 12345678910111213import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: outfile = os.path.splitext(infile)[0] + ".thumbnail" if infile != outfile: try: im = Image.open(infile) x, y = im.size #用 im.size 获取原图档的尺寸 im.thumbnail((x//2, y//2)) im.save(outfile, "JPEG") except IOError: print "cannot create thumbnail for", infile 剪裁图档按照 horizon 和 vertic 两个变量切割当前目录下所有图片（包括子目录）。 123456789101112131415161718192021222324import Image as imgimport osimgTypes = ['.png','.jpg','.bmp']horizon = 8vertic = 1for root, dirs, files in os.walk('.'): for currentFile in files: crtFile = root + '\\' + currentFile if crtFile[crtFile.rindex('.'):].lower() in imgTypes: crtIm = img.open(crtFile) crtW, crtH = crtIm.size hStep = crtW // horizon vStep = crtH // vertic for i in range(vertic): for j in range(horizon): crtOutFileName = crtFile[:crtFile.rindex('.')] + \ '_' + str(i) + '_' + str(j)\ + crtFile[crtFile.rindex('.'):].lower() box = (j * hStep, i * vStep, (j + 1) * hStep, (i + 1) * vStep) cropped = crtIm.crop(box) cropped.save(crtOutFileName) 变形与粘贴transpose() 方法可以将图片左右颠倒、上下颠倒、旋转 90°、旋转 180° 或旋转 270°。paste() 方法则可以将一个 Image 示例粘贴到另一个 Image 示例上。 尝试将一张图片的左半部分截取下来，左右颠倒之后旋转 180°；将图片的右半边不作更改粘贴到左半部分；最后将修改过的左半部分粘贴到右半部分。 123456789101112131415161718192021222324252627282930313233from PIL import ImageimageFName = 'source.png'def iamge_transpose(image): ''' Input: a Image instance Output: a transposed Image instance Function: * switches the left and the right part of a Image instance * for the left part of the original instance, flips left and right\ and then make it upside down. ''' xsize, ysize = image.size xsizeLeft = xsize // 2 # while xsizeRight = xsize - xsizeLeft boxLeft = (0, 0, xsizeLeft, ysize) boxRight = (xsizeLeft, 0, xsize, ysize) boxLeftNew = (0, 0, xsize - xsizeLeft, ysize) boxRightNew = (xsize - xsizeLeft, 0, xsize, ysize) partLeft = image.crop(boxLeft).transpose(Image.FLIP_LEFT_RIGHT).\ transpose(Image.ROTATE_180) partRight = image.crop(boxRight) image.paste(partRight, boxLeftNew) image.paste(partLeft, boxRightNew) return imageavatar = Image.open(imageFName)avatar = iamge_transpose(avatar)avatar.show() 以 xsize 和 ysize 接收图片的宽和高，然后以 xsizeLeft 计算得到左半边图片的大小。需要注意的是，我们构建了四个元组，并命名为盒子。这个盒子用直角坐标的值在 image 的画布上框定了一个区域。注意，Image 模块以图片的左上角为直角坐标原点，向右为 x 轴正方向，向下为 y 轴正方向。元组中的前两个数，代表区域左上角的坐标值；后两个数代表区域右下角的坐标值。 接下来的代码相当易懂。我们先用 crop() 方法将原图 boxLeft 的区域（也就是原图的左半边）切下来，然后用 transpose() 方法先后进行左右颠倒和旋转 180° 的工作，并最周公将它保存在 partLeft 这个实例中。而 partRight 的操作更为简单。 函数的最后，我们用 paste() 方法，将前两步得到的 partLeft 和 partRight 分别粘贴到指定的区域；并最终返回 image 示例。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_login.py_glob]]></title>
    <url>%2F2018%2F04%2F14%2FC06-login-py-glob%2F</url>
    <content type="text"><![CDATA[基本用法 glob.glob（pathname), 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。 glob.iglob(pathname), 获取一个可编历对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而glob.iglob一次只获取一个匹配路径。 列出子目录中的文件，必须把子目录包含在模式中。 12345678import glob# get all py filesfiles = glob.glob('*.py')print files# Output# ['arg.py', 'g.py', 'shut.py', 'test.py'] 123456789101112131415import itertools as it, globdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements print filename# output#=========## test.txt# arg.py# g.py# shut.py# test.py 12345678910111213141516import itertools as it, glob, osdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements realpath = os.path.realpath(filename) print realpath# output#=========## C:\xxx\pyfunc\test.txt# C:\xxx\pyfunc\arg.py# C:\xxx\pyfunc\g.py# C:\xxx\pyfunc\shut.py# C:\xxx\pyfunc\test.py]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_search2.py_csv]]></title>
    <url>%2F2018%2F04%2F14%2FC05-search2-py-csv%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样：12345678Symbol,Price,Date,Time,Change,Volume"AA",39.48,"6/11/2007","9:36am",-0.18,181800"AIG",71.38,"6/11/2007","9:36am",-0.15,195500"AXP",62.58,"6/11/2007","9:36am",-0.46,935000"BA",98.31,"6/11/2007","9:36am",+0.12,104800"C",53.08,"6/11/2007","9:36am",-0.25,360900"CAT",78.29,"6/11/2007","9:36am",-0.23,225400 读取csv数据row[0] 访问Symbol， row[4] 访问Change1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f,delimiter='\t') headers = next(f_csv) for row in f_csv: # Process row ... row.Symbol 和 row.Change 代替下标访问123456789101112131415161718from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) #可能有一个包含非法标识符的列头行 Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ...#或者 row['Symbol']， row['Change']import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... csv.DictReader, csv.DictWriter用法12345678910111213141516171819202122232425FIELDS = ['Name', 'Sex', 'E-mail', 'Blog'] # DictWriter csv_file = open('test.csv', 'wb') writer = csv.DictWriter(csv_file, fieldnames=FIELDS) # write header writer.writerow(dict(zip(FIELDS, FIELDS))) d = &#123;&#125; d['Name'] = 'Qi' d['Sex'] = 'Male' d['E-mail'] = 'redice@163.com' d['Blog'] = 'http://www.redicecn.com' writer.writerow(d) csv_file.close() # DictReader # A easier way for skipping the header # Usually we need a extra flag variables for d in csv.DictReader(open('test.csv', 'rb')): print d # Output: # &#123;'Blog': 'http://www.redicecn.com', 'E-mail': 'redice@163.com', 'Name': 'Qi', 'Sex': 'Male'&#125; 写入csv数据123456789101112131415161718192021222324252627#创建writer对象headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)#创建DictWriter对象headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) csv数据进行类型转换123456789field_types = [ ('Price', float), ('Change', float), ('Volume', int) ]with open('stocks.csv') as f: for row in csv.DictReader(f): row.update((key, conversion(row[key])) for key, conversion in field_types) print(row)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_process_test.py_multiprocessing]]></title>
    <url>%2F2018%2F04%2F14%2FC04-process-test-py-multiprocessing%2F</url>
    <content type="text"><![CDATA[multiprocessing是Python的标准模块，它既可以用来编写多进程，也可以用来编写多线程。如果是多线程的话，用multiprocessing.dummy即可。 如果每个子进程执行需要消耗的时间非常短（执行+1操作等），这不必使用多进程，因为进程的启动关闭也会耗费资源。 当然使用多进程往往是用来处理CPU密集型（科学计算）的需求，如果是IO密集型（文件读取，爬虫等）则可以使用多线程去处理。 创建管理进程模块： Process（用于创建进程模块） Pool（用于创建管理进程池） Queue（用于进程通信，资源共享） Value，Array（用于进程通信，资源共享） Pipe（用于管道通信） Manager（用于资源共享） 同步子进程模块： Condition Event Lock RLock Semaphore]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_threaded_crawler.py_threading]]></title>
    <url>%2F2018%2F04%2F14%2FC04-threaded-crawler-py-threading%2F</url>
    <content type="text"><![CDATA[创建threading.Thread的子类来包装一个线程对象 threading.Thread类的使用： 1，在自己的线程类的init里调用threading.Thread.init(self, name = threadname) Threadname为线程的名字2， run()，通常需要重写，编写代码实现做需要的功能。 3，getName()，获得线程对象名称 4，setName()，设置线程对象名称 5，start()，启动线程 6，join([timeout])，等待另一线程结束后再运行。 7，setDaemon(bool)，设置子线程是否随主线程一起结束，必须在start()之前调用。默认为False。 8，isDaemon()，判断线程是否随主线程一起结束。 9，isAlive()，检查线程是否在运行中。 12345678910111213141516171819202122232425262728293031323334353637import threadingimport timeclass timer(threading.Thread): #The timer class is derived from the class threading.Thread def __init__(self, num, interval): threading.Thread.__init__(self) self.thread_num = num self.interval = interval self.thread_stop = False def run(self): #Overwrite run() method, put what you want the thread do here while not self.thread_stop: print 'Thread Object(%d), Time:%s\n' %(self.thread_num, time.ctime()) time.sleep(self.interval) def stop(self): self.thread_stop = True def test(): thread1 = timer(1, 1) thread2 = timer(2, 2) thread1.start() thread2.start() time.sleep(10) thread1.stop() thread2.stop() return if __name__ == '__main__': test()假设两个线程对象t1和t2都要对num=0进行增1运算，t1和t2都各对num修改10次，num的最终的结果应该为20。但是由于是多线程访问，有可能出现下面情况：在num=0时，t1取得num=0。系统此时把t1调度为”sleeping”状态，把t2转换为”running”状态，t2页获得num=0。然后t2对得到的值进行加1并赋给num，使得num=1。然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给num。这样，明明t1和t2都完成了1次加1工作，但结果仍然是num=1。上面的case描述了多线程情况下最常见的问题之一：数据共享。当多个线程都要去修改某一个共享数据的时候，我们需要对数据访问进行同步。 简单的同步 最简单的同步机制就是“锁”。锁对象由threading.RLock类创建。线程可以使用锁的acquire()方法获得锁，这样锁就进入“locked”状态。每次只有一个线程可以获得锁。如果当另一个线程试图获得这个锁的时候，就会被系统变为“blocked”状态，直到那个拥有锁的线程调用锁的release()方法来释放锁，这样锁就会进入“unlocked”状态。“blocked”状态的线程就会收到一个通知，并有权利获得锁。如果多个线程处于“blocked”状态，所有线程都会先解除“blocked”状态，然后系统选择一个线程来获得锁，其他的线程继续沉默（“blocked”）。 Python的threading module是在建立在thread module基础之上的一个module，在threading module中，暴露了许多thread module中的属性。在thread module中，python提供了用户级的线程同步工具“Lock”对象。而在threading module中，python又提供了Lock对象的变种: RLock对象。RLock对象内部维护着一个Lock对象，它是一种可重入的对象。对于Lock对象而言，如果一个线程连续两次进行acquire操作，那么由于第一次acquire之后没有release，第二次acquire将挂起线程。这会导致Lock对象永远不会release，使得线程死锁。RLock对象允许一个线程多次对其进行acquire操作，因为在其内部通过一个counter变量维护着线程acquire的次数。而且每一次的acquire操作必须有一个release操作与之对应，在所有的release操作完成之后，别的线程才能申请该RLock对象。 12345678910111213141516171819202122import threadingimport timemylock=threading.RLock()num=0f=file('test_result.txt','w')class dog(theading.Thread): def __init__(self,name): theading.Thread,__init__(self) self.name=name def run(self): global num while num&lt;=5: time.sleep(0.5) mylock.acquire() print "Th(%s) locked, number: %d\n" %(self.name, num) f.write(self.name+" "+str(num)+'\n') print "Th(%s) released, number: %d\n" %(self.name, num) mylock.release() num += 1 条件同步 锁只能提供最基本的同步。假如只在发生某些事件时才访问一个“临界区”，这时需要使用条件变量 Condition。Condition对象是对Lock对象的包装，在创建Condition对象时，其构造函数需要一个Lock对象作为参数，如果没有这个Lock对象参数，Condition将在内部自行创建一个Rlock对象。在Condition对象上，当然也可以调用acquire和release操作，因为内部的Lock对象本身就支持这些操作。但是Condition的价值在于其提供的wait和notify的语义。 条件变量是如何工作的呢？首先一个线程成功获得一个条件变量后，调用此条件变量的wait()方法会 导致这个线程释放这个锁，并进入“blocked”状态，直到另一个线程调用同一个条件变量的notify()方法来唤醒那个进入“blocked”状态的线程。如果调用这个条件变量的notifyAll()方法的话就会唤醒所有的在等待的线程。 如果程序或者线程永远处于“blocked”状态的话，就会发生死锁。所以如果使用了锁、条件变量等同步机制的话，一定要注意仔细检查，防止死锁情况的发生。对于可能产生异常的临界区要使用异常处理机制中的finally子句来保证释放锁。等待一个条件变量的线程必须用notify()方法显式的唤醒，否则就永远沉默。保证每一个wait()方法调用都有一个相对应的notify()调用，当然也可以调用notifyAll()方法以防万一。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import threadingimport time con = threading.Condition()x=0 class Producer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x&gt;0: con.wait() else: for i in range(5): x += 1 print "producing... "+str(x) con.notify() print x con.release() class Consumer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x==0: print "consumer wait" con.wait() else: for i in range(5): x -= 1 print "consuming... "+str(x) con.notify() print x con.release() def test(): print "start consumer\n" th1 = Consumer("consumer") print "start producer\n" th2 = Producer("producer") th1.start() th2.start() th1.join() th2.join() if __name__ == '__main__': test() 同步队列 Python中的Queue对象也提供了对线程同步的支持。使用Queue对象可以实现多个生产者和多个消费 者形成的FIFO的队列。 生产者将数据依次存入队列，消费者依次从队列中取出数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import threadingimport timeimport Queueimport random #producerclass Producer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): print "%s is producing %d to the queue!\n" %(self.getName(), i) self.data.put(i) time.sleep(random.randrange(10)/5) print "%s finished!\n" %(self.getName()) #consumerclass Consumer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): something = self.data.get() print "%s is consuming. %d in the queue is consumed!\n" %(self.getName(), something) time.sleep(random.randrange(10)) print "%s finished!\n" %(self.getName()) def test(): queue = Queue.Queue() th1 = Producer(queue) th2 = Consumer(queue) th1.start() th2.start() th1.join() th2.join() print "all threads terminate!\n" if __name__ == '__main__': test() join的用法123456789101112131415161718192021222324252627282930313233343536import threadingimport timedef context(tJoin): print 'in threadContext.' tJoin.start() # 将阻塞tContext直到threadJoin终止。 tJoin.join() # tJoin终止后继续执行。 print 'out threadContext.'def join(): print 'in threadJoin.' time.sleep(1) print 'out threadJoin.'tJoin = threading.Thread(target=join)tContext = threading.Thread(target=context, args=(tJoin,))tContext.start()#结果：in threadContext.in threadJoin.out threadJoin.out threadContext.&gt; tContext = threading.Thread(target=context, args=(tJoin,))&gt; tContext.start()# tJoin = threading.Thread(target=join)执行后，只是创建了一个线程对象tJoin，但并未启动该线程,这两# 句执行后，创建了另一个线程对象tContext并启动该线程（打印in threadContext.），同时将tJoin线程# 对象作为参数传给context函数，在context函数中，启动了tJoin这个线程，同时该线程又调用了join()函# 数（tJoin.join()），那tContext线程将等待tJoin这线程执行完成后，才能继续tContext线程后面的，所以# 先执行join()函数,tJoin线程执行结束后，继续执行tContext线程，于是打印输出了out threadContext.，# 于是就看到我们上面看到的输出结果，并且无论执行多少次，结果都是这个顺序。但如果将context()函# 数中tJoin.join()这句注释掉，再执行该程序，打印输出的结果顺序就不定了，因为此时这两线程就是并# 发执行的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_disk_cached.py_shutil]]></title>
    <url>%2F2018%2F04%2F14%2FC03-disk-cached-py-shutil%2F</url>
    <content type="text"><![CDATA[os模块提供了对目录或者文件的新建/删除/查看文件属性，还提供了对文件以及目录的路径操作。比如说：绝对路径，父目录…… 但是，os文件的操作还应该包含移动 复制 打包 压缩 解压等操作，这些os模块都没有提供，shutil则就是对os中文件操作的补充， shutil是shell utility的缩写。 shutil 模块shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 os 模块os.sep 可以取代操作系统特定的路径分隔符。windows下为 ‘\‘os.name 字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是 ‘posix’os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径os.getenv() 获取一个环境变量，如果没有返回noneos.putenv(key, value) 设置一个环境变量值os.listdir(path) 返回指定目录下的所有文件和目录名os.remove(path) 函数用来删除一个文件os.system(command) 函数用来运行shell命令os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 ‘\r\n’，Linux使用 ‘\n’ 而Mac使用 ‘\r’os.path.split(path) 函数返回一个路径的目录名和文件名os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录os.path.exists() 函数用来检验给出的路径是否真地存在os.curdir 返回当前目录 (‘.’)os.mkdir(path) 创建一个目录os.makedirs(path) 递归的创建目录os.chdir(dirname) 改变工作目录到dirnameos.path.getsize(name) 获得文件大小，如果name是目录返回0Los.path.abspath(name) 获得绝对路径os.path.normpath(path) 规范path字符串形式os.path.splitext() 分离文件名与扩展名os.path.join(path,name) 连接目录与文件名或目录os.path.basename(path) 返回文件名os.path.dirname(path) 返回文件路径os.walk(top,topdown=True,onerror=None) 遍历迭代目录os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。os.renames(old, new) 递归重命名文件夹或者文件。像rename()]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_mongo_cache.py_pymongo]]></title>
    <url>%2F2018%2F04%2F14%2FC03-mongo-cache-py-pymongo%2F</url>
    <content type="text"><![CDATA[基本用法 123456789101112131415161718192021222324252627282930313233import pymongoclient = pymongo.MongoClient(host='localhost', port=27017)db = client.cache# 指定为test 数据库collection = db.webpage#MongoDB 的每个数据库又包含了许多集合 Collection，也就类似与关系型数据库中的表condition = &#123;'name': 'Kevin'&#125;student = collection.find_one(condition)student['age'] = 25result = collection.update(condition, student)或者result = collection.update(condition, &#123;'$set': student&#125;)#只更新 student 字典内存在的字段，如果其原先还有其他字段则不会更新，也不会删除。而如果不用 $set 的话则会把之前的数据全部用 student 字典替换，如果原本存在其他的字段则会被删除。condition = &#123;'age': &#123;'$gt': 20&#125;&#125;result = collection.update_one(condition, &#123;'$inc': &#123;'age': 1&#125;&#125;)print(result)print(result.matched_count, result.modified_count)#指定查询条件为年龄大于 20，然后更新条件为 &#123;'$inc': &#123;'age': 1&#125;&#125;，也就是年龄加 1，执行之后会将第一条符合条件的数据年龄加 1# find_and_modify用法class MongoQueue: def pop(self): """Get an outstanding URL from the queue and set its status to processing. If the queue is empty a KeyError exception is raised. """ record = self.db.crawl_queue.find_and_modify( query=&#123;'status': self.OUTSTANDING&#125;, update=&#123;'$set': &#123;'status': self.PROCESSING, 'timestamp': datetime.now()&#125;&#125; ) if record: return record['_id'] else: self.repair() raise KeyError() mongodb存储二进制数据 BSON是一种类json的一种二进制形式的存储格式，简称Binary JSON，它和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。{“hello”:”world”} 这是一个BSON的例子，其中”hello”是key name，它一般是cstring类型，字节表示是cstring::= (byte) “/x00” ,其中表示零个或多个byte字节，/x00表示结束符;后面的”world”是value值，它的类型一般是、string,double,array,binarydata等类型。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pymongoimport bson.binaryfrom pymongo import MongoClientfrom cStringIO import StringIOdef insertFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image filename = 'F:/测试数据/hehe.jpg'.decode('utf-8') with open (filename,'rb') as myimage: content = StringIO(myimage.read()) coll.save(dict( content= bson.binary.Binary(content.getvalue()), filename = 'hehe.jpg' )) def getFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image data = coll.find_one(&#123;'filename':'hehe.jpg'&#125;) out = open('F:/测试数据/test4.jpg'.decode('utf-8'),'wb') out.write(data['content']) out.close()getFile()#Here is an example of how to save some data to MongoDB and then load it:&gt;&gt;&gt; url = 'http://example.webscraping.com/view/United-Kingdom-239'&gt;&gt;&gt; html = '...'&gt;&gt;&gt; db = client.cache &gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)ObjectId('5518c0644e0c87444c12a577')&gt;&gt;&gt; db.webpage.find_one(url=url)&#123;u'_id': ObjectId('5518c0644e0c87444c12a577'), u'html': u'...', u'url': u'http://example.webscraping.com/view/United-Kingdom-239'&#125;#A problem with the preceding example is that if we now insert another document #with the same URL, MongoDB will happily insert it for us, as follows:&gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)&gt;&gt;&gt; db.webpage.find(url=url).count()2#Now we have multiple records for the same URL when we are only interested in #storing the latest data. To prevent duplicates, we can set the ID to the URL and #perform upsert, which means updating the existing record if it exists; otherwise, #insert a new one, as shown here:&gt;&gt;&gt; self.db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': html&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': ''&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.find_one(&#123;'_id': url&#125;)&#123;u'_id': u'http://example.webscraping.com/view/United-Kingdom-239', u'html': u'...'&#125;#A timestamp index was created in the constructor. This is a handy MongoDB feature that will #automatically delete records in a specified number of seconds after the given timestamp. This means#that we do not need to manually check whether a record is still valid, as in the DiskCache class.&gt;&gt;&gt;expires=timedelta(days=30)&gt;&gt;&gt;self.db.webpage.create_index('timestamp',expireAfterSeconds=expires.total_seconds()) zlib压缩与解压缩1234567891011121314151617181920212223242526272829303132import zlibdef compress(infile, dst, level=9): infile = open(infile, 'rb') dst = open(dst, 'wb') compress = zlib.compressobj(level) data = infile.read(1024) while data: dst.write(compress.compress(data)) data = infile.read(1024) dst.write(compress.flush())def decompress(infile, dst): infile = open(infile, 'rb') dst = open(dst, 'wb') decompress = zlib.decompressobj() data = infile.read(1024) while data: dst.write(decompress.decompress(data)) data = infile.read(1024) dst.write(decompress.flush()) if __name__ == "__main__": infile = "1.txt" dst = "1.zlib.txt" compress(infile, dst) infile = "1.zlib.txt" dst = "2.txt" decompress(infile, dst) print "done~"compressobj返回一个压缩对象，用来压缩不能一下子读入内存的数据流。 level 从9到-1表示压缩等级，其中1最快但压缩度最小，9最慢但压缩度最大，0不压缩，默认是-1大约相当于与等级6，是一个压缩速度和压缩度适中的level。 picklepickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle模块只能在python中使用，python中几乎所有的数据类型（列表，字典，集合，类等）都可以用pickle来序列化 pickle.dump(obj, file[, protocol]) 序列化对象，并将结果数据流写入到文件对象中。参数protocol是序列化模式，默认值为0，表示以文本的形式序列化。protocol的值还可以是1或2，表示以二进制的形式序列化。 dumps()函数执行和dump() 函数相同的序列化，但是与dump不同的dumps并不将转换后的字符串写入文件，而是将所得到的转换后的数据以字符串的形式返回。 pickle.load(file) 反序列化对象。将文件中的数据解析为一个Python对象。 loads()函数执行和load()函数一样的反序列化。 loads接受一个字符串参数，将字符串解码成为python的数据类型，函数loads和dumps进行的是互逆的操作。12345678910111213141516171819202122232425import cPickle #序列化到文件obj = 123,"abcdedf",["ac",123],&#123;"key":"value","key1":"value1"&#125;print obj#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)#r 读写权限 r b 读写到二进制文件f = open(r"d:\a.txt","r ")cPickle.dump(obj,f)f.close()f = open(r"d:\a.txt")print cPickle.load(f)#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;) #序列化到内存（字符串格式保存），然后对象可以以任何方式处理如通过网络传输obj1 = cPickle.dumps(obj)print type(obj1)#输出：&lt;type 'str'&gt;print obj1#输出：python专用的存储格式obj2 = cPickle.loads(obj1)print type(obj2)#输出：&lt;type 'tuple'&gt;print obj2#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_downloader.py_datetime]]></title>
    <url>%2F2018%2F04%2F14%2FC03-downloader-py-datetime%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718from datetime import datetime&gt;&gt;&gt; today=datetime.today()&gt;&gt;&gt; today.date()datetime.date(2018, 4, 2)&lt;!-- more --&gt;&gt;&gt;&gt; today.time()datetime.time(4, 52, 20, 254000)&gt;&gt;&gt; next_month=today.replace(month=today.month+1)&gt;&gt;&gt; next_monthdatetime.datetime(2018, 5, 2, 4, 52, 20, 254000)&gt;&gt;&gt; next_month.strftime('%Y-%m-%d %H:%M:%S')'2018-05-02 04:52:20'&gt;&gt;&gt; import time&gt;&gt;&gt; t=time.mktime(today.timetuple()) #将一个datetime对象转成时间戳，很遗憾的是python并没直接提供这个方法，但是提供了一个timetuple()方法，它返回一个time.struct_time对象，通过它我们可以构造出时间戳&gt;&gt;&gt; t1522669940.0]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[春天的一些照片]]></title>
    <url>%2F2018%2F04%2F14%2F%E6%98%A5%E5%A4%A9%E7%9A%84%E4%B8%80%E4%BA%9B%E7%85%A7%E7%89%87%2F</url>
    <content type="text"><![CDATA[这些照片肯定不是出自本人，双休日这么好的日子，宅着，看看nba, 看看番。 摄于2018年3月，同事去某个植物园，可能是上海植物园，陶堰情操，拍拍花草，感觉比以前拍的好看点。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>photo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask 博客配置]]></title>
    <url>%2F2018%2F04%2F14%2Fflask-%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[服务器，域名，DNSvps: vultr debian7 domain: Freenom ，具体操作可见此处(需科学上网) DNS: cloudfare, 具体操作可见此处(需科学上网)，这篇文章介绍的是cloudXNS, 也适用于cloudfare 服务器环境设置 工具 git bash 或者 putty，git bash是windows上安装git时顺带安装的bash模拟器，后续还用到git,所以直接安装git 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#此处ip是vps的ip地址， 22是ssh连接的默认端口号， 如果没有修改过默认端口，这个-p可以省略不写了ssh -p 22 root@ip#输入vps的密码后就可以登录了，此时建议换一个密码[root@vultr:~]passwd#新建一个普通用户,名为user, ‘adduser’也可以新建用户， 但是创建的用户是三无用户，无家目录/无bash/无密码，所以此处用的是'useradd',记性不好，这两个命令常常搞错。。[root@vultr:~]# user#创建密码[root@vultr:~]passwd user#此时新用户创建完成， 还需把它加入sudoers列表中，让它可以使用root的命令[root@vultr:~]apt-get install sudo #debian7 上没有默认安装sudo命令[root@vultr:~]visudo...root ALL=(ALL) ALLuser ALL=(ALL) ALL #新增加这一行就Ok了#切换到user用户[root@vultr:~]su - user[user@vultr:~]$#这个博客是基于flask,python3.5写的，所以还得安装python3.5, debian7默认已安装的是python2.7[user@vultr:~]$sudo apt-get update[user@vultr:~]$sudo dpkg -l python* #可以看到最新的python是到3.1#安装依赖[user@vultr:~]$sudo apt-get install -y build-essential libncurses5-dev libncursesw5-dev libreadline6-dev libdb5.1-dev libgdbm-dev libsqlite3-dev libssl-dev libbz2-dev libexpat1-dev liblzma-dev zlib1g-dev#下载安装包[user@vultr:~]$wget --no-check-certificate https://www.python.org/ftp/python/3.5.1/Python-3.5.1.tgz#编译安装[user@vultr:~]$tar xzvf Python-3.5.1.tgz[user@vultr:~]$cd Python-3.5.1.tgz[user@vultr:~]$./configure --prefix=/usr/local/python3 #创建 Makefile 这个文件，prefix是安装路径[user@vultr:~]$make all #all会编译所有子模块，如sqlite3等[user@vultr:~]$sudo make install#成功安装后应该可以看到一下信息Installing collected packages: setuptools, pipSuccessfully installed pip-7.1.2 setuptools-18.2#按照提示升级pip3[user@vultr:~]$pip3 install --upgrade pip#为当前用户user添加路径[user@vultr:~]$vim ~/.bashrc添加 export PATH=$PATH:/usr/local/python3/bin[user@vultr:~]$source ~/.bashrc #将当前bashrc设置读入目前的bash环境中[user@vultr:~]$python3 --verison #当前python3的版本#为python3, pip3, virtualenv设置软链接, 这里没有把原来的python命令删除[user@vultr:~]$sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/virtualenv /usr/bin/virtualenv #/usr/bin 所以一般用户用的指令放的地方 安装数据库redis123456789101112131415161718#redis安装，方法一$wget http://download.redis.io/releases/redis-stable.tar.gz$ xzf redis-stable.tar.gz$cd redis-stable$make$make test #run the test after build$sudo make install #redis安装， 方法二#其实直接可以用apt-get安装啦$sudo apt-get install redis-server#安装完成后， redis会自动启动$ps -aux | grep redis #检查redis进程$netstat -nlt | grep 6379 #检查redis网络监听端口$sudo /etc/init.d/redis-server status # /etc/init.d/ #系统服务启动的接口放在这个目录下 安装postgresql123456789101112131415#添加postgrelsql apt repository$sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ wheezy-pgdg main" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'#导入密匙$wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -#安装$sudo apt-get update$sudo apt-get install postgresql postgresql-contrib#默认情况下安装PostgreSQL数据库服务器后，它将创建一个用户'postgres'，角色为'postgres'。 它还创建一个名称为“postgres”的系统帐户#登陆$sudo su - postgres$psql 上传文件到vps一般来说有两种方式， 一种是scp, 一种git clone 123456789101112#从本地复制目录到远程服务器， -r 递归复制整个目录$scp -r local_folder remote_username@remote_ip:remote_folder #此处remote_folder为dir=/home/&#123;user&#125;/blog，local_folder为your_blogname#如果是git clone的话，先注册个git hub 账号，然后创建一个repository,把本地文件push到这个库里$git init $git add .$git commit -m 'upload'$git configure --global$git remote add origin https://github...$git push -u origin master#感觉用git clone比较快点，而且可以随时随地上传，比较方便 安装虚拟环境把文件上传到vps后，就需要安装python3的虚拟环境了 12345678#在当前目录下，比如dir=/home/&#123;user&#125;/blog$ virtualenv venv #此时dir下生成一个venv文件夹$ source ./venv/bin/activate #激活虚拟环境（关闭虚拟环境 deactivate）#安装库$ pip install -r requirements.txt redis配置根据config.py设置的redis 密码，将/etc/redis/redis.conf 中设置成相应的密码 123$ sudo cp /etc/redis/redis.conf /usr/local/etc/redis.conf$ sudo vim /usr/local/etc/redis.conf #找到requirepass这一行，取消注释，并加上自己的password$ sudo redis-server /usr/local/etc/redis.conf #按照配置路径启动 postgresql配置根据config.py设置 ，SQLALCHEMY_DATABASE_URI = ‘postgresql://user:password@localhost/blog’，一个数据库名为’blog’, 用户是’user’，密码是’password’ 12345678$sudo -u postgres psqlpostgres= CREATE DATABASE blog;postgres= CREATE USER user WITH PASSWORD 'password';postgres= ALTER ROLE user SET client_encoding TO 'utf8';postgres= ALTER ROLE user SET default_transaction_isolation TO 'read committed';postgres= ALTER ROLE user SET timezone TO 'UTC';postgres= GRANT ALL PRIVILEGES ON DATABASE blog TO user;postgres= \q nginx安装和配置123456789101112131415161718192021222324252627#安装最新稳定版$echo deb http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list #更新库$echo deb-src http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list$wget http://nginx.org/keys/nginx_signing.key &amp;&amp; apt-key add nginx_signing.key #升级key$apt-get install nginx#配置文件$ sudo vim /etc/nginx/site-avaliable/blog添加以下内容server &#123; listen 80; server_name whistlestop.ml; # 域名， 用ip也行 location / &#123; proxy_pass http://127.0.0.1:8000; # gunicorn.py里的端口号 proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; $ cp /etc/nginx/sites-avaliable/blog /etc/nginx/sites-enabled/blog$ rm /etc/nginx/sites-enable/default #把原来的default文件删除#重启nginx$ sudo service nginx restart 启动博客1gunicorn --config gunicorn.py manager:app 后续要做的事 数据库备份 markdown预览功能 整个vps如何维护]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
</search>

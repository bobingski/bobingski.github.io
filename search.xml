<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sqlalchemy_C10_Reflection with SQLAlchemy ORM and Automap]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C10-Reflection-with-SQLAlchemy-ORM-and-Automap%2F</url>
    <content type="text"><![CDATA[Reflection lets you populate a SQLAlchemy object from an existing database; reflection works on tables, views, indexes, and foreign keys. But what if you want to reflect a database schema into ORM-style classes? Fortunately, the handy SQLAlchemy extension automap lets you do just that. Reflecting a Database with AutomapIn order to reflect a database, instead of using the declarative_base we’ve been using with the ORM so far, we’re going to use the automap_base. Let’s start by creating a Base object to work with. _Creating a Base object with automap_base_ 12&gt;&gt;&gt; from sqlalchemy.ext.automap import automap_base&gt;&gt;&gt; Base= automap_base() Next, we need an engine connected to the database that we want to reflect. Initializaing an engine for the Chinook database 12from sqlalchemy import create_engineengine= create_engine('sqlite:///Chinook_Sqlite.sqlite') With the Base and engine setup, we have everything we need to reflect the database. Using the prepare method on the Base object we created will scan everything available on the engine we just created, and reflect everything it can. &gt;&gt;&gt; Base.prepare(engine, reflect= True) That one line of code is all you need to reflect the entire database! This reflection has created ORM objects for each table that is accessible under the class property of the automap Base.123456789101112&gt;&gt;&gt; Base.classes.keys()['Album','Customer','Playlist','Artist','Track','Employee','MediaType','InvoiceLine','Invoice','Genre'] create some objects to reference the Artist and Album tables: 12&gt;&gt;&gt; Artist= Base.classes.Artist&gt;&gt;&gt; Album= Base.classes.Album Using the Artist table1234567891011121314&gt;&gt;&gt; from sqlalchemy.orm import Session&gt;&gt;&gt; session= Session(engine)&gt;&gt;&gt; for artist in session.query(Artist).limit(10):... print(artist.ArtistId, artist.Name)(1, u'AC/DC')(2, u'Accept')(3, u'Aerosmith')(4, u'Alanis Morissette')(5, u'Alice In Chains')(6, u'Ant\xf4nio Carlos Jobim')(7, u'Apocalyptica')(8, u'Audioslave')(9, u'BackBeat')(10, u'Billy Cobham') Reflected RelationshipsAutomap can automatically reflect and establish many-to-one, one-to-many, and many-to-many relationships. When automap creates a relationship, it creates a &lt;related_object&gt;_collection property on the object. Using the relationship between Artist and Album to print related data 12345&gt;&gt;&gt; artist= session.query(Artist).first()&gt;&gt;&gt; for album in artist.album_collection:... print('&#123;&#125;- &#123;&#125;'.format(artist.Name, album.Title))AC/DC - For Those About To Rock We Salute YouAC/DC - Let There Be Rock]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C08_Understanding the Session and Exceptions]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C08-Understanding-the-Session-and-Exceptions%2F</url>
    <content type="text"><![CDATA[The SQLAlchemy SessionUnderstanding the session states can be useful for troubleshooting exceptions and handling unexpected behaviors. There are four possible states for data object instances: Transient The instance is not in session, and is not in the database. Pending The instance has been added to the session with add(), but hasn’t been flushed or committed. Persistent The object in session has a corresponding record in the database. Detached The instance is no longer connected to the session, but has a record in the database. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;&gt;&gt; session.add(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Truepersistent: False detached: False&gt;&gt;&gt; cc_cookie=Cookie('chocolate chip','http://','CC01',12,0.50)&gt;&gt;&gt; from sqlalchemy import inspect&gt;&gt;&gt; insp= inspect(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: True pending: Falsepersistent: False detached: False&gt;&gt;&gt; session.commit()&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Falsepersistent: True detached: False&gt;&gt;&gt; session.expunge(cc_cookie)&gt;&gt;&gt; for state in ['transient', 'pending','persistent','detached']:... print('&#123;:&gt;10&#125;: &#123;&#125;'.format(state, getattr(insp, state)))... transient: False pending: Falsepersistent: False detached: True&gt;&gt;&gt; session.add(cc_cookie)&gt;&gt;&gt; cc_cookie.cookie_name= 'Change cholocate chip'&gt;&gt;&gt; insp.modifiedTrue&gt;&gt;&gt; for attr, attr_state in insp.attrs.items():... if attr_state.history.has_changes():... print('&#123;&#125;: &#123;&#125;'.format(attr, attr_state.value))... print('History: &#123;&#125;\n'.format(attr_state.history))... cookie_name: Change cholocate chipHistory: History(added=['Change cholocate chip'], unchanged=(), deleted=())&gt;&gt;&gt; ExceptionsMultipleResultsFound ExceptionThis exception occurs when we use the .one() query method, but get more than one result back. 1234567891011121314151617181920212223&gt;&gt;&gt; results=session.query(Cookie).one()Traceback (most recent call last): File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2884, in one ret = self.one_or_none() File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2863, in one_or_none "Multiple rows were found for one_or_none()")sqlalchemy.orm.exc.MultipleResultsFound: Multiple rows were found for one_or_none()During handling of the above exception, another exception occurred:Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2887, in one "Multiple rows were found for one()")sqlalchemy.orm.exc.MultipleResultsFound: Multiple rows were found for one()&gt;&gt;&gt; from sqlalchemy.orm.exc import MultipleResultsFound&gt;&gt;&gt; try:... results= session.query(Cookie).one()... except MultipleResultsFound as error:... print('too many cookies...')... too many cookies...&gt;&gt;&gt; DetachedInstanceErrorThis exception occurs when we attempt to access an attribute on an instance that needs to be loaded from the database, but the instance we are using is not currently attached to the database. 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; cookiemon = User('cookiemon', 'mon@cookie.com', '111-111-1111', 'password')&gt;&gt;&gt; session.add(cookiemon)&gt;&gt;&gt; o1=Order()&gt;&gt;&gt; o1.user= cookiemon&gt;&gt;&gt; session.add(o1)&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='Change chocolate chip').one()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\query.py", line 2890, in one raise orm_exc.NoResultFound("No row was found for one()")sqlalchemy.orm.exc.NoResultFound: No row was found for one()&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='Change cholocate chip').one()&gt;&gt;&gt; ccCookie(cookie_name='Change cholocate chip', cookie_recipe_url='http://', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; line1= LineItem(order= o1, cookie= cc, quantity= 2, extended_cost= 1.00)&gt;&gt;&gt; session.add(line1)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; order= session.query(Order).first()&gt;&gt;&gt; session.expunge(order)&gt;&gt;&gt; order.line_itemsTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\attributes.py", line 242, in __get__ return self.impl.get(instance_state(instance), dict_) File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\attributes.py", line 599, in get value = self.callable_(state, passive) File "C:\FluentPython\env\lib\site-packages\sqlalchemy\orm\strategies.py", line 596, in _load_for_state (orm_util.state_str(state), self.key)sqlalchemy.orm.exc.DetachedInstanceError: Parent instance &lt;Order at 0x665f3b0&gt; is not bound to a Session; lazy load operation of attribute 'line_items' cannot proceed (Background on this error at: http://sqlalche.me/e/bhk3)&gt;&gt;&gt; Transactions12345678910111213141516171819202122232425262728293031323334353637383940414243cookiemon = User('cookiemon', 'mon@cookie.com', '111-111-1111', 'password') cc = Cookie('chocolate chip', 'http://some.aweso.me/cookie/recipe.html', 'CC01', 12, 0.50)dcc = Cookie('dark chocolate chip', 'http://some.aweso.me/cookie/recipe_dark.html', 'CC02', 1, 0.75)session.add(cookiemon)session.add(cc)session.add(dcc)o1 = Order()o1.user = cookiemonsession.add(o1)line1 = LineItem(order=o1, cookie=cc, quantity=9, extended_cost=4.50)session.add(line1)session.commit() o2 = Order()o2.user = cookiemonsession.add(o2)line1 = LineItem(order=o2, cookie=cc, quantity=2, extended_cost=1.50)line2 = LineItem(order=o2, cookie=dcc, quantity=9, extended_cost=6.75)session.add(line1)session.add(line2)session.commit() def ship_it(order_id): order = session.query(Order).get(order_id) for li in order.line_items: li.cookie.quantity = li.cookie.quantity - li.quantity session.add(li.cookie) order.shipped = True session.add(order) session.commit() print('shipped order ID: &#123;&#125;'.format(order_id)) 1234567891011121314from sqlalchemy.exc import IntegrityError def ship_it(order_id): order = session.query(Order).get(order_id) for li in order.line_items: li.cookie.quantity = li.cookie.quantity - li.quantity session.add(li.cookie) order.shipped = True session.add(order) try: session.commit() print('shipped order ID: &#123;&#125;'.format(order_id)) except IntegrityError as error: print('ERROR: &#123;!s&#125;'.format(error.orig)) session.rollback()]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C06_Defining Schema with SQLAlchemy ORM_02]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C06-Defining-Schema-with-SQLAlchemy-ORM-02%2F</url>
    <content type="text"><![CDATA[The Session 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; from sqlalchemy.orm import sessionmaker&gt;&gt;&gt; &gt;&gt;&gt; engine= create_engine('sqlite:///:memory:')&gt;&gt;&gt; Session= sessionmaker(bind= engine)&gt;&gt;&gt; session =Session()&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; from sqlalchemy.orm import sessionmaker&gt;&gt;&gt; engine = create_engine('sqlite:///:memory:')&gt;&gt;&gt; Session = sessionmaker(bind=engine)&gt;&gt;&gt; session = Session()&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; from sqlalchemy import Column, Integer, Numeric, String, DateTime, ForeignKey, Boolean&gt;&gt;&gt; from sqlalchemy.ext.declarative import declarative_base&gt;&gt;&gt; from sqlalchemy.orm import relationship, backref&gt;&gt;&gt; Base= declarative_base()&gt;&gt;&gt; class Cookie(Base):... __tablename__= 'cookies'... ... cookie_id = Column(Integer(), primary_key=True)... cookie_name = Column(String(50), index=True)... cookie_recipe_url = Column(String(255))... cookie_sku = Column(String(55))... quantity = Column(Integer())... unit_cost = Column(Numeric(12, 2))... ... def __repr__(self):... return "Cookie(cookie_name='&#123;self.cookie_name&#125;', " \... "cookie_recipe_url='&#123;self.cookie_recipe_url&#125;', " \... "cookie_sku='&#123;self.cookie_sku&#125;', " \... "quantity=&#123;self.quantity&#125;, " \... "unit_cost=&#123;self.unit_cost&#125;)".format(self=self)... &gt;&gt;&gt; class User(Base):... __tablename__ = 'users'... ... user_id = Column(Integer(), primary_key=True)... username = Column(String(15), nullable=False, unique=True)... email_address = Column(String(255), nullable=False)... phone = Column(String(20), nullable=False)... password = Column(String(25), nullable=False)... created_on = Column(DateTime(), default=datetime.now)... updated_on = Column(DateTime(), default=datetime.now, onupdate=datetime.now)... ... def __repr__(self):... return "User(username='&#123;self.username&#125;', " \... "email_address='&#123;self.email_address&#125;', " \... "phone='&#123;self.phone&#125;', " \... "password='&#123;self.password&#125;')".format(self=self)... &gt;&gt;&gt; class Order(Base):... __tablename__ = 'orders'... order_id = Column(Integer(), primary_key=True)... user_id = Column(Integer(), ForeignKey('users.user_id'))... shipped = Column(Boolean(), default=False)... ... user = relationship("User", backref=backref('orders', order_by=order_id))... ... def __repr__(self):... return "Order(user_id=&#123;self.user_id&#125;, " \... "shipped=&#123;self.shipped&#125;)".format(self=self)... ... &gt;&gt;&gt; class LineItem(Base):... __tablename__ = 'line_items'... line_item_id = Column(Integer(), primary_key=True)... order_id = Column(Integer(), ForeignKey('orders.order_id'))... cookie_id = Column(Integer(), ForeignKey('cookies.cookie_id'))... quantity = Column(Integer())... extended_cost = Column(Numeric(12, 2))... ... order = relationship("Order", backref=backref('line_items', order_by=line_item_id))... cookie = relationship("Cookie", uselist=False)... ... def __repr__(self):... return "LineItems(order_id=&#123;self.order_id&#125;, " \... "cookie_id=&#123;self.cookie_id&#125;, " \... "quantity=&#123;self.quantity&#125;, " \... "extended_cost=&#123;self.extended_cost&#125;)".format(... self=self) ... &gt;&gt;&gt; Base.metadata.create_all(engine) Inserting DataInserting a single object 1234567cc_cookie = Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)session.add(cc_cookie) session.commit() When commit() is called on the session, the cookie is actually inserted into the database. It also updates cc_cookie with the primary key of the record in the database. 12345&gt;&gt;&gt; print(cc_cookie.cookie_id)C:\FluentPython\env\lib\site-packages\sqlalchemy\sql\sqltypes.py:603: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. 'storage.' % (dialect.name, dialect.driver))1&gt;&gt;&gt; When we create the instance of the Cookie class and then add it to the session, nothing is sent to the database. It’s not until we call commit() on the session that anything is sent to the database. Multiple inserts 123456789101112131415dcc = Cookie(cookie_name='dark chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity=1, unit_cost=0.75)mol = Cookie(cookie_name='molasses', cookie_recipe_url='http://some.aweso.me/cookie/recipe_molasses.html', cookie_sku='MOL01', quantity=1, unit_cost=0.80)session.add(dcc) session.add(mol) session.flush() print(dcc.cookie_id)print(mol.cookie_id) we used the flush() method on the session instead of commit().A flush is like a commit; however, it doesn’t perform a database commit and end the transaction. Because of this, the dcc and mol instances are still connected to the session, and can be used to perform additional database tasks without triggering additional database queries. We also issue the session.flush() statement one time, even though we added multiple records into the database. This actually results in two insert statements being sent to the database inside a single transaction. The second method of inserting multiple records into the database is great when you want to insert data into the table and you don’t need to perform additional work on that data. Bulk inserting multiple records 1234567891011121314c1 = Cookie(cookie_name='peanut butter', cookie_recipe_url='http://some.aweso.me/cookie/peanut.html', cookie_sku='PB01', quantity=24, unit_cost=0.25)c2 = Cookie(cookie_name='oatmeal raisin', cookie_recipe_url='http://some.okay.me/cookie/raisin.html', cookie_sku='EWW01', quantity=100, unit_cost=1.00)&gt;&gt;&gt; session.bulk_save_objects([c1,c2])&gt;&gt;&gt; session.commit()&gt;&gt;&gt; c1.cookie_id&gt;&gt;&gt; c1 object isn’t associated with the session, and can’t refresh its cookie_id for printing. The method demonstrated is substantially faster than performing multiple individual adds and inserts.This speed does come at the expense of some features we get in the normal add and commit, such as: Relationship settings and actions are not respected or triggered. The objects are not connected to the session. Fetching primary keys is not done by default. No events will be triggered. If you are inserting multiple records and don’t need access to relationships or the inserted primary key, use bulk_save_objects or its related methods. This is especially true if you are ingesting data from an external data source such as a CSV or a large JSON document with nested arrays. Querying DataGet all the cookies 1234567&gt;&gt;&gt; cookies= session.query(Cookie).all()&gt;&gt;&gt; for cookie in cookies: print(cookie)... Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)Cookie(cookie_name='peanut butter', cookie_recipe_url='http://some.aweso.me/cookie/peanut.html', cookie_sku='PB01', quantity=24, unit_cost=0.25)Cookie(cookie_name='oatmeal raisin', cookie_recipe_url='http://some.okay.me/cookie/raisin.html', cookie_sku='EWW01', quantity=100, unit_cost=1.00)&gt;&gt;&gt; These objects are connected to the session, which means we can change them or delete them and persist that change to the database. In addition to using the query as an iterable or calling the all() method, there are many other ways of accessing the data. You can use the following methods to fetch results: first() Returns the first record object if there is one. one() Queries all the rows, and raises an exception if anything other than a single result is returned. scalar() Returns the first element of the first result, None if there is no result, or an error if there is more than one result. Controlling the Columns in the QueryTo limit the fields that are returned from a query, we need to pass in the columns we want in the query() method constructor separated by columns. _Select only cookie_name_1234&gt;&gt;&gt; session.query(Cookie.cookie_id, Cookie.cookie_name).first()(1, 'chocolate chip')&gt;&gt;&gt; OrderingHowever, if we want the list to be returned in a particular order, we can chain an order_by() statement to our select. Order by quantity ascending 1234567&gt;&gt;&gt; for cookie in session.query(Cookie).order_by(Cookie.quantity):... print('&#123;:3&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name))... 12 - chocolate chip 24 - peanut butter100 - oatmeal raisin&gt;&gt;&gt; If you want to sort in reverse or descending order, use the desc() statement. The desc() function wraps the specific column you want to sort in a descending manner. Order by quantity descending 12345678&gt;&gt;&gt; from sqlalchemy import desc&gt;&gt;&gt; for cookie in session.query(Cookie).order_by(desc(Cookie.quantity)):... print('&#123;:3&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name))... 100 - oatmeal raisin 24 - peanut butter 12 - chocolate chip&gt;&gt;&gt; The desc() function can also be used as a method on a column object, such as Cookie.quantity.desc(). However, that can be a bit more confusing to read in long statements. LimitingIn prior examples, we used the first() method to get just a single row back. While our query() gave us the one row we asked for, the actual query ran over and accessed all the results, not just the single record. If we want to limit the query, we can use array slice notation to actually issue a limit statement as part of our query. Two fewest cookie inventories 1234&gt;&gt;&gt; query= session.query(Cookie).order_by(Cookie.quantity)[:2]&gt;&gt;&gt; print([result.cookie_name for result in query])['chocolate chip', 'peanut butter']&gt;&gt;&gt; In addition to using the array slice notation, it is also possible to use the limit() statement. Two fewest cookie inventories with limit123query = session.query(Cookie).order_by(Cookie.quantity).limit(2)print([result.cookie_name for result in query]) Built-In SQL Functions and LabelsSQLAlchemy can also leverage SQL functions found in the backend database. Two very commonly used database functions are SUM() and COUNT(). To use these functions, we need to import the sqlalchemy.func module generator that makes them available. These functions are wrapped around the column(s) on which they are operating. Summing our cookies 12345&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; inv_count= session.query(func.sum(Cookie.quantity)).scalar()&gt;&gt;&gt; inv_count136&gt;&gt;&gt; Notice the use of scalar, which will return only the leftmost column in the first record. Counting our inventory records 123&gt;&gt;&gt; rec_count= session.query(func.count(Cookie.cookie_name)).first()&gt;&gt;&gt; rec_count(3,) Using functions such as count() and sum() will end up returning tuples or results with column names like count_1. These types of returns are often not what we want. Also, if we have several counts in a query we’d have to know the occurrence number in the statement, and incorporate that into the column name, so the fourth count() function would be count_4. This simply is not as explicit and clear as we should be in our naming, especially when surrounded with other Python code. Renaming our count column 123456&gt;&gt;&gt; rec_count= session.query(func.count(Cookie.cookie_name).label('inventory_count')).first()&gt;&gt;&gt; rec_count.keys()['inventory_count']&gt;&gt;&gt; rec_count.inventory_count3&gt;&gt;&gt; FilteringFiltering queries is done by appending filter() statements to our query. A typical filter() clause has a column, an operator, and a value or column. It is possible to chain multiple filters() clauses together or comma separate multiple ClauseElement expressions in a single filter, and they will act like ANDs in traditional SQL statements. Filtering by cookie name with filter 1234&gt;&gt;&gt; record = session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').first()&gt;&gt;&gt; print(record)Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; There is also a filter_by() method that works similarly to the filter() method except instead of explicity providing the class as part of the filter expression it uses attribute keyword expressions from the primary entity of the query or the last entity that was joined to the statement. It also uses a keyword assignment instead of a Boolean. _Filtering by cookie name with filter_by_ 1234&gt;&gt;&gt; record= session.query(Cookie).filter_by(cookie_name='chocolate chip').first()&gt;&gt;&gt; print(record)Cookie(cookie_name='chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe.html', cookie_sku='CC01', quantity=12, unit_cost=0.50)&gt;&gt;&gt; Finding names with “chocolate” in them 1234567&gt;&gt;&gt; query= session.query(Cookie).filter(Cookie.cookie_name.like('%chocolate%'))&gt;&gt;&gt; for record in query:... print(record.cookie_name)... chocolate chipdark chocolate chip&gt;&gt;&gt; we are using the Cookie.cookie_name column inside of a filter statement as a type of ClauseElement to filter our results, and we are taking advantage of the like() method that is available on ClauseElements. There are many other methods available. If we don’t use one of the ClauseElement methods, then we will have an operator in our filter clauses. OperatorsString concatenation with + 123456789&gt;&gt;&gt; results= session.query(Cookie.cookie_name, 'SKU-'+Cookie.cookie_sku).all()&gt;&gt;&gt; for row in results:print(row)... ('chocolate chip', 'SKU-CC01')('dark chocolate chip', 'SKU-CC02')('molasses', 'SKU-MOL01')('peanut butter', 'SKU-PB01')('oatmeal raisin', 'SKU-EWW01')&gt;&gt;&gt; Inventory value by cookie 1234567891011121314&gt;&gt;&gt; from sqlalchemy import cast&gt;&gt;&gt; query= session.query(Cookie.cookie_name, cast(... (Cookie.quantity * Cookie.unit_cost),Numeric(12,2)... ).label('inv_cost') # using the label() function to rename the column... )... &gt;&gt;&gt; for result in query:print('&#123;&#125;- &#123;&#125;'.format(result.cookie_name, result.inv_cost))... chocolate chip- 6.00dark chocolate chip- 0.75molasses- 0.80peanut butter- 6.00oatmeal raisin- 100.00&gt;&gt;&gt; cast is a function that allows us to convert types. In this case, we will be getting back results such as 6.0000000000, so by casting it, we can make it look like currency. It is also possible to accomplish the same task in Python with print(‘{} -{:.2f}’.format(row.cookie_name, row.inv_cost)). If we need to combine where statements, we can use a couple of different methods. One of those methods is known as Boolean operators. Boolean Operatorswhen you write A &lt; B &amp; C &lt; D, what you are actually writing is A &lt; (B&amp;C) &lt; D, when you probably intended to get (A &lt; B) &amp;(C &lt; D). Often we want to chain multiple where clauses together in inclusive and exclusionary manners; this should be done via conjunctions. ConjunctionsWhile it is possible to chain multiple filter() clauses together, it’s often more readable and functional to use conjunctions to accomplish the desired effect. I also prefer to use conjunctions instead of Boolean operators, as conjunctions will make your code more expressive. The conjunctions in SQLAlchemy are and_(), or_(), and not_(). They have underscores to separate them from the built-in keywords. Using the or() conjunction 123456789101112131415161718&gt;&gt;&gt; query= session.query(Cookie).filter(... Cookie.quantity &gt; 23,... Cookie.unit_cost &lt; 0.4)... &gt;&gt;&gt; from sqlalchemy import and_, or_, not_&gt;&gt;&gt; query = session.query(Cookie).filter(... or_(... Cookie.quantity.between(10, 50),... Cookie.cookie_name.contains('chip')... )... )... &gt;&gt;&gt; for result in query: print(result.cookie_name)... chocolate chipdark chocolate chippeanut butter&gt;&gt;&gt; Updating Data Updating data via object 123456&gt;&gt;&gt; query=session.query(Cookie)&gt;&gt;&gt; cc_cookie= query.filter(Cookie.cookie_name=='chocolate chip').first()&gt;&gt;&gt; cc_cookie.quantity = cc_cookie.quantity + 120&gt;&gt;&gt; session.commit()&gt;&gt;&gt; print(cc_cookie.quantity)132 Updating data in place 1234567&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query=query.filter(Cookie.cookie_name=='chocolate chip')&gt;&gt;&gt; query.update(&#123;Cookie.quantity: Cookie.quantity - 20&#125;)1&gt;&gt;&gt; cc_cookie= query.first()&gt;&gt;&gt; print(cc_cookie.quantity)112 The update() method causes the record to be updated outside of the session, and returns the number of rows updated. Deleting DataTo create a delete statement, you can use either the delete() function or the delete() method on the table from which you are deleting data. Unlike insert() and update(), delete() takes no values parameter, only an optional where clause (omitting the where clause will delete all rows from the table). Deleting data 12345678&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query= query.filter(Cookie.cookie_name=='dark chocolate chip')&gt;&gt;&gt; dcc_cookie=query.one()&gt;&gt;&gt; session.delete(dcc_cookie)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; dcc_cookie= query.first()&gt;&gt;&gt; print(dcc_cookie)None It is also possible to delete data in place without having the object 1234567&gt;&gt;&gt; query= session.query(Cookie)&gt;&gt;&gt; query= query.filter(Cookie.cookie_name=='molasses')&gt;&gt;&gt; query.delete()1&gt;&gt;&gt; mol_cookie= query.first()&gt;&gt;&gt; print(mol_cookie)None Adding related objects 12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; cookiemon = User(username='cookiemon',... email_address='mon@cookie.com',... phone='111-111-1111',... password='password')... &gt;&gt;&gt; cakeeater = User(username='cakeeater',... email_address='cakeeater@cake.com',... phone='222-222-2222',... password='password')... &gt;&gt;&gt; pieperson = User(username='pieperson',... email_address='person@pie.com',... phone='333-333-3333',... password='password')... &gt;&gt;&gt; session.add(cookiemon)&gt;&gt;&gt; session.add(cakeeater)&gt;&gt;&gt; session.add(pieperson)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; o1=Order()&gt;&gt;&gt; o1.user= cookiemon&gt;&gt;&gt; session.add(o1)&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').one()&gt;&gt;&gt; line1= LineItem(cookie= cc, quantity= 2, extended_cost= 1.00)&gt;&gt;&gt; pb= session.query(Cookie ).filter(Cookie.cookie_name=='peanut butter').one()&gt;&gt;&gt; line2= LineItem(quantity= 12, extended_cost= 3.00)&gt;&gt;&gt; line2.cookie= pb&gt;&gt;&gt; line2.order = o1&gt;&gt;&gt; o1.line_items.append(line1)&gt;&gt;&gt; o1.line_items.append(line2)&gt;&gt;&gt; session.commit()&gt;&gt;&gt; o2=Order()&gt;&gt;&gt; o2.user= cakeeater&gt;&gt;&gt; cc= session.query(Cookie).filter(Cookie.cookie_name=='chocolate chip').one()&gt;&gt;&gt; line1= LineItem(cookie=cc, quantity= 24, extended_cost=12.00)&gt;&gt;&gt; oat= session.query(Cookie).filter(Cookie.cookie_name=='oatmeal raisin').one()&gt;&gt;&gt; line2= LineItem(cookie=oat, quantity= 6, extended_cost=6.00)&gt;&gt;&gt; o2.line_items.append(line1)&gt;&gt;&gt; o2.line_items.append(line2)&gt;&gt;&gt; session.add(o2)&gt;&gt;&gt; session.commit() Joinslet’s use the join() and outerjoin() methods to take a look at how to query related data. For example, to fulfill the order placed by the cookiemon user, we need to determine how many of each cookie type were ordered. This requires you to use a total of three joins to get all the way down to the name of the cookies. Using join to select from multiple tables 123456789 Cookie.cookie_name, LineItem.quantity, LineItem.extended_cost)query = query.join(User).join(LineItem).join(Cookie)results = query.filter(User.username == 'cookiemon').all()print(results)[ (u'1', u'cookiemon', u'111-111-1111', u'chocolate chip', 2, Decimal('1.00')) (u'1', u'cookiemon', u'111-111-1111', u'peanut butter', 12, Decimal('3.00'))] Using outerjoin to select from multiple tables 1234567891011&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; query=session.query(User.username, func.count(Order.order_id))&gt;&gt;&gt; query=query.outerjoin(Order).group_by(User.username)&gt;&gt;&gt; print(query)SELECT users.username AS users_username, count(orders.order_id) AS count_1 FROM users LEFT OUTER JOIN orders ON users.user_id = orders.user_id GROUP BY users.username&gt;&gt;&gt; for row in query: print(row)... ('cakeeater', 1)('cookiemon', 0)('pieperson', 0) However, what if we have a self-referential table like a table of managers and their reports? The ORM allows us to establish a relationship that points to the same table; however, we need to specify an option called remote_side to make the relationship a many to one: 1234567&gt;&gt;&gt; class Employee(Base):... __tablename__='employees'... id= Column(Integer(), primary_key= True)... manager_id= Column(Integer(), ForeignKey('employees.id'))... name= Column(String(255), nullable= False)... manager= relationship('Employee',backref=backref('reports'), remote_side=[id])... Establishes a relationship back to the same table, specifies the remote_side, and makes the relationship a many to one. add an employee and another employee that reports to her: 12345marsha = Employee(name='Marsha')fred = Employee(name='Fred')marsha.reports.append(fred)session.add(marsha)session.commit() print the employees that report to Marsha, we would do so by accessing the reports property as follows: 12for report in marsha.reports: print(report.name) GroupingWhen using grouping, you need one or more columns to group on and one or morecolumns that it makes sense to aggregate with counts, sums, etc., as you would innormal SQL. 1234query = session.query(User.username, func.count(Order.order_id))query = query.outerjoin(Order).group_by(User.username)for row in query: print(row) Chaining1234567891011def get_orders_by_customer(cust_name): query = session.query(Order.order_id, User.username, User.phone, Cookie.cookie_name, LineItem.quantity, LineItem.extended_cost) query = query.join(User).join(LineItem).join(Cookie) results = query.filter(User.username == cust_name).all() return resultsget_orders_by_customer('cakeeater')[(u'2', u'cakeeater', u'222-222-2222', u'chocolate chip', 24, Decimal('12.00')),(u'2', u'cakeeater', u'222-222-2222', u'oatmeal raisin', 6, Decimal('6.00'))] Conditional chaining 12345678910111213141516171819202122232425&gt;&gt;&gt; def get_orders_by_customer(cust_name, shipped= None, details= False):... query= session.query(Order.order_id, User.username, User.phone)... ... query= query.join(User)... if details:... query= query.add_columns(Cookie.cookie_name,LineItem.quantity,... LineItem.extended_cost... )... query=query.join(LineItem).join(Cookie)... if shipped is not None:... query= query.where(Order.shipped == shipped)... results= query.filter(User.username== cust_name).all()... return results... &gt;&gt;&gt; &gt;&gt;&gt; query= session.query(Order.order_id, User.username, User.phone)&gt;&gt;&gt; query= query.join(User)&gt;&gt;&gt; print(query)SELECT orders.order_id AS orders_order_id, users.username AS users_username, users.phone AS users_phone FROM orders JOIN users ON users.user_id = orders.user_id&gt;&gt;&gt; query=query.join(LineItem).join(Cookie)&gt;&gt;&gt; print(query)SELECT orders.order_id AS orders_order_id, users.username AS users_username, users.phone AS users_phone FROM orders JOIN users ON users.user_id = orders.user_id JOIN line_items ON orders.order_id = line_items.order_id JOIN cookies ON cookies.cookie_id = line_items.cookie_id&gt;&gt;&gt; Raw Queries12345from sqlalchemy import textquery = session.query(User).filter(text("username='cookiemon'"))print(query.all())[User(username='cookiemon', email_address='mon@cookie.com', phone='111-111-1111', password='password')]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C06_Defining Schema with SQLAlchemy ORM]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C06-Defining-Schema-with-SQLAlchemy-ORM%2F</url>
    <content type="text"><![CDATA[In SQLAlchemy Core, we created a metadata container and then declared a Table object associated with that metadata. In SQLAlchemy ORM, we are going to define a class that inherits from a special base class called the declarative_base. The declarative_base combines a metadata container and a mapper that maps our class to a database table. It also maps instances of the class to records in that table if they have been saved. Inherit from the declarative_base object. Contain __tablename__, which is the table name to be used in the database. Contain one or more attributes that are Column objects. Ensure one or more attributes make up a primary key. Defining Tables via ORM Classes 12345678910111213141516171819202122232425262728293031from sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Table, Column, Integer, Numeric, String, BooleanBase = declarative_base()class Cookie(Base): __tablename__ = 'cookies' cookie_id = Column(Integer(), primary_key=True) cookie_name = Column(String(50), index=True) cookie_recipe_url = Column(String(255)) cookie_sku = Column(String(55)) quantity = Column(Integer()) unit_cost = Column(Numeric(12, 2))Cookie.__table__Table('cookies', MetaData(bind=None), Column('cookie_id', Integer(), table=&lt;cookies&gt;, primary_key=True, nullable=False), Column('cookie_name', String(length=50), table=&lt;cookies&gt;), Column('cookie_recipe_url', String(length=255), table=&lt;cookies&gt;), Column('cookie_sku', String(length=55), table=&lt;cookies&gt;), Column('quantity', Integer(), table=&lt;cookies&gt;), Column('unit_cost', Numeric(precision=12, scale=2), table=&lt;cookies&gt;), schema=None)from datetime import datetimefrom sqlalchemy import DateTimeclass User(Base): __tablename__ = 'users' user_id = Column(Integer(), primary_key=True) username = Column(String(15), nullable=False, unique=True) email_address = Column(String(255), nullable=False) phone = Column(String(20), nullable=False) password = Column(String(25), nullable=False) created_on = Column(DateTime(), default=datetime.now) updated_on = Column(DateTime(), default=datetime.now, onupdate=datetime.now) Keys, Constraints, and IndexesWhen using the ORM, we are building classes and not using the table constructor. In the ORM, these can be added by using the __table_args__ attribute on our class. __table_args__ expects to get a tuple of additional table arguments. 123456class SomeDataClass(Base): __tablename__ = 'somedatatable' __table_args__ = (ForeignKeyConstraint(['id'], ['other_table.id']), CheckConstraint(unit_cost &gt;= 0.00', name='unit_cost_positive')) RelationshipsThe ORM uses a similar ForeignKey column to constrain and link the objects; however, it also uses a relationship directive to provide a property that can be used to access the related object. Table with a relationship 12345678910&gt;&gt;&gt; from sqlalchemy import ForeignKey, Boolean&gt;&gt;&gt; from sqlalchemy.orm import relationship, backref&gt;&gt;&gt; class Order(Base):... __tablename__='orders'... order_id = Column(Integer(), primary_key= True)... user_id = Column(Integer(), ForeignKey('users.user_id'))... shipped= Column(Boolean(), default=False)... user= relationship('User', backref=backref('orders', order_by=order_id))... &gt;&gt;&gt; Looking at the user relationship defined in the Order class, it establishes a one-to-many relationship with the User class. We can get the User related to this Order by accessing the user property. This relationship also establishes an orders property on the User class via the backref keyword argument, which is ordered by the order_id. The relationship directive needs a target class for the relationship, and can optionally include a back reference to be added to target class. SQLAlchemy knows to use the ForeignKey we defined that matches the class we defined in the relationship. In the preceding example, the ForeignKey(users.user_id), which has the users table’s user_id column, maps to the User class via the __tablename__ attribute of users and forms the relationship. More tables with relationships 12345678910&gt;&gt;&gt; class LineItem(Base):... __tablename__='line_items'... line_item_id= Column(Integer(), primary_key= True)... order_id= Column(Integer(),ForeignKey('orders.order_id'))... cookie_id=Column(Integer(), ForeignKey('cookies.cookie_id'))... quantity= Column(Integer())... extended_cost=Column(Numeric(12,2))... order = relationship('Order',backref=backref('line_items', order_by= line_item_id))... cookie=relationship('Cookie', uselist=False) # This establishes a one-to-one relationship.... Persisting the SchemaTo create our database tables, we are going to use the create_all method on the metadata within our Base instance. It requires an instance of an engine, just as it did in SQLAlchemy Core: 123&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; engine= create_engine('sqlite:///:memory:')&gt;&gt;&gt; Base.metadata.create_all(engine)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C05_Reflection]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C05-Reflection%2F</url>
    <content type="text"><![CDATA[Reflection is a technique that allows us to populate a SQLAlchemy object from an existing database. Reflecting Individual TablesFor our first reflection, we are going to generate the Artist table. We’ll need a metadata object to hold the reflected table schema information, and an engine attached to the Chinook database. Setting up our initial objects123from sqlalchemy import MetaData, create_enginemetadata = MetaData()engine = create_engine('sqlite:///Chinook_Sqlite.sqlite') Instead of defining the columns by hand, we are going to use the autoload and autoload_with keyword arguments. This will reflect the schema information into the metadata object and store a reference to the table in the artist variable. Reflecting the Artist table 12from sqlalchemy import Tableartist = Table('Artist', metadata, autoload=True, autoload_with=engine) Using the Artist table 1234567891011121314artist.columns.keys() from sqlalchemy import selects = select([artist]).limit(10) engine.execute(s).fetchall()[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')] Viewing the metadata123456789album = Table('Album', metadata, autoload=True, autoload_with=engine)metadata.tables['album']Table('album', MetaData(bind=None), Column('AlbumId', INTEGER(), table=&lt;album&gt;, primary_key=True, nullable=False), Column('Title', NVARCHAR(length=160), table=&lt;album&gt;, nullable=False), Column('ArtistId', INTEGER(), table=&lt;album&gt;, nullable=False), schema=None)) the foreign key to the Artist table does not appear to have been reflected. 12in: album.foreign_keysout: set() add the missing ForeignKey, and restore the relationship:12345from sqlalchemy import ForeignKeyConstraintalbum.append_constraint( ForeignKeyConstraint(['ArtistId'], ['artist.ArtistId'])) use the relationship to join the tables properly 12str(artist.join(album))'artist JOIN album ON artist.'ArtistId' = album.'ArtistId'' Reflecting a Whole DatabaseIn order to reflect a whole database, we can use the reflect method on the metadata object. The reflect method will scan everything available on the engine supplied, and reflect everything it can. 12345metadata.reflect(bind=engine)metadata.tables.keys()dict_keys(['InvoiceLine', 'Employee', 'Invoice', 'album', 'Genre', 'PlaylistTrack', 'Album', 'Customer', 'MediaType', 'Artist', 'Track', 'artist', 'Playlist']) Query Building with Reflected ObjectsUsing a reflected table in query 1234567891011121314playlist = metadata.tables['Playlist'] from sqlalchemy import selects = select([playlist]).limit(10) engine.execute(s).fetchall()[(1, 'Music'), (2, 'Movies'), (3, 'TV Shows'), (4, 'Audiobooks'), (5, '90’s Music'), (6, 'Audiobooks'), (7, 'Movies'), (8, 'Music'), (9, 'Music Videos'), (10, 'TV Shows')]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C04_Testing]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C04-Testing%2F</url>
    <content type="text"><![CDATA[db.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141from datetime import datetimefrom sqlalchemy import MetaData, Table, Column, Integer, Numeric, String, DateTime, ForeignKey, Boolean, create_enginefrom sqlalchemy.sql import insertclass DataAccessLayer: connection = None engine = None conn_string = None metadata= MetaData() cookies= Table('cookies', metadata, Column('cookie_id',Integer(), primary_key= True), Column('cookie_name',String(50), index= True), Column('cookie_recipe_url',String(255)), Column('cookie_sku', String(55)), Column('quantity', Integer()), Column('unit_cost',Numeric(12,2)) ) users = Table('users', metadata, Column('user_id', Integer(), primary_key=True), Column('customer_number', Integer(), autoincrement=True), Column('username', String(15), nullable=False, unique=True), Column('email_address', String(255), nullable=False), Column('phone', String(20), nullable=False), Column('password', String(25), nullable=False), Column('created_on', DateTime(), default=datetime.now), Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now) ) orders = Table('orders', metadata, Column('order_id', Integer()), Column('user_id', ForeignKey('users.user_id')), Column('shipped', Boolean(), default=False) ) line_items = Table('line_items', metadata, Column('line_items_id', Integer(), primary_key=True), Column('order_id', ForeignKey('orders.order_id')), Column('cookie_id', ForeignKey('cookies.cookie_id')), Column('quantity', Integer()), Column('extended_cost', Numeric(12, 2))) def db_init(self, conn_string): self.engine= create_engine(conn_string or self.conn_string) self.metadata.create_all(self.engine) self.connection= self.engine.connect()dal= DataAccessLayer()def prep_db(): ins= dal.cookies.insert() dal.connection.execute(ins, cookie_name='dark chocolate ship', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity='1', unit_cost='0.75') inventory_list=[ &#123; 'cookie_name': 'peanut butter', 'cookie_recipe_url': 'http://some.aweso.me/cookie/peanut.html', 'cookie_sku': 'PB01', 'quantity': '24', 'unit_cost': '0.25' &#125;, &#123; 'cookie_name': 'oatmeal raisin', 'cookie_recipe_url': 'http://some.okay.me/cookie/raisin.html', 'cookie_sku': 'EWW01', 'quantity': '100', 'unit_cost': '1.00' &#125; ] dal.connection.execute(ins, inventory_list) customer_list=[ &#123; 'username': 'cookiemon', 'email_address': 'mon@cookie.com', 'phone': '111-111-1111', 'password': 'password' &#125;, &#123; 'username': 'cakeeater', 'email_address': 'cakeeater@cake.com', 'phone': '222-222-2222', 'password': 'password' &#125;, &#123; 'username': 'pieguy', 'email_address': 'guy@pie.com', 'phone': '333-333-3333', 'password': 'password' &#125; ] ins=dal.users.insert() dal.connection.execute(ins, customer_list) ins= insert(dal.orders).values(user_id=1, order_id='wlk001') dal.connection.execute(ins) ins=insert(dal.line_items) order_items=[ &#123; 'order_id': 'wlk001', 'cookie_id': 1, 'quantity': 2, 'extended_cost': 1.00 &#125;, &#123; 'order_id': 'wlk001', 'cookie_id': 3, 'quantity': 12, 'extended_cost': 3.00 &#125; ] dal.connection.execute(ins, order_items) ins= insert(dal.orders).values(user_id =2, order_id='o1001') dal.connection.execute(ins) ins=insert(dal.line_items) order_items=[ &#123; 'order_id': 'ol001', 'cookie_id': 1, 'quantity': 24, 'extended_cost': 12.00 &#125;, &#123; 'order_id': 'ol001', 'cookie_id': 4, 'quantity': 6, 'extended_cost': 6.00 &#125; ] dal.connection.execute(ins, order_items) app.py 1234567891011121314151617181920212223242526272829from db import dalfrom sqlalchemy.sql import selectdef get_orders_by_customer(cust_name, shipped=None, details=False): columns=[ dal.orders.c.order_id, dal.users.c.username, dal.users.c.phone ] joins=dal.users.join(dal.orders) if details: columns.extend([ dal.cookies.c.cookie_name, dal.line_items.c.quantity, dal.line_items.c.extended_cost ]) joins=joins.join(dal.line_items).join(dal.cookies) cust_orders=select(columns).select_from( joins ).where( dal.users.c.username == cust_name ) if shipped is not None: cust_orders=cust_orders.where( dal.orders.c.shipped == shipped ) return dal.connection.execute(cust_orders).fetchall() _test_app.py_ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import unittestfrom decimal import Decimalfrom db import dal, prep_dbfrom app import get_orders_by_customerclass TestApp(unittest.TestCase): @classmethod def setUpClass(cls): dal.db_init('sqlite:///:memory:') prep_db() def test_orders_by_customer_blank(self): results= get_orders_by_customer('') self.assertEqual(results,[]) def test_orders_by_customer_blank_shipped(self): results=get_orders_by_customer('',True) self.assertEqual(results, []) def test_orders_by_customer_blank_notshipped(self): results = get_orders_by_customer('', False) self.assertEqual(results, []) def test_orders_by_customer_blank_details(self): results = get_orders_by_customer('', details=True) self.assertEqual(results, []) def test_orders_by_customer_blank_shipped_details(self): results = get_orders_by_customer('', True, True) self.assertEqual(results, []) def test_orders_by_customer_blank_notshipped_details(self): results = get_orders_by_customer('', False, True) self.assertEqual(results, []) def test_orders_by_customer(self): expected_results = [(u'wlk001', u'cookiemon', u'111-111-1111')] results = get_orders_by_customer('cookiemon') self.assertEqual(results, expected_results) def test_orders_by_customer_shipped_only(self): results = get_orders_by_customer('cookiemon', True) self.assertEqual(results, []) def test_orders_by_customer_unshipped_only(self): expected_results = [(u'wlk001', u'cookiemon', u'111-111-1111')] results = get_orders_by_customer('cookiemon', False) self.assertEqual(results, expected_results) def test_orders_by_customer_with_details(self): expected_results = [ (u'wlk001', u'cookiemon', u'111-111-1111', u'dark chocolate chip', 2, Decimal('1.00')), (u'wlk001', u'cookiemon', u'111-111-1111', u'oatmeal raisin', 12, Decimal('3.00')) ] results = get_orders_by_customer('cookiemon', details=True) self.assertEqual(results, expected_results) def test_orders_by_customer_shipped_only_with_details(self): results = get_orders_by_customer('cookiemon', True, True) self.assertEqual(results, []) def test_orders_by_customer_unshipped_only_details(self): expected_results = [ (u'wlk001', u'cookiemon', u'111-111-1111', u'dark chocolate chip', 2, Decimal('1.00')), (u'wlk001', u'cookiemon', u'111-111-1111', u'oatmeal raisin', 12, Decimal('3.00')) ] results = get_orders_by_customer('cookiemon', False, True) self.assertEqual(results, expected_results)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C03_Exceptions and Transaction]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C03-Exceptions-and-Transaction%2F</url>
    <content type="text"><![CDATA[AttributeErrorAttributeErrors occur when you try to access an attribute of an object that isn’t present on that object.Causing an AttributeError 1234567891011121314151617&gt;&gt;&gt; from sqlalchemy import select, insert&gt;&gt;&gt; ins= insert(users).values(... username='cookiemon',... email_address='mon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; result=connection.execute(ins)&gt;&gt;&gt; s=select([users.c.username])&gt;&gt;&gt; results=connection.execute(s)&gt;&gt;&gt; for result in results:... print(result.username,result.password)... Traceback (most recent call last): File "&lt;stdin&gt;", line 2, in &lt;module&gt;AttributeError: Could not locate column in row for column 'password' In this case, it is because our row from the ResultProxy does not have a password column. We only queried for the username. IntegrityErrorAnother common SQLAlchemy error is the IntegrityError, which occurs when we try to do something that would violate the constraints configured on a Column or Table. 1234567891011121314&gt;&gt;&gt; s=select([users.c.username])&gt;&gt;&gt; connection.execute(s).fetchall()[('cookiemon',)]&gt;&gt;&gt; ins= insert(users).values(... username= 'cookiemon',... email_address='damon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; results=connection.execute(ins)Traceback (most recent call last):...sqlite3.IntegrityError: UNIQUE constraint failed: users.username handling ErrorsCatching an exception 123456789101112&gt;&gt;&gt; from sqlalchemy.exc import IntegrityError&gt;&gt;&gt; ins= insert(users).values(... username= 'cookiemon',... email_address='damon@cookie.com',... phone='111-111-1111',... password='password'... )... &gt;&gt;&gt; try: result= connection.execute(ins)... except IntegrityError as error:... print(error.orig.message, error.params)... TransactionsWhen we start a transaction, we record the current state of our database; then we can execute multiple SQL statements. If all the SQL statements in the transaction succeed, the database continues on normally and we discard the prior database state. However, if one or more of those statements fail, we can catch that error and use the prior state to roll back back any statements that succeeded. Transactions are initiated by calling the begin() method on the connection object. The result of this call is a transaction object that we can use to control the result of all our statements. If all our statements are successful, we commit the transaction by calling the commit() method on the transaction object. If not, we call the rollback() method on that same object. Let’s rewrite the ship_it function to use a transaction to safely execute our statements. _Transactional ship_it_123456789101112131415161718192021from sqlalchemy.exc import IntegrityError def ship_it(order_id): s = select([line_items.c.cookie_id, line_items.c.quantity]) s = s.where(line_items.c.order_id == order_id) transaction = connection.begin() cookies_to_ship = connection.execute(s).fetchall() try: for cookie in cookies_to_ship: u = update(cookies).where(cookies.c.cookie_id == cookie.cookie_id) u = u.values(quantity = cookies.c.quantity-cookie.quantity) result = connection.execute(u) u = update(orders).where(orders.c.order_id == order_id) u = u.values(shipped=True) result = connection.execute(u) print('Shipped order ID: &#123;&#125;'.format(order_id)) transaction.commit() except IntegrityError as error: transaction.rollback() print(error)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C02_Built-In SQL Functions and Labels]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C02-Built-In-SQL-Functions-and-Labels%2F</url>
    <content type="text"><![CDATA[Two very commonly used database functions are SUM() and COUNT(). To use these functions, we need to import the sqlalchemy.sql.func module where they are found. These functions are wrapped around the column(s) on which they are operating.Summing our cookies 12345from sqlalchemy.sql import funcs= select([func.sum(cookies.c.quantity)])rp= connection.execute(s)print(rp.scalar())# Notice the use of scalar, which will return only the leftmost column in the first record.This results in: 137 Counting our inventory records 12345s= select([func.count(cookies.c.cookie_name)])rp= connection.execute(s)record= rp.first()print(record.keys())print(record.count_1) # The column name is autogenerated and is commonly &lt;func_name&gt;_&lt;position&gt; Renaming our count column123456s= select([func.count(cookies.c.cookie_name).label('inventory_count')])rp= connection.execute(s)record= rp.first()print(record.keys())print(record.inventory_count) FilteringFiltering queries is done by adding where() statements just like in SQL. A typical where() clause has a column, an operator, and a value or column. It is possible to chain multiple where() clauses together, and they will act like ANDs in traditional SQL statements. 12345s= select([cookies]).where( cookies.c.cookie_name == 'chocolate chip')rp= connection.execute(s)record= rp.first()print(record.items())# calling the items() method on the row object, which will give a list of columns and values. Finding names with chocolate in them 1234s= select([cookies]).where( cookies.c.cookie_name.like('%chocolate%'))rp= connection.execute(s)for record in rp.fetchall(): print(record.cookie_name) String concatenation with +123s= select([cookies.c.cookie_name, 'SKU-'+ cookies.c.cookie_sku])for row in connection.execute(s): print(row) Inventory value by cookie 123456from sqlalchemy import casts= select([... cookies.c.cookie_name,... cast((cookies.c.quantity* cookies.c.unit_cost),Numeric(12,2)).label('inv_cost')... ])for row in connection.execute(s): print('&#123;&#125; - &#123;&#125;'.format(row.cookie_name, row.inv_cost)) Using the and() conjunction 12345678&gt;&gt;&gt; from sqlalchemy import and_, or_, not_&gt;&gt;&gt; s= select([cookies]).where(... and_(... cookies.c.quantity &gt; 23,... cookies.c.unit_cost &lt; 0.40... )... )for row in connection.execute(s): print(row.cookie_name) Using the or() conjunction 12345678from sqlalchemy import and_, or_, not_&gt;&gt;&gt; s= select([cookies]).where(... or_(... cookies.c.quantity.between(10, 50),... cookies.c.cookie_name.contains('chip')... )... )for row in connection.execute(s): print(row.cookie_name) Updating DataUpdating data123456789from sqlalchemy import updateu= update(cookies).where(cookies.c.cookie_name == 'chocolate chip')u=u.values(quantity=(cookies.c.quantity + 120))result= connection.execute(u)print(result.rowcount)s=select([cookies]).where(cookies.c.cookie_name == 'chocolate chip')result= connection.execute(s).first()for key in result.keys(): print('&#123;:&gt;20&#125;: &#123;&#125;'.format(key, result[key])) Deleting Data Deleting data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from sqlalchemy import deleteu=delete(cookies).where(cookies.c.cookie_name == 'dark chocolate chip')result= connection.execute(u)print(result.rowcount)customer_list = [ &#123; 'username': 'cookiemon', 'email_address': 'mon@cookie.com', 'phone': '111-111-1111', 'password': 'password' &#125;, &#123; 'username': 'cakeeater', 'email_address': 'cakeeater@cake.com', 'phone': '222-222-2222', 'password': 'password' &#125;, &#123; 'username': 'pieguy', 'email_address': 'guy@pie.com', 'phone': '333-333-3333', 'password': 'password' &#125;]ins = users.insert()result = connection.execute(ins, customer_list)# Now that we have customers, we can start to enter their orders and line items into the system as well:ins= insert(orders).values(user_id =1, order_id =1)result= connection.execute(ins)ins= insert(line_items)order_items = [ &#123; 'order_id': 1, 'cookie_id': 1, 'quantity': 2, 'extended_cost': 1.00 &#125;, &#123; 'order_id': 1, 'cookie_id': 3, 'quantity': 12, 'extended_cost': 3.00 &#125;]result= connection.execute(ins, order_items)ins= insert(orders).values(user_id =2, order_id = 2)result- connection.execute(ins)ins=insert(line_items)order_items = [ &#123; 'order_id': 2, 'cookie_id': 1, 'quantity': 24, 'extended_cost': 12.00 &#125;, &#123; 'order_id': 2, 'cookie_id': 4, 'quantity': 6, 'extended_cost': 6.00 &#125;]result= connection.execute(ins, order_items) Joins12345678910111213141516171819&gt;&gt;&gt; columns=[orders.c.order_id,... users.c.username,... users.c.phone,... cookies.c.cookie_name,... line_items.c.quantity,... line_items.c.extended_cost... ]&gt;&gt;&gt; cookiemon_orders=select(columns)... &gt;&gt;&gt; &gt;&gt;&gt; cookiemon_orders=cookiemon_orders.select_from(... orders.join(users).join(line_items).join(cookies)... ).where(users.c.username == 'cookiemon')... &gt;&gt;&gt; print(cookiemon_orders)SELECT orders.order_id, users.username, users.phone, cookies.cookie_name, line_items.quantity, line_items.extended_cost FROM orders JOIN users ON users.user_id = orders.user_id JOIN line_items ON orders.order_id = line_items.order_id JOIN cookies ON cookies.cookie_id = line_items.cookie_id WHERE users.username = :username_1&gt;&gt;&gt; using outerjoin to select from multiple tables 12345678910111213141516&gt;&gt;&gt; from sqlalchemy import func&gt;&gt;&gt; columns=[... users.c.username,... func.count(orders.c.order_id)... ]... &gt;&gt;&gt; all_orders=select(columns).select_from(... users.outerjoin(orders)... ).group_by(users.c.username)... # SQLAlchemy knows how to join the users and orders tables because of the foreign key defined in the orders table.&gt;&gt;&gt; print(all_orders)SELECT users.username, count(orders.order_id) AS count_1 FROM users LEFT OUTER JOIN orders ON users.user_id = orders.user_id GROUP BY users.username AliasesWhen using joins, it is often necessary to refer to a table more than once. In SQL, this is accomplished by using aliases in the query. For instance, suppose we have the following (partial) schema that tracks the reporting structure within an organization:123456employee_table = Table( 'employee', metadata, Column('id', Integer, primary_key=True), Column('manager', None, ForeignKey('employee.id')), Column('name', String(255))) Now suppose we want to select all the employees managed by an employee named Fred. In SQL, we might write the following:12345SELECT employee.nameFROM employee, employee AS managerWHERE employee.manager_id = manager.id AND manager.name = 'Fred' SQLAlchemy also allows the use of aliasing selectables in this type of situation via the alias() function or method: 12345678&gt;&gt;&gt; manager = employee_table.alias('mgr')&gt;&gt;&gt; stmt = select([employee_table.c.name],... and_(employee_table.c.manager_id==manager.c.id,... manager.c.name=='Fred'))&gt;&gt;&gt; print(stmt)SELECT employee.nameFROM employee, employee AS mgrWHERE employee.manager_id = mgr.id AND mgr.name = ? SQLAlchemy can also choose the alias name automatically, which is useful for guaranteeing that there are no name collisions: 12345678&gt;&gt;&gt; manager = employee_table.alias()&gt;&gt;&gt; stmt = select([employee_table.c.name],... and_(employee_table.c.manager_id==manager.c.id,... manager.c.name=='Fred'))&gt;&gt;&gt; print(stmt)SELECT employee.nameFROM employee, employee AS employee_1WHERE employee.manager_id = employee_1.id AND employee_1.name = ? GroupingWhen using grouping, you need one or more columns to group on and one or more columns that it makes sense to aggregate with counts, sums, etc. Grouping data 12345678&gt;&gt;&gt; columns=[... users.c.username,... func.count(orders.c.order_id)... ]... &gt;&gt;&gt; all_orders=select(columns).select_from(users.outerjoin(orders)).group_by(... users.c.username... ) ChainingChaining 1234567891011121314151617&gt;&gt;&gt; def get_orders_by_customer(cust_name):... columns=[... orders.c.order_id,... users.c.username,... usere.c.phone,... cookies.c.cookie_name,... line_items.c.quantity.... line_items.c.extended_cost... ]... cust_orders=select(columns).select_from(... users.join(orders).join(line_items).join(cookies)... ).where(... users.c.username == cust_name... )... result= connection.execute(cust_orders).fetchall()... return result... Conditional chaining 1234567891011121314151617181920212223&gt;&gt;&gt; def get_orders_by_customer(cust_name, shipped=None, details= False):... columns=[... orders.c.order_id,... users.c.username,... users.c.phone... ]... joins=users.join(orders)... if details:... columns.extend([... cookies.c.cookie_name,... line_items.c.quantity,... line_items.c.extended_cost... ])... joins=joins.join(line_items).join(cookies)... cust_orders=select(columns).select_from(joins).where(... users.c.username == cust_name... )... if shipped is not None:... cust_orders=cust_orders.where(orders.c.shipped == shipped)... result= connection.execute(cust_orders).fetchall()... return result... &gt;&gt;&gt; Raw QueriesIt is also possible to execute raw SQL statements or use raw SQL in part of a SQLAlchemy Core query. It still returns a ResultProxy, and you can continue to interact with it just as you would a query built using the SQL Expression syntax of SQLAlchemy Core. Full raw queries 12result= connection.execute('select * from orders').fetchall()print(result) Partial text query1234&gt;&gt;&gt; from sqlalchemy import text&gt;&gt;&gt; stmt= select([users]).where(text('username="cookiemon"'))print(connection.execute(stmt).fetchall())]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C02_Inserting Data_Querying Data]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C02-Inserting-Data-Querying-Data%2F</url>
    <content type="text"><![CDATA[Inserting Data12345678910&gt;&gt;&gt; ins= cookies.insert().values(... cookie_name='chololate chip',... cookie_recipe_url='http://some.aweso.me/cookie/recipe.html',... quantity= '12',... unit_cost= '0.50')... &gt;&gt;&gt; print(str(ins))INSERT INTO cookies (cookie_name, cookie_recipe_url, quantity, unit_cost) VALUES (:cookie_name, :cookie_recipe_url, :quantity, :unit_cost)&gt;&gt;&gt; The compile() method on the ins object returns a SQLCompiler object that gives us access to the actual parameters that will be sent with the query via the params attribute: 1234&gt;&gt;&gt; ins.compile().params&#123;'cookie_recipe_url': 'http://some.aweso.me/cookie/recipe.html', 'quantity': '12', 'cookie_name': 'chololate chip', 'unit_cost': '0.50'&#125;&gt;&gt;&gt; This compiles the statement via our dialect but does not execute it, and we access the params attribute of that statement. We can use the execute() method on our connection to send the statement to the database.We can also get the ID of the record we just inserted by accessing the inserted_primary_key attribute: 12result= connection.execute(ins)result.inserted_primary_key In addition to having insert as an instance method off a Table object, it is also available as a top-level function for those times that you want to build a statement “generatively” (a step at a time) or when the table may not be initially known. 12345678&gt;&gt;&gt; from sqlalchemy import insert&gt;&gt;&gt; ins= insert(cookies).values(... cookie_name='chocolate chip',... cookie_recipe_url='recipe.html',... quantity='12',... unit_cost = '0.50')... &gt;&gt;&gt; The execute method of the connection object can take more than just statements. It is also possible to provide the values as keyword arguments to the execute method after our statement. When the statement is compiled, it will add each one of the keyword argument keys to the columns list, and it adds each one of their values to the VALUES part of the SQL statement 12345678910ins = cookies.insert()result = connection.execute( ins, cookie_name='dark chocolate chip', cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html', cookie_sku='CC02', quantity='1', unit_cost='0.75')result.inserted_primary_key Multiple inserts 1234567891011121314151617inventory_list = [ &#123; 'cookie_name': 'peanut butter', 'cookie_recipe_url': 'http://some.aweso.me/cookie/peanut.html', 'cookie_sku': 'PB01', 'quantity': '24', 'unit_cost': '0.25' &#125;, &#123; 'cookie_name': 'oatmeal raisin', 'cookie_recipe_url': 'http://some.okay.me/cookie/raisin.html', 'cookie_sku': 'EWW01', 'quantity': '100', 'unit_cost': '1.00' &#125;]result = connection.execute(ins, inventory_list) Querying DataTo begin building a query, we start by using the select function, which is analogous to the standard SQL SELECT statement. Initially, let’s select all the records in our cookies table12345from sqlalchemy.sql import selects = select([cookies]) rp = connection.execute(s)results = rp.fetchall() # This tells rp, the ResultProxy, to return all the rows Remember we can use str(s) to look at the SQL statement the database will see, which in this case is SELECT cookies.cookie_id, cookies.cookie_name, cookies.cookie_recipe_url, cookies.cookie_sku, cookies.quantity, cookies.unit_cost FROM cookies. It is also possible to use the select method on the Table object to do this. 1234from sqlalchemy.sql import selects=cookies.select()rp=connection.execute(s)results=rp.fetchall() ResultProxyA ResultProxy is a wrapper around a DBAPI cursor object, and its main goal is to make it easier to use and manipulate the results of a statement. Handling rows with a ResultProxy 123first_row = results[0]first_row.cookie_name # Access column by name.first_row[cookies.c.cookie_name] # Access column by Column object Iterating over a ResultProxy 123rp = connection.execute(s)for record in rp: print(record.cookie_name) You can use the following methods as well to fetch results: first() Returns the first record if there is one and closes the connection. fetchone() Returns one row, and leaves the cursor open for you to make additional fetch calls. scalar() Returns a single value if a query results in a single record with one column. Controlling the Columns in the Query_Select only cookie_name and quantity_ 1234s = select([cookies.c.cookie_name, cookies.c.quantity])rp= connection.execute(s)print(rp.keys()) # (u'chocolate chip', 12),result= rp.first() OrderingOrder by quantity ascending 12345s= select([cookies.c.cookie_name, cookies.c.quantity])s = s.order_by(cookies.c.quantity)rp= connection.execute(s)for cookie in rp: print('&#123;&#125; - &#123;&#125;'.format(cookie.quantity, cookie.cookie_name)) We saved the select statement into the s variable, used that s variable and added the order_by statement to it, and then reassigned that to the s variable. This is an example of how to compose statements in a generative or step-by-step fashion. This is the same as combining the select and the order_by statements into one line as shown here: s = select([...]).order_by(...) Order by quantity descending 123from sqlalchemy import descs= select([cookies.c.cookie_name, cookies.c.quantity])s= s.order_by(desc(cookies.c.quantity)) The desc() function can also be used as a method on a Column object, such as cookies.c.quantity.desc(). However, that can be a bit more confusing to read in long statements, so I always use desc() as a function. LimitingWe used the first() or fetchone() methods to get just a single row back. While our ResultProxy gave us the one row we asked for, the actual query ran over and accessed all the results, not just the single record. If we want to limit the query, we can use the limit() function to actually issue a limit statement as part of our query. Two smallest cookie inventories 12345s= select([cookies.c.cookie_name, cookies.c.quantity])s= s.order_by(cookies.c.quantity)s= s.limit(2)rp= connection.execute(s)print([result.cookie_name for result in rp])]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C01_Persisting the Tables]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C01-Persisting-the-Tables%2F</url>
    <content type="text"><![CDATA[All of our tables and additional schema definitions are associated with an instance of metadata. Persisting the schema to the database is simply a matter of calling the create_all() method on our metadata instance with the engine where it should create those tables: metadata.create_all(engine)By default, create_all will not attempt to re-create tables that already exist in the database, and it is safe to run multiple times. It’s wiser to use a database migration tool like Alembic to handle any changes to existing tables or additional schema than to try to handcode changes directly in your application code. from sqlalchemy import (MetaData, Table, Column, Integer, Numeric, String, DateTime, ForeignKey, create_engine)metadata = MetaData()engine= create_engine(‘sqlite:///:memory:’)metadata.create_all(engine)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy_C01_Keys and Constraints_Indexes_Relationships and ForeignKeyConstraints]]></title>
    <url>%2F2018%2F05%2F25%2Fsqlalchemy-C01-Keys-and-Constraints-Indexes-Relationships-and-ForeignKeyConstraints%2F</url>
    <content type="text"><![CDATA[Keys and ConstraintsKeys and constraints are used as a way to ensure that our data meets certain requirements prior to being stored in the database. The objects that represent keys and constraints can be found inside the base SQLAlchemy module, and three of the more common ones can be imported as shown here:from sqlalchemy import PrimaryKeyConstraint, UniqueConstraint, CheckConstraintThe most common key type is a primary key, which is used as the unique identifier for each record in a database table and is used used to ensure a proper relationship between two pieces of related data in different tables. As you saw earlier in Example 1-1 and Example 1-2, a column can be made a primary key simply by using the primary_key keyword argument. You can also define composite primary keys by assigning the setting primary_key to True on multiple columns. The key will then essentially be treated like a tuple in which the columns marked as a key will be present in the order they were defined in the table. Primary keys can also be defined after the columns in the table constructor, as shown in the following snippet. You can add multiple columns separated by commas to create a composite key. If we wanted to explicitly define the key as shown in Example 1-2, it would look like this:PrimaryKeyConstraint(&#39;user_id&#39;, name=&#39;user_pk&#39;) Another common constraint is the unique constraint, which is used to ensure that no two values are duplicated in a given field. For our online cookie delivery service, for example, we would want to ensure that each customer had a unique username to log into our systems. We can also assign unique constraints on columns, as shown before in the username column, or we can define them manually as shown here: UniqueConstraint(&#39;username&#39;, name=&#39;uix_username&#39;) Not shown in Example 1-2 is the check constraint type. This type of constraint is used to ensure that the data supplied for a column matches a set of user-defined criteria. In the following example, we are ensuring that unit_cost is never allowed to be less than 0.00 because every cookie costs something to make (remember from Economics 101: TINSTAAFC—that is, there is no such thing as a free cookie!): CheckConstraint(&#39;unit_cost &gt;= 0.00&#39;, name=&#39;unit_cost_positive&#39;) IndexesIndexes are used to accelerate lookups for field values, and in Example 1-1, we created an index on the cookie_name column because we know we will be searching by that often. When indexes are created as shown in that example, you will have an index called ix_cookies_cookie_name. We can also define an index using an explicit construction type. Multiple columns can be designated by separating them by a comma. You can also add a keyword argument of unique=True to require the index to be unique as well. When creating indexes explicitly, they are passed to the Table constructor after the columns. To mimic the index created in Example 1-1, we could do it explicitly as shown here:123from sqlalchemy import IndexIndex('ix_cookies_cookie_name', 'cookie_name') We can also create functional indexes that vary a bit by the backend database being used. This lets you create an index for situations where you often need to query based on some unusual context. For example, what if we want to select by cookie SKU and name as a joined item, such as SKU0001 Chocolate Chip? We could define an index like this to optimize that lookup:Index(&#39;ix_test&#39;, mytable.c.cookie_sku, mytable.c.cookie_name)) Relationships and ForeignKeyConstraints One way to implement a relationship is shown in Example 1-3 in the line_items table on the order_id column; this will result in a ForeignKeyConstraint to define the relationship between the two tables. In this case, many line items can be present for a single order. However, if you dig deeper into the line_items table, you’ll see that we also have a relationship with the cookies table via the cookie_id ForeignKey. This is because line_items is actually an association table with some additional data on it between orders and cookies. Association tables are used to enable many-to-many relationships between two other tables. A single ForeignKey on a table is typically a sign of a one-to-many relationship; however, if there are multiple ForeignKey relationships on a table, there is a strong possibility that it is an association table. 123456789101112131415161718192021222324252627282930313233343536373839from sqlalchemy import MetaDatafrom sqlalchemy import Table, Column, Integer, Numeric, String, ForeignKey, DateTime, Booleanfrom datetime import datetimemetadata= MetaData()cookies=Table('cookies', metadata, Column('cookie_id',Integer(),primary_key= True), Column('cookie_name',String(50), index= True), Column('cookie_recipe_url',String(255)), Column('quantity', Integer()), Column('unit_cost',Numeric(12,2)) )users=Table('users', metadata, Column('user_id', Integer(), primary_key= True), Column('username',String(15), nullable=False, unique=True), Column('email_address',String(255), nullable=False), Column('phone',String(20),nullable=False), Column('password',String(25),nullable=False), Column('created_on', DateTime(),default=datetime.now), Column('updated_on',datetime(),default=datetime.now,onupdate=datetime.now) )orders=Table('orders', metadata, Column('order_id', Integer(),primary_key=True), Column('user_id', ForeignKey('users.user_id')), Column('shipped',Boolean(), default=False) )line_items=Table('line_items', metadata, Column('line_items_id', Integer(), primary_key= True), Column('order_id', ForeignKey('orders.order_id')), Column('cookie_id', ForeignKey('cookies.cookie_id')), Column('quantity', Integer()), Column('extended_cost', Numeric(12,2)) )]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_Class Metaprogramming]]></title>
    <url>%2F2018%2F05%2F25%2FC21-Class-Metaprogramming%2F</url>
    <content type="text"><![CDATA[A Class Factory_record_factory.py: a simple class factory_ 1234567891011121314151617181920212223242526272829def record_factory(cls_name, field_names): try: field_names= field_names.replace(',',' ').split() except AttributeError: pass field_names= tuple(field_names) def __init__(self, *args, **kwargs): attrs=dict(zip(self.__slots__, args)) attrs.update(kwargs) for name, value in attrs.items(): setattr(self, name, value) def __iter__(self): for name in self.__slots__: yield getattr(self, name) def __repr__(self): values=', '.join('&#123;&#125;=&#123;!r&#125;'.format(*i) for i in zip(self.__slots__, self)) return '&#123;&#125;(&#123;&#125;)'.format(self.__class__.__name__, values) cls_attrs=dict( __slots__= field_names, __init__= __init__, __iter__= __iter__, __repr__= __repr__ ) return type(cls_name, (object,), cls_attrs) We usually think of type as a function, because we use it like one, e.g., type(my_object) to get the class of the object—same as my_object.__class__. However, type is a class. It behaves like a class that creates a new class when invoked with three arguments: 12MyClass = type('MyClass', (MySuperClass, MyMixin), &#123;'x': 42, 'x2': lambda self: self.x * 2&#125;) The three arguments of type are named name, bases, and dict—the latter being a mapping of attribute names and attributes for the new class. The preceding code is functionally equivalent to this: 1234class MyClass(MySuperClass, MyMixin): x = 42 def x2(self): return self.x * 2 The novelty here is that the instances of type are classes, like MyClass here. In summary, the last line of record_factory builds a class named by the value of cls_name, with object as its single immediate superclass and with class attributes named __slots__, __init__, __iter__, and __repr__, of which the last three are instance methods. Instances of classes created by record_factory have a limitation: they are not serializable—that is, they can’t be used with the dump/load functions from the pickle module. A Class Decorator for Customizing DescriptorsA class decorator is very similar to a function decorator: it’s a function that gets a class object and returns the same class or a modified one. _bulkfood_v6.py: LineItem using Quantity and NonBlank descriptors_ 123456789101112131415import model_v6 as model@model.entityclass LineItem: description= model.NonBlank() weight= model.Quantity() price= model.Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price _model_v6.py: a class decorator_ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def entity(cls): for key, attr in cls.__dict__.items(): if isinstance(attr, Validated): type_name = type(attr).__name__ attr.storage_name = '_&#123;&#125;#&#123;&#125;'.format(type_name, key) return clsimport abcclass AutoStorage: __counter = 0 def __init__(self): cls= self.__class__ prefix= cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.storage_name) def __set__(self, instance,value): setattr(instance, self.storage_name, value)class Validated(abc.ABC, AutoStorage): #__set__ delegates validation to a validate method def __set__(self, instance, value): value= self.validate(instance, value) super().__set__(instance, value) # uses the returned value to invoke __set__ on a superclass, which # performs the actual storage. @abc.abstractmethod def validate(self, instance, value): '''return validated value or raise ValueError'''class Quantity(Validated): def validate(self, instance, value): if value &lt; 0: raise ValueError('value must be &gt; 0') return valueclass NonBlank(Validated): def validate(self, instance, value): value = value.strip() if len(value) == 0: raise ValueError('value cannot be empty or blank') return value _bulkfood_v6.py: doctests for new storage_name descriptor attributes_ 123456789&gt;&gt;&gt; raisins = LineItem('Golden raisins', 10, 6.95) &gt;&gt;&gt; dir(raisins)[:3] ['_NonBlank#description', '_Quantity#price', '_Quantity#weight'] &gt;&gt;&gt; LineItem.description.storage_name '_NonBlank#description' &gt;&gt;&gt; raisins.description 'Golden raisins' &gt;&gt;&gt; getattr(raisins, '_NonBlank#description') 'Golden raisins' A significant drawback of class decorators is that they act only on the class where they are directly applied. This means subclasses of the decorated class may or may not inherit the changes made by the decorator, depending on what those changes are. What Happens When: Import Time Versus RuntimeAlthough compiling is definitely an import-time activity, other things may happen at that time, because almost every statement in Python is executable in the sense that they potentially run user code and change the state of the user program. In particular, the import statement is not merely a declaration but it actually runs all the top-level code of the imported module when it’s imported for the first time in the process—further imports of the same module will use a cache, and only name binding occurs then. That top-level code may do anything, including actions typical of “runtime”, such as connecting to a database. That’s why the border between “import time” and “runtime” is fuzzy: the import statement can trigger all sorts of “runtime” behavior. The interpreter executes a def statement on the top level of a module when the module is imported, but what does that achieve? The interpreter compiles the function body (if it’s the first time that module is imported), and binds the function object to its global name, but it does not execute the body of the function, obviously. In the usual case, this means that the interpreter defines top-level functions at import time, but executes their bodies only when—and if—the functions are invoked at runtime. For classes, the story is different: at import time, the interpreter executes the body of every class, even the body of classes nested in other classes. Execution of a class body means that the attributes and methods of the class are defined, and then the class object itself is built. In this sense, the body of classes is “top-level code”: it runs at import time. Metaclasses 101Consider the Python object model: classes are objects, therefore each class must be an instance of some other class. By default, Python classes are instances of type. In other words, type is the metaclass for most built-in and user-defined classes: 123456789&gt;&gt;&gt; 'spam'.__class__&lt;class 'str'&gt;&gt;&gt;&gt; str.__class__&lt;class 'type'&gt;&gt;&gt;&gt; from bulkfood_v6 import LineItem&gt;&gt;&gt; LineItem.__class__&lt;class 'type'&gt;&gt;&gt;&gt; type.__class__&lt;class 'type'&gt; To avoid infinite regress, type is an instance of itself, as the last line shows. str and LineItem are instances of type. They all are subclasses of object. The classes object and type have a unique relationship: object is an instance of type, and type is a subclass of object. This relationship is “magic”: it cannot be expressed in Python because either class would have to exist before the other could be defined. The fact that type is an instance of itself is also magical. The important takeaway here is that all classes are instances of type, but metaclasses are also subclasses of type, so they act as class factories. In particular, a metaclass can customize its instances by implementing __init__. A metaclass __init__ method can do everything a class decorator can do. A Metaclass for Customizing Descriptors_model_v7.py: the EntityMeta metaclass and one instance of it, Entity model_v7.py: the EntityMeta metaclass and one instance of it,Entity_ 1234567891011class EntityMeta(type): def __init__(self, name, bases, attr_dict): super().__init__(name, bases, attr_dict) for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name= type(attr).__name__ attr.storage_name= '_&#123;&#125;#&#123;&#125;'.format(type_name, key)class Entity(metaclass=EntityMeta): '''Business entity with validated fields''' _bulkfood_v7.py: inheriting from model.Entity can work, if a metaclass is behind the scenes_ 1234567891011121314import model_v7 as modelclass LineItem(model.Entity): description= model.NonBlank() weight= model.Quantity() price= model.Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price The Metaclass __prepare__ Special MethodIn some applications it’s interesting to be able to know the order in which the attributes of a class are defined. For example, a library to read/write CSV files driven by userdefined classes may want to map the order of the fields declared in the class to the order of the columns in the CSV file. As we’ve seen, both the type constructor and the __new__ and __init__ methods of metaclasses receive the body of the class evaluated as a mapping of names to attributes. However, by default, that mapping is a dict, which means the order of the attributes as they appear in the class body is lost by the time our metaclass or class decorator can look at them. The solution to this problem is the __prepare__ special method, introduced in Python 3. This special method is relevant only in metaclasses, and it must be a class method (i.e., defined with the @classmethod decorator). The __prepare__ method is invoked by the interpreter before the__new__ method in the metaclass to create the mapping that will be filled with the attributes from the class body. Besides the metaclass as first argument, __prepare__ gets the name of the class to be constructed and its tuple of base classes, and it must return a mapping, which will be received as the last argument by __new__ and then __init__ when the metaclass builds a new class. _model_v8.py: the EntityMeta metaclass uses prepare, and Entity now has a field_names class method_ 12345678910111213141516171819202122232425262728import collectionsclass EntityMeta(type): # Return an empty OrderedDict instance, where the class attributes will be # stored. @classmethod def __prepare__(cls, name, bases): return collections.OrderedDict() def __init__(cls, name, bases, attr_dict): super().__init__(name, bases, attr_dict) cls._field_names= [] # attr_dict here is the OrderedDict obtained by the interpreter when it # called __prepare__ before calling __init__. for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name= type(attr).__name__ attr.storage_name= '_&#123;&#125;#&#123;&#125;'.format(type_name, key) cls._field_names.append(key)class Entity(metaclass=EntityMeta): '''Business entity with validated fields''' @classmethod def field_names(cls): for name in cls._field_names: yield name _bulkfood_v8.py: doctest showing the use of field_names—no changes are needed in the LineItem class; field_names is inherited from model.Entity_ 123456&gt;&gt;&gt; for name in LineItem.field_names(): ... print(name) ... description weight price Classes as Objectscls.\_bases___The tuple of base classes of the class. cls.\_qualname___A new attribute in Python 3.3 holding the qualified name of a class or function, which is a dotted path from the global scope of the module to the class definition. For example, in Example 21-6, the __qualname__ of the inner class ClassTwo is the string ‘ClassOne.ClassTwo‘, while its __name__ is just ‘ClassTwo‘. cls.\_subclasses__()_This method returns a list of the immediate subclasses of the class. The implementation uses weak references to avoid circular references between the superclass and its subclasses—which hold a strong reference to the superclasses in their __bases__ attribute. The method returns the list of subclasses that currently exist in memory. cls.mro()The interpreter calls this method when building a class to obtain the tuple of superclasses that is stored in the __mro__ attribute of the class. A metaclass can override this method to customize the method resolution order of the class under construction.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_Attribute Descriptors_02]]></title>
    <url>%2F2018%2F05%2F25%2FC20-Attribute-Descriptors-02%2F</url>
    <content type="text"><![CDATA[Overriding Versus Nonoverriding DescriptorsRecall that there is an important asymmetry in the way Python handles attributes. Reading an attribute through an instance normally returns the attribute defined in the instance, but if there is no such attribute in the instance, a class attribute will be retrieved. On the other hand, assigning to an attribute in an instance normally creates the attribute in the instance, without affecting the class at all.This asymmetry also affects descriptors, in effect creating two broad categories of descriptors depending on whether the __set__ method is defined. Overriding DescriptorA descriptor that implements the __set__ method is called an overriding descriptor, because although it is a class attribute, a descriptor implementing __set__ will override attempts to assign to instance attributes. Properties are also overriding descriptors: if you don’t provide a setter function, the default __set__ from the property class will raise AttributeError to signal that the attribute is read-only. descriptorkinds.py: simple classes for studying descriptor overriding behavior 12345678910111213141516171819202122232425262728293031323334353637383940414243def cls_name(obj_or_cls): cls= type(obj_or_cls) if cls is type: cls= obj_or_cls return cls.__name__.split('.')[-1]def display(obj): cls= type(obj) if cls in None: return '&lt;class &#123;&#125;&gt;'.format(obj.__name__) elif cls in [type(None), int]: return repr(obj) else: return '&lt;&#123;&#125; object&gt;'.format(cls_name(obj))def print_args(name, *args): pseudo_args= ', '.join(display(x) for x in args) print('-&gt; &#123;&#125;.__&#123;&#125;__(&#123;&#125;)'.format(cls_name(args[0]),name, pseudo_args))class Overriding: '''data descriptor or enforced descriptor''' def __get__(self, instance, owner): print_args('get',self, instance, owner) def __set__(self, instance, value): print_args('set', self, instance, value)class OverridingNoGet: '''without __get__''' def __set__(self, instance, value): print_args('set', self, instance, value)class NonOveriding: def __get__(self, instance, owner): print_args('get',self, instance, owner)class Managed: over= Overriding() over_no_get= OverridingNoGet() non_over= NonOveriding() def spam(self): print('-&gt; Managed.spam(&#123;&#125;)'.format(display(self))) Behavior of an overriding descriptor: obj.over is an instance of Overriding 123456789101112131415161718 &gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) &gt;&gt;&gt; Managed.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, None, &lt;class Managed&gt;) &gt;&gt;&gt; obj.over = 7 -&gt; Overriding.__set__(&lt;Overriding object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) &gt;&gt;&gt; obj.__dict__['over'] = 8 &gt;&gt;&gt; vars(obj) &#123;'over': 8&#125; &gt;&gt;&gt; obj.over -&gt; Overriding.__get__(&lt;Overriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;)# even with an instance attribute named over, the Managed.over descriptor still overrides attempts to read obj.over. Overriding Descriptor Without __get__Usually, overriding descriptors implement both __set__ and __get__, but it’s also possible to implement only __set__. In this case, only writing is handled by the descriptor. Reading the descriptor through an instance will return the descriptor object itself because there is no __get__ to handle that access. If a namesake instance attribute is created with a new value via direct access to the instance __dict__, the __set__ method will still override further attempts to set that attribute, but reading that attribute will simply return the new value from the instance, instead of returning the descriptor object. In other words, the instance attribute will shadow the descriptor, but only when reading. _Overriding descriptor without get: obj.over_no_get is an instance of OverridingNoGet_ 123456789101112131415&gt;&gt;&gt; obj.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; Managed.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; obj.over_no_get = 7 -&gt; OverridingNoGet.__set__(&lt;OverridingNoGet object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over_no_get &lt;__main__.OverridingNoGet object at 0x665bcc&gt; &gt;&gt;&gt; obj.__dict__['over_no_get'] = 9 &gt;&gt;&gt; obj.over_no_get 9 &gt;&gt;&gt; obj.over_no_get = 7 -&gt; OverridingNoGet.__set__(&lt;OverridingNoGet object&gt;, &lt;Managed object&gt;, 7) &gt;&gt;&gt; obj.over_no_get 9 Nonoverriding DescriptorIf a descriptor does not implement __set__, then it’s a nonoverriding descriptor. Setting an instance attribute with the same name will shadow the descriptor, rendering it ineffective for handling that attribute in that specific instance. Methods are implemented as nonoverriding descriptors. _Behavior of a nonoverriding descriptor: obj.non_over is an instance of NonOverriding_ 1234567891011121314151617&gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;)# The obj now has an instance attribute named non_over, which shadows the namesake descriptor # attribute in the Managed class. &gt;&gt;&gt; obj.non_over = 7 &gt;&gt;&gt; obj.non_over 7 &gt;&gt;&gt; Managed.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, None, &lt;class Managed&gt;) &gt;&gt;&gt; del obj.non_over &gt;&gt;&gt; obj.non_over -&gt; NonOverriding.__get__(&lt;NonOverriding object&gt;, &lt;Managed object&gt;, &lt;class Managed&gt;) Python contributors and authors use different terms when discussing these concepts. Overriding descriptors are also called data descriptors or enforced descriptors. Nonoverriding descriptors are also known as nondata descriptors or shadowable descriptors. Overwriting a Descriptor in the ClassRegardless of whether a descriptor is overriding or not, it can be overwritten by assignment to the class. Example 20-12 Any descriptor can be overwritten on the class itself 123456&gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; Managed.over = 1 &gt;&gt;&gt; Managed.over_no_get = 2 &gt;&gt;&gt; Managed.non_over = 3 &gt;&gt;&gt; obj.over, obj.over_no_get, obj.non_over (1, 2, 3) Example 20-12 reveals another asymmetry regarding reading and writing attributes: although the reading of a class attribute can be controlled by a descriptor with __get__ attached to the managed class, the writing of a class attribute cannot be handled by a descriptor with __set__ attached to the same class. Methods Are DescriptorsA function within a class becomes a bound method because all user-defined functions have a __get__ method, therefore they operate as descriptors when attached to a class. A method is a nonoverriding descriptor12345678910 &gt;&gt;&gt; obj = Managed() &gt;&gt;&gt; obj.spam # Reading from obj.spam retrieves a bound method object. &lt;bound method Managed.spam of &lt;descriptorkinds.Managed object at 0x74c80c&gt;&gt; &gt;&gt;&gt; Managed.spam # But reading from Managed.spam retrieves a function. &lt;function Managed.spam at 0x734734&gt; &gt;&gt;&gt; obj.spam = 7 &gt;&gt;&gt; obj.spam 7# Assigning a value to obj.spam shadows the class attribute, rendering the spam method inaccessible # from the obj instance. Because functions do not implement __set__, they are nonoverriding descriptors. obj.spam and Managed.spam retrieve different objects. As usual with descriptors, the __get__ of a function returns a reference to itself when the access happens through the managed class. But when the access goes through an instance, the __get__ of the function returns a bound method object: a callable that wraps the function and binds the managed instance (e.g., obj) to the first argument of the function (i.e., self), like the functools.partial function does. _method_is_descriptor.py: a Text class, derived from UserString_12345678910import collectionsclass Text(collections.UserString): def __repr__(self): return 'Text(&#123;!r&#125;)'.format(self.data) def reverse(self): return self[::-1] Experiments with a method12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; word = Text('forward') &gt;&gt;&gt; word Text('forward') &gt;&gt;&gt; word.reverse() Text('drawrof') &gt;&gt;&gt; Text.reverse(Text('backward')) Text('drawkcab') &gt;&gt;&gt; type(Text.reverse), type(word.reverse) (&lt;class 'function'&gt;, &lt;class 'method'&gt;)# Text.reverse operates as a function, even working with objects that are not instances of Text. &gt;&gt;&gt; list(map(Text.reverse, ['repaid', (10, 20, 30), Text('stressed')])) ['diaper', (30, 20, 10), Text('desserts')]# Any function is a nonoverriding descriptor. Calling its __get__ with an instance retrieves a method # bound to that instance. &gt;&gt;&gt; Text.reverse.__get__(word) &lt;bound method Text.reverse of Text('forward')&gt;# Calling the function’s __get__ with None as the instance argument retrieves the function itself. &gt;&gt;&gt; Text.reverse.__get__(None, Text) &lt;function Text.reverse at 0x101244e18&gt;# The expression word.reverse actually invokes Text.reverse.__get__(word), returning the bound method. &gt;&gt;&gt; word.reverse &lt;bound method Text.reverse of Text('forward')&gt;# The bound method object has a __self__ attribute holding a reference to the instance on which the # method was called. &gt;&gt;&gt; word.reverse.__self__ Text('forward')# The __func__ attribute of the bound method is a reference to the original function attached to the # managed class. &gt;&gt;&gt; word.reverse.__func__ is Text.reverse True The bound method object also has a __call__ method, which handles the actual invocation. This method calls the original function referenced in __func__, passing the __self__ attribute of the method as the first argument. That’s how the implicit binding of the conventional self argument works. The way functions are turned into bound methods is a prime example of how descriptors are used as infrastructure in the language. Descriptor Usage TipsThe following list addresses some practical consequences of the descriptor characteristics just described: Use property to Keep It SimpleThe property built-in actually creates overriding descriptors implementing both __set__ and __get__, even if you do not define a setter method. The default __set__ of a property raises AttributeError: can’t set attribute, so a property is the easiest way to create a read-only attribute, avoiding the issue described next. Read-only descriptors require \_set___If you use a descriptor class to implement a read-only attribute, you must remember to code both __get__ and __set__, otherwise setting a namesake attribute on an instance will shadow the descriptor. The __set__ method of a read-only attribute should just raise AttributeError with a suitable message. Validation descriptors can work with \_set__ only_In a descriptor designed only for validation, the __set__ method should check the value argument it gets, and if valid, set it directly in the instance __dict__ using the descriptor instance name as key. That way, reading the attribute with the same name from the instance will be as fast as possible, because it will not require a __get__. Caching can be done efficiently with \_get__ only_If you code just the __get__ method, you have a nonoverriding descriptor. These are useful to make some expensive computation and then cache the result by setting an attribute by the same name on the instance. The namesake instance attribute will shadow the descriptor, so subsequent access to that attribute will fetch it directly from the instance __dict__ and not trigger the descriptor __get__ anymore. Nonspecial methods can be shadowed by instance attributesBecause functions and methods only implement __get__, they do not handle attempts at setting instance attributes with the same name, so a simple assignment like _my_obj.the_method = 7_ means that further access to the_method through that instance will retrieve the number 7—without affecting the class or other instances. However, this issue does not interfere with special methods. The interpreter only looks for special methods in the class itself, in other words, repr(x) is executed as x.__class__.__repr__(x), so a __repr__ attribute defined in x has no effect on repr(x). For the same reason, the existence of an attribute named __getattr__ in an instance will not subvert the usual attribute access algorithm.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_Attribute Descriptors_01]]></title>
    <url>%2F2018%2F05%2F25%2FC20-Attribute-Descriptors-01%2F</url>
    <content type="text"><![CDATA[Descriptors are a way of reusing the same access logic in multiple attributes. For example, field types in ORMs such as the Django ORM and SQL Alchemy are descriptors, managing the flow of data from the fields in a database record to Python object attributes and vice versa.A descriptor is a class that implements a protocol consisting of the __get__, __set__, and __delete__ methods. The property class implements the full descriptor protocol. As usual with protocols, partial implementations are OK. In fact, most descriptors we see in real code implement only __get__ and __set__, and many implement only one of these methods. Descriptor Example: Attribute ValidationWe’ll continue the series of LineItem examples where we left it, in “Coding a Property Factory” , by refactoring the quantity property factory into a Quantity descriptor class. LineItem Take # 3: A Simple DescriptorA class implementing a __get__, a __set__, or a __delete__ method is a descriptor.You use a descriptor by declaring instances of it as class attributes of another class. Descriptor classA class implementing the descriptor protocol. That’s Quantity in Figure 20-1. Managed classThe class where the descriptor instances are declared as class attributes—LineItem in Figure 20-1. _bulkfood_v3.py: quantity descriptors manage attributes in LineItem_ 1234567891011121314151617181920212223242526class Quantity: def __init__(self, storage_name): self.storage_name = storage_name # __set__ is called when there is an attempt to assign to the managed # attribute. Here, self is the descriptor instance (i.e., LineItem.weight or # LineItem.price), instance is the managed instance (a LineItem instance), and # value is the value being assigned. def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.storage_name] = value else: raise ValueError('value must be &gt; 0')class LineItem: weight= Quantity('weight') price= Quantity('price') def __init__(self, description, weight, price): self.description = description self.weight = weight self.price = price def subtotal(self): return self.weight * self.price When coding a __set__ method, you must keep in mind what the self and instance arguments mean: self is the descriptor instance, and instance is the managed instance. Descriptors managing instance attributes should store values in the managed instances. That’s why Python provides the instance argument to the descriptor methods. It may be tempting, but wrong, to store the value of each managed attribute in the descriptor instance itself. In other words, in the __set__ method, instead of coding: instance.__dict__[self.storage_name] = value the tempting but bad alternative would be: self.__dict__[self.storage_name] = value To understand why this would be wrong, think about the meaning of the first two arguments to __set__: self and instance. Here, self is the descriptor instance, which is actually a class attribute of the managed class. You may have thousands of LineItem instances in memory at one time, but you’ll only have two instances of the descriptors: LineItem.weight and LineItem.price. So anything you store in the descriptor instances themselves is actually part of a LineItem class attribute, and therefore is shared among all LineItem instances. A drawback is the need to repeat the names of the attributes when the descriptors are instantiated in the managed class body. It would be nice if the LineItem class could be declared like this: 123class LineItem: weight = Quantity() price = Quantity() The problem is that the righthand side of an assignment is executed before the variable exists. The expression Quantity() is evaluated to create a descriptor instance, and at this time there is no way the code in the Quantity class can guess the name of the variable to which the descriptor will be bound (e.g., weight or price). LineItem Take # 4: Automatic Storage Attribute NamesTo avoid retyping the attribute name in the descriptor declarations, we’ll generate a unique string for the storage_name of each Quantity instance. _bulkfood_v4.py: each Quantity descriptor gets a unique storage_name_ 123456789101112131415161718192021222324252627282930class Quantity: __counter = 0 def __init__(self): cls = self.__class__ prefix = cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): return getattr(instance, self.storage_name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.storage_name, value) else: raise ValueError('value must be &gt; 0')class LineItem: weight = Quantity() price = Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price Here we can use the higher-level getattr and setattr built-ins to store the value—instead of resorting to instance.__dict__—because the managed attribute and the storage attribute have different names, so calling getattr on the storage attribute will not trigger the descriptor, avoiding the infinite recursion. If you test bulkfood_v4.py, you can see that the weight and price descriptors work as expected, and the storage attributes can also be read directly, which is useful for debugging: 123456&gt;&gt;&gt; from bulkfood_v4 import LineItem&gt;&gt;&gt; coconuts = LineItem('Brazilian coconut', 20, 17.95)&gt;&gt;&gt; coconuts.weight, coconuts.price(20, 17.95)&gt;&gt;&gt; getattr(raisins, '_Quantity#0'), getattr(raisins, '_Quantity#1')(20, 17.95) Note that __get__ receives three arguments: self, instance, and owner. The owner argument is a reference to the managed class (e.g., LineItem), and it’s handy when the descriptor is used to get attributes from the class. If a managed attribute, such as weight, is retrieved via the class like LineItem.weight, the descriptor __get__ method receives None as the value for the instance argument. This explains the Attribute error in the next console session: 1234567&gt;&gt;&gt; from bulkfood_v4 import LineItem&gt;&gt;&gt; LineItem.weightTraceback (most recent call last): ... File ".../descriptors/bulkfood_v4.py", line 54, in __get__ return getattr(instance, self.storage_name)AttributeError: 'NoneType' object has no attribute '_Quantity#0' _bulkfood_v4b.py (partial listing): when invoked through the managed class, get returns a reference to the descriptor itself_ 123456789101112131415161718192021222324252627282930313233class Quantity: __counter = 0 def __init__(self): cls = self.__class__ prefix = cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self # If the call was not through an instance, return the descriptor itself return getattr(instance, self.storage_name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.storage_name, value) else: raise ValueError('value must be &gt; 0')class LineItem: weight = Quantity() price = Quantity() def __init__(self, description, weight, price): self.description= description self.weight= weight self.price = price def subtotal(self): return self.weight * self.price Trying out: 123456&gt;&gt;&gt; from bulkfood_v4b import LineItem&gt;&gt;&gt; LineItem.price&lt;bulkfood_v4b.Quantity object at 0x100721be0&gt;&gt;&gt;&gt; br_nuts = LineItem('Brazil nuts', 10, 34.95)&gt;&gt;&gt; br_nuts.price34.95 LineItem Take # 5: A New Descriptor Type _model_v5.py: the refactored descriptor classes_ 12345678910111213141516171819202122232425262728293031323334353637383940414243import abcclass AutoStorage: __counter = 0 def __init__(self): cls= self.__class__ prefix= cls.__name__ index= cls.__counter self.storage_name= '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.storage_name) def __set__(self, instance,value): setattr(instance, self.storage_name, value)class Validated(abc.ABC, AutoStorage): #__set__ delegates validation to a validate method def __set__(self, instance, value): value= self.validate(instance, value) super().__set__(instance, value) # uses the returned value to invoke __set__ on a superclass, which # performs the actual storage. @abc.abstractmethod def validate(self, instance, value): '''return validated value or raise ValueError'''class Quantity(Validated): def validate(self, instance, value): if value &lt; 0: raise ValueError('value must be &gt; 0') return valueclass NonBlank(Validated): def validate(self, instance, value): value = value.strip() if len(value) == 0: raise ValueError('value cannot be empty or blank') return value]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_Dynamic Attributes and Properties_02]]></title>
    <url>%2F2018%2F05%2F25%2FC19-Dynamic-Attributes-and-Properties-02%2F</url>
    <content type="text"><![CDATA[Using a Property for Attribute ValidationSo far, we have only seen the @property decorator used to implement read-only properties. In this section, we will create a read/write property. LineItem Take #1: Class for an Item in an Order _bulkfood_v1.py: the simplest LineItem class_ 123456789class LineItem: def __init__(self, description, weight, price): self.description = description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price A negative weight results in a negative subtotal 123456&gt;&gt;&gt; raisins = LineItem('Golden raisins', 10, 6.95) &gt;&gt;&gt; raisins.subtotal() 69.5 &gt;&gt;&gt; raisins.weight = -20 # garbage in... &gt;&gt;&gt; raisins.subtotal() # garbage out... -139.0 LineItem Take #2: A Validating PropertyImplementing a property will allow us to use a getter and a setter, but the interface of LineItem will not change (i.e., setting the weight of a LineItem will still be written as raisins.weight = 12). _bulkfood_v2.py: a LineItem with a weight property_ 12345678910111213141516171819202122class LineItem: def __init__(self, description, weight, price): self.description = description self.weight= weight self.price= price def subtotal(self): return self.weight * self.price @property def weight(self): # The methodsthat implement a property all have the name of the public # attribute: weight. return self.__weight # The actual value is stored in a private attribute __weight @property.setter def weight(self, value): if value &gt; 0: self.__weight = value else: raise ValueError('value must be &gt; 0') Note how a LineItem with an invalid weight cannot be created now: 1234&gt;&gt;&gt; walnuts = LineItem('walnuts', 0, 10.00)Traceback (most recent call last): ...ValueError: value must be &gt; 0 Now we have protected weight from users providing negative values. Although buyers usually can’t set the price of an item, a clerical error or a bug may create a LineItem with a negative price. To prevent that, we could also turn price into a property, but this would entail some repetition in our code. There are two ways to abstract away property definitions: using a property factory or a descriptor class. The descriptor class approach is more flexible, and we’ll devote Chapter 20 to a full discussion of it. Properties are in fact implemented as descriptor classes themselves. A Proper Look at PropertiesAlthough often used as a decorator, the property built-in is actually a class. In Python, functions and classes are often interchangeable, because both are callable and there is no new operator for object instantiation, so invoking a constructor is no different than invoking a factory function. And both can be used as decorators, as long as they return a new callable that is a suitable replacement of the decorated function. This is the full signature of the property constructor: property(fget=None, fset=None, fdel=None, doc=None) All arguments are optional, and if a function is not provided for one of them, the corresponding operation is not allowed by the resulting property object. Properties Override Instance AttributesProperties are always class attributes, but they actually manage attribute access in the instances of the class. Instance attribute does not shadow class property 1234567891011121314151617181920212223&gt;&gt;&gt; class Class:... data='the class data attr'... @property... def prop(self): return 'the prop value'... &gt;&gt;&gt; obj=Class()&gt;&gt;&gt; Class.prop&lt;property object at 0x05B54CC0&gt;&gt;&gt;&gt; obj.prop'the prop value'&gt;&gt;&gt; obj.prop= 'foo'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attribute&gt;&gt;&gt; obj.__dict__['prop'] = 'foo'&gt;&gt;&gt; vars(obj)&#123;'prop': 'foo'&#125;&gt;&gt;&gt; obj.prop # reading obj.prop still runs the property getter. The property is not shadowed by an instance attribute.'the prop value'&gt;&gt;&gt; Class.prop ='baz' # Overwriting Class.prop destroys the property object.&gt;&gt;&gt; obj.prop # Now obj.prop retrieves the instance attribute. Class.prop is not a property anymore, so it no longer overrides obj.prop.'foo' New class property shadows existing instance attribute 123456789101112&gt;&gt;&gt; obj.data'the class data attr'&gt;&gt;&gt; obj.data= 'bar'&gt;&gt;&gt; Class.data'the class data attr'&gt;&gt;&gt; Class.data = property(lambda self: 'the "data" prop value')&gt;&gt;&gt; obj.data # obj.data is now shadowed by the Class.data property.'the "data" prop value'&gt;&gt;&gt; del Class.data&gt;&gt;&gt; obj.data'bar'&gt;&gt;&gt; The main point of this section is that an expression like obj.attr does not search for attr starting with obj. The search actually starts at obj.__class__, and only if there is no property named attr in the class, Python looks in the obj instance itself. This rule applies not only to properties but to a whole category of descriptors, the overriding descriptors. Property DocumentationWhen tools such as the console help() function or IDEs need to display the documentation of a property, they extract the information from the _\doc__ attribute of the property. If used with the classic call syntax, property can get the documentation string as the doc argument:weight = property(get_weight, set_weight, doc=&#39;weight in kilograms&#39;) When property is deployed as a decorator, the docstring of the getter method—the one with the @property decorator itself—is used as the documentation of the property as a whole. Coding a Property Factory_bulkfood_v2prop.py: the quantity property factory in use_ 12345678910111213141516171819202122232425262728class LineItem: weight= quantity('weight') price= quantity('price') # Use the factory to define the first custom property, weight, as a class # attribute. def __init__(self, description, weight, price): self.description= description self.weight= weight self.price= price def subtotal(self): return self.weight * self.pricedef quantity(storage_name): # qty_getter references storage_name, so it will be preserved in the closure # of this function; the value is retrieved directly from the instance.__dict__ # to bypass the property and avoid an infinite recursion. def qty_getter(instance): return instance.__dict__[storage_name] def qty_setter(instance, value): if value &gt; 0: instance.__dict__[storage_name] = value else: raise ValueError('value must be &gt; 0') return property(qty_getter, qty_setter) _bulkfood_v2prop.py: the quantity property factory_ 12345678910&gt;&gt;&gt; nutmeg = LineItem('Moluccan nutmeg', 8, 13.95)# Reading the weight and price through the properties shadowing the namesake instance attributes.&gt;&gt;&gt; nutmeg.weight, nutmeg.price (8, 13.95)# Using vars to inspect the nutmeg instance: here we see the actual instance attributes used to store the values.&gt;&gt;&gt; sorted(vars(nutmeg).items()) [('description', 'Moluccan nutmeg'), ('price', 13.95), ('weight', 8)] The weight property overrides the weight instance attribute so that every reference to self.weight or nutmeg.weight is handled by the property functions, and the only way to bypass the property logic is to access the instance __dict__ directly. Handling Attribute DeletionRecall from the Python tutorial that object attributes can be deleted using the del statement: del my_object.an_attribute In practice, deleting attributes is not something we do every day in Python, and the requirement to handle it with a property is even more unusual. But it is supported, and I can think of a silly example to demonstrate it. In a property definition, the @my_propety.deleter decorator is used to wrap the method in charge of deleting the attribute managed by the property. blackknight.py 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; class BlackKnight:... ... def __init__(self):... self.members = ['an arm', ... 'another arm',... 'a leg'... ]... ... self.phrases = ["'Tis but a scratch.",... "It's just a flesh wound.",... "I'm invincible!"]... ... @property... def member(self):... print('next member is:')... return self.members[0]... ... @member.deleter... def member(self):... text='BLACK KNIGHT (loses &#123;&#125;)\n-- &#123;&#125;'... print(text.format(self.members.pop(0), self.phrases.pop(0)))... &gt;&gt;&gt; knight= BlackKnight()&gt;&gt;&gt; knight.membernext member is:'an arm'&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses an arm)-- 'Tis but a scratch.&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses another arm)-- It's just a flesh wound.&gt;&gt;&gt; del knight.memberBLACK KNIGHT (loses a leg)-- I'm invincible!&gt;&gt;&gt; Using the classic call syntax instead of decorators, the fdel argument is used to set the deleter function. For example, the member property would be coded like this in the body of the BlackKnight class: member = property(member_getter, fdel=member_deleter) If you are not using a property, attribute deletion can also be handled by implementing the lower-level __delattr__ special method. Essential Attributes and Functions for Attribute HandlingSpecial Attributes that Affect Attribute HandlingThe behavior of many of the functions and special methods listed in the following sections depend on three special attributes: __class__A reference to the object’s class (i.e., obj.__class__ is the same as type(obj)). Python looks for special methods such as __getattr__ only in an object’s class, and not in the instances themselves. __dict__A mapping that stores the writable attributes of an object or class. An object that has a __dict__ can have arbitrary new attributes set at any time. If a class has a __slots__ attribute, then its instances may not have a __dict__. See __slots__ (next). __slots__An attribute that may be defined in a class to limit the attributes its instances can have. __slots__ is a tuple of strings naming the allowed attributes. If the ‘__dict__‘ name is not in __slots__ , then the instances of that class will not have a __dict__ of their own, and only the named attributes will be allowed in them. Built-In Functions for Attribute HandlingThese five built-in functions perform object attribute reading, writing, and introspection: dir([object])Lists most attributes of the object. The official docs say dir is intended for interactive use so it does not provide a comprehensive list of attributes, but an “interesting” set of names. dir can inspect objects implemented with or without a __dict__. The __dict__ attribute itself is not listed by dir, but the __dict__ keys are listed. Several special attributes of classes, such as __mro__, __bases__, and __name__ are not listed by dir either. If the optional object argument is not given, dir lists the names in the current scope. getattr(object, name[, default])Gets the attribute identified by the name string from the object. This may fetch an attribute from the object’s class or from a superclass. If no such attribute exists, getattr raises AttributeError or returns the default value, if given. hasattr(object, name)Returns True if the named attribute exists in the object, or can be somehow fetched through it (by inheritance, for example). The documentation explains: “This is implemented by calling getattr(object, name) and seeing whether it raises an AttributeError or not.” setattr(object, name, value)Assigns the value to the named attribute of object, if the object allows it. This may create a new attribute or overwrite an existing one. vars([object])Returns the __dict__ of object; vars can’t deal with instances of classes that define __slots__ and don’t have a __dict__ (contrast with dir, which handles such instances). Without an argument, vars() does the same as locals(): returns a dict representing the local scope. Special Methods for Attribute HandlingAttribute access using either dot notation or the built-in functions getattr, hasattr, and setattr trigger the appropriate special methods listed here. Reading and writing attributes directly in the instance __dict__ does not trigger these special methods— and that’s the usual way to bypass them if needed. In other words, assume that the special methods will be retrieved on the class itself, even when the target of the action is an instance. For this reason, special methods are not shadowed by instance attributes with the same name. In the following examples, assume there is a class named Class, obj is an instance of Class, and attr is an attribute of obj. For every one of these special methods, it doesn’t matter if the attribute access is done using dot notation or one of the built-in functions listed in “Built-In Functions for Attribute Handling”. For example, both obj.attr and getattr(obj, ‘attr’, 42) trigger Class.__getattribute__(obj, ‘attr’). delattr(self, name)Always called when there is an attempt to delete an attribute using the del statement; e.g., del obj.attr triggers Class.__delattr__(obj, ‘attr’). __dict__(self)Called when dir is invoked on the object, to provide a listing of attributes; e.g.,dir(obj) triggers Class.__dict__(obj). __getattr__(self, name)Called only when an attempt to retrieve the named attribute fails, after the obj, Class, and its superclasses are searched. The expressions obj.no_such_attr, get attr(obj, ‘.no_such_attr’), and hasattr(obj, ‘.no_such_attr’) may trigger Class.__getattr__(obj, ‘.no_such_attr’), but only if an attribute by that name cannot be found in obj or in Class and its superclasses. __getattribute__(self, name)Always called when there is an attempt to retrieve the named attribute, except when the attribute sought is a special attribute or method. Dot notation and the getattr and hasattr built-ins trigger this method. __getattr__ is only invoked after __getattribute__, and only when __getattribute__ raises AttributeError. To retrieve attributes of the instance obj without triggering an infinite recursion, implementations of __getattribute__ should use super().__getattribute__(obj, name). __setattr__(self, name, value)Always called when there is an attempt to set the named attribute. Dot notation and the setattr built-in trigger this method; e.g., both obj.attr = 42 and setattr(obj, ‘attr’, 42) trigger Class.__setattr__(obj, ‘attr’, 42). In practice, because they are unconditionally called and affect practically every attribute access, the __getattribute__ and __setattr__ special methods are harder to use correctly than __getattr__—which only handles nonexisting attribute names. Using properties or descriptors is less error prone than defining these special methods.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_Dynamic Attributes and Properties_01]]></title>
    <url>%2F2018%2F05%2F25%2FC19-Dynamic-Attributes-and-Properties-01%2F</url>
    <content type="text"><![CDATA[Data attributes and methods are collectively known as attributes in Python: a method is just an attribute that is callable. Besides data attributes and methods, we can also create properties, which can be used to replace a public data attribute with accessor methods (i.e., getter/setter), without changing the class interface.Besides properties, Python provides a rich API for controlling attribute access and implementing dynamic attributes. The interpreter calls special methods such as __getattr__ and __setattr__ to evaluate attribute access using dot notation (e.g.,obj.attr). A user-defined class implementing __getattr__ can implement “virtual attributes” by computing values on the fly whenever somebody tries to read a nonexistent attribute like obj.no_such_attribute. Data Wrangling with Dynamic Attributes123456789101112131415161718192021222324252627282930313233343536373839**_osconfeed.py: downloading osconfeed.json_**from urllib.request import urlopenimport warningsimport osimport jsonURL='http://www.oreilly.com/pub/sc/osconfeed'JSON= 'data/osconfeed.json'def load(): if not os.path.exists(JSON): msg= 'downloading &#123;&#125; to &#123;&#125;'.format(URL, JSON) warnings.warn(msg) with urlopen(URL) as remote, open(JSON, 'wb') as local: local.write(remote.read()) with open(JSON) as fp: return json.load(fp) # The json.load function parses a JSON file and returns native Python objects.&gt;&gt;&gt; feed = load() &gt;&gt;&gt; sorted(feed['Schedule'].keys()) ['conferences', 'events', 'speakers', 'venues'] &gt;&gt;&gt; for key, value in sorted(feed['Schedule'].items()): ... print('&#123;:3&#125; &#123;&#125;'.format(len(value), key)) ...1 conferences 484 events 357 speakers 53 venues &gt;&gt;&gt; feed['Schedule']['speakers'][-1]['name'] 'Carina C. Zona' &gt;&gt;&gt; feed['Schedule']['speakers'][-1]['serial'] 141590 &gt;&gt;&gt; feed['Schedule']['events'][40]['name'] 'There *Will* Be Bugs' &gt;&gt;&gt; feed['Schedule']['events'][40]['speakers'] [3471, 5199] Exploring JSON-Like Data with Dynamic AttributesThe syntax feed[‘Schedule’][‘events’][40][‘name’] is cumbersome. In JavaScript, you can get the same value by writing feed.Schedule.events[40].name. It’s easy to implement a dict-like class that does the same in Python. The keystone of the FrozenJSON class is the __getattr__ method, It’s essential to recall that the __getattr__ special method is only invoked by the interpreter when the usual process fails to retrieve an attribute (i.e., when the named attribute cannot be found in the instance, nor in the class or in its superclasses). explore0.py: turn a JSON dataset into a FrozenJSON holding nested FrozenJSON objects, lists, and simple types 1234567891011121314151617181920212223from collections import abcclass FrozenJson: ''' read-only for navigating a JSON-like object using attribute notation ''' def __init__(self, mapping): self.__data= dict(mapping) def __getattr__(self, name): if hasattr(self.__data, name): return getattr(self.__data, name) else: return FrozenJson.build(self.__data[name]) @classmethod def build(cls, obj): if isinstance(obj, abc.Mapping): return cls(obj) elif isinstance(obj, abc.MutableSequence): return [cls.build(item) for item in obj] else: return obj The Invalid Attribute Name ProblemThe FrozenJSON class has a limitation: there is no special handling for attribute namesthat are Python keywords.12345678910&gt;&gt;&gt; grad=FrozenJson(&#123;'name':'Jim','class':1982&#125;)&gt;&gt;&gt; grad.class File "&lt;stdin&gt;", line 1 grad.class ^SyntaxError: invalid syntax&gt;&gt;&gt; getattr(grad, 'class')1982&gt;&gt;&gt; But the idea of FrozenJSON is to provide convenient access to the data, so a better solution is checking whether a key in the mapping given to FrozenJSON.__init__ is a keyword, and if so, append an _ to it, so the attribute can be read like this: _explore1.py: append a _ to attribute names that are Python keywords_ 123456789101112131415import keyword def __init__(self, mapping): self.__data= &#123;&#125; for key, value in mapping.items(): if keyword.iskeyword(key): key+='_' self.__data[key]=value# A similar problem may arise if a key in the JSON is not a valid Python identifier:&gt;&gt;&gt; x = FrozenJSON(&#123;'2be':'or not'&#125;)&gt;&gt;&gt; x.2be File "&lt;stdin&gt;", line 1 x.2be ^SyntaxError: invalid syntax Such problematic keys are easy to detect in Python 3 because the str class provides the s.isidentifier() method, which tells you whether s is a valid Python identifier according to the language grammar. Flexible Object Creation with __new__We often refer to__init__ as the constructor method, but that’s because we adopted jargon from other languages. The special method that actually constructs an instance is __new__: it’s a class method (but gets special treatment, so the @classmethod decorator is not used), and it must return an instance. That instance will in turn be passed as the first argument self of __init__. Because __init__ gets an instance when called, and it’s actually forbidden from returning anything, __init__ is really an “initializer.” The real constructor is __new__—which we rarely need to code because the implementation inherited from object suffices. The path just described, from __new__ to __init__, is the most common, but not the only one. The __new__ method can also return an instance of a different class, and when that happens, the interpreter does not call __init__. In other words, the process of building an object in Python can be summarized with this pseudocode: pseudo-code for object construction12345678def object_maker(the_class, some_arg): new_object = the_class.__new__(some_arg) if isinstance(new_object, the_class): the_class.__init__(new_object, some_arg) return new_object# the following statements are roughly equivalentx = Foo('bar')x = object_maker(Foo, 'bar') explore2.py: using new instead of build to construct new objects that may or may not be instances of FrozenJSON 1234567891011121314151617181920212223242526272829from collections import abcfrom keyword import iskeywordclass FrozenJSON: def __new__(cls, arg): if isinstance(arg, abc.Mapping): return super().__new__(cls) # The default behavior is to delegate to the __new__ of a super class. In # this case, we are calling __new__ from the object base class, passing # FrozenJSON as the only argument. elif isinstance(arg, abc.MutableSequence): return [cls(item) for item in arg] else: return arg def __init__(self, mapping): self.__data=&#123;&#125; for key, value in mapping.items(): if iskeyword(key): key+='_' self.__data[key]= value def __getattr__(self, name): if hasattr(self.__data, name): return getattr(self.__data, name) else: return FrozenJSON(self.__data[name]) Restructuring the OSCON Feed with shelveThe funny name of the standard shelve module makes sense when you realize that pickle is the name of the Python object serialization format—and of the module that converts objects to/from that format. Because pickle jars are kept in shelves, it makes sense that shelve provides pickle storage. The shelve.open high-level function returns a shelve.Shelf instance—a simple key-value object database backed by the dbm module, with these characteristics: shelve.Shelf subclasses abc.MutableMapping, so it provides the essential methods we expect of a mapping type In addition, shelve.Shelf provides a few other I/O management methods, like sync and close; it’s also a context manager. Keys and values are saved whenever a new value is assigned to a key. The keys must be strings. The values must be objects that the pickle module can handle. schedule1.py: exploring OSCON schedule data saved to a shelve.Shelf 12345678910111213141516171819202122232425import warningsimport osconfeedDB_NAME= 'data/schedule1_db'CONFERENCE= 'conference.115'class Record: def __init__(self, **kwargs): self.__dict__.update(kwargs) # a common shortcut to build an instance with attributes created from # keyword argumentsdef load_db(db): raw_data= osconfeed.load() warnings.warn('loading '+ DB_NAME) for collection, rec_list in raw_data['Shedule'].items(): record_type=collection[:-1] #record_type is set to the collection name without the trailing 's' for record in rec_list: key= '&#123;&#125;.&#123;&#125;'.format(record_type, record['serial']) record['serial'] = key db[key]= Record(**record) Trying out the functionality provided by schedule1.py 1234567891011&gt;&gt;&gt; import shelve &gt;&gt;&gt; db = shelve.open(DB_NAME) &gt;&gt;&gt; if CONFERENCE not in db: ... load_db(db) ... &gt;&gt;&gt; speaker = db['speaker.3471'] &gt;&gt;&gt; type(speaker) &lt;class 'schedule1.Record'&gt; &gt;&gt;&gt; speaker.name, speaker.twitter ('Anna Martelli Ravenscroft', 'annaraven') &gt;&gt;&gt; db.close() The Record.__init__ method illustrates a popular Python hack. Recall that the__dict__ of an object is where its attributes are kept—unless __slots__ is declared inthe class.So, updating an instance __dict__ with a mapping is a quick way to create a bunch ofattributes in that instance. schedule2.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import warningsimport inspectimport osconfeedDB_NAME= 'data/schedule2_db'CONFERENCE='conference.115'class Record: def __init__(self, **kwargs): self.__dict__.update(**kwargs) def __eq__(self, other): if isinstance(other, Record): return self.__dict__== other.__dict__ else: return NotImplementedclass MissingDatabaseError(RuntimeError): '''when a database id required but was not set'''class DbRecord(Record): __db = None @staticmethod def set_db(db): DbRecord.__db = db @staticmethod def get_db(): return DbRecord.__db @classmethod def fetch(cls, ident): db= cls.get_db() try: return db[ident] except TypeError: if db is None: msg= 'database not set; call "&#123;&#125;.set_db(my_db)"' raise MissingDatabaseError(msg.format(cls.__name__)) else: raise def __repr__(self): if hasattr(self, 'serial'): cls_name = self.__class__.__name__ return '&lt;&#123;&#125; serial=&#123;!r&#125;&gt;'.format(cls_name, self.serial) else: return super().__repr__()class Event(DbRecord): @property def venue(self): key= 'venue.&#123;&#125;'.format(self.venue_serial) return self.__class__.fetch(key) @property def speakers(self): if not hasattr(self, '_speaker_objs'): spkr_serials= self.__dict__['speakers'] fetch= self.__class__.fetch self._speaker_objs=[fetch('speaker.&#123;&#125;'.format(key)) for key in spkr_serials] return self._speaker_objs def __repr__(self): if hasattr(self, 'name'): cls_name = self.__class__.__name__ return '&lt;&#123;&#125; &#123;!r&#125;&gt;'.format(cls_name, self.name) else: return super().__repr__()def load_db(db): raw_data= osconfeed.load() warnings.warn('loading '+DB_NAME) for collection, rec_list in raw_data['Schedule'].items(): record_type= collection[:-1] cls_name= record_type.capitalize() cls= globals().get(cls_name, DbRecord) if inspect.isclass(cls) and issubclass(cls, DbRecord): factory= cls else: factory= DbRecord for record in rec_list: key= '&#123;&#125;.&#123;&#125;'.format(record_type, record['serial']) record['serial']= key db[key]= factory(**record) Extract from the doctests of schedule2.py1234567891011121314&gt;&gt;&gt; DbRecord.set_db(db) &gt;&gt;&gt; event = DbRecord.fetch('event.33950') &gt;&gt;&gt; event &lt;Event 'There *Will* Be Bugs'&gt; &gt;&gt;&gt; event.venue &lt;DbRecord serial='venue.1449'&gt; &gt;&gt;&gt; event.venue.name 'Portland 251' &gt;&gt;&gt; for spkr in event.speakers: ... print('&#123;0.serial&#125;: &#123;0.name&#125;'.format(spkr)) ... speaker.3471: Anna Martelli Ravenscroft speaker.5199: Alex Martelli]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C17_Concurrency with Futures(Not finished)]]></title>
    <url>%2F2018%2F05%2F25%2FC17-Concurrency-with-Futures%2F</url>
    <content type="text"><![CDATA[flags.py: sequential download script; some functions will be reused by the other scripts import osimport timeimport sysimport requests POP20_CC = (‘CN IN US ID BR PK NG BD RU JP ‘ ‘MX PH VN ET EG DE IR TR CD FR’).split() BASE_URL= ‘http://flupy.org/data/flags&#39; DEST_DIR= ‘downloads/‘ def save_flag(img, filename): path= os.path.join(DEST_DIR, filename) with open(path, ‘wb’) as fp: fp.write(img) def get_flag(cc): url=’{}/{cc}/{cc}.gif’.format(BASE_URL, cc= cc.lower()) resp = requests.get(url) return resp.content Display a string and flush sys.stdout so we can see progress in a one-linedisplay; this is needed because Python normally waits for a line break to flushthe stdout buffer.def show(text): print(text,end= ‘ ‘) sys.stdout.flush() def download_many(cc_list): for cc in sorted(cc_list): image= get_flag(cc) show(cc) save_flag(image, cc.lower()+’.gif’) return len(cc_list) def main(download_many): t0=time.time() count= download_many(POP20_CC) elapsed= time.time() - t0 msg= ‘\n{} flags downloaded in {:.2f}s’ print(msg.format(count, elapsed)) if name== ‘main‘: main(download_many) Downloading with concurrent.futures_flags_threadpool.py: threaded download script using futures.ThreadPoolExecutor_ from concurrent import futures from flags import save_flag, get_flag, show, main MAX_WORKERS = 20 def download_one(cc): image= get_flag(cc) show(cc) save_flag(img, cc.lower()+’.gif’) return cc def download_many(cc_list): workers= min(MAX_WORKERS, len(cc_list)) with futures.ThreadPoolExecutor(workers) as executor: res=executor.map(download_one, sorted(cc_list)) return len(list(res)) if name== ‘main‘: main(download_many) flags_threadpool_ac.pydef download_many(cc_list): cc_list= cc_list[:5] with futures.ThreadPoolExecutor(max_workers = 3) as executor: to_do=[] for cc in sorted(cc_list): future = executor.submit(download_one, cc) # executor.submit schedules the callable to be executed, and returns a # future representing this pending operation. to_do.append(future) msg= &apos;Scheduled for {}: {}&apos; print(msg.format(cc, future)) results=[] for future in futures.as_completed(to_do):#as_completed yields futures as they are completed. res= future.result() # Get the result of this future. msg=&apos;{} result: {!r}&apos; print(msg.format(future, res)) results.append(res) return len(results) Blocking I/O and the GILThe CPython interpreter is not thread-safe internally, so it has a Global Interpreter Lock(GIL), which allows only one thread at a time to execute Python bytecodes. That’s whya single Python process usually cannot use multiple CPU cores at the same time. When we write Python code, we have no control over the GIL, but a built-in functionor an extension written in C can release the GIL while running time-consuming tasks.In fact, a Python library coded in C can manage the GIL, launch its own OS threads,and take advantage of all available CPU cores. This complicates the code of the libraryconsiderably, and most library authors don’t do itHowever, all standard library functions that perform blocking I/O release the GIL whenwaiting for a result from the OS. This means Python programs that are I/O bound canbenefit from using threads at the Python level: while one Python thread is waiting fora response from the network, the blocked I/O function releases the GIL so anotherthread can run. Launching Processes with concurrent.futures]]></content>
  </entry>
  <entry>
    <title><![CDATA[C16_Coroutines]]></title>
    <url>%2F2018%2F05%2F25%2FC16-Coroutines%2F</url>
    <content type="text"><![CDATA[We find two main senses for the verb “to yield” in dictionaries: to produce or to give way. Both senses apply in Python when we use the yield keyword in a generator. A line such as yield item produces a value that is received by the caller of next(…), and it also gives way, suspending the execution of the generator so that the caller may proceed until it’s ready to consume another value by invoking next() again. The caller pulls values from the generator.A coroutine is syntactically like a generator: just a function with the yield keyword in its body. However, in a coroutine, yield usually appears on the right side of an expression (e.g., datum = yield), and it may or may not produce a value—if there is no expression after the yield keyword, the generator yields None. The coroutine may receive data from the caller, which uses .send(datum) instead of next(…) to feed the coroutine. Usually, the caller pushes values into the coroutine. It is even possible that no data goes in or out through the yield keyword. Regardless of the flow of data, yield is a control flow device that can be used to implement cooperative multitasking: each coroutine yields control to a central scheduler so that other coroutines can be activated. How Coroutines Evolved from Generators Using .send(…), the caller of the generator can post data that then becomes the value of the yield expression inside the generator function. This allows a generator to be used as a coroutine: a procedure that collaborates with the caller, yielding and receiving values from the caller. In addition to .send(…), PEP 342 also added .throw(…) and .close() methods that respectively allow the caller to throw an exception to be handled inside the generator, and to terminate it. When you start thinking of yield primarily in terms of control flow, you have the mindset to understand coroutines. Basic Behavior of a Generator Used as a Coroutine 123456789101112131415&gt;&gt;&gt; def simple_coroutine():... print('-&gt; coroutine started')... x = yield # 1... print('-&gt; coroutine received:', x)... &gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro &lt;generator object simple_coroutine at 0x05D8CA80&gt;&gt;&gt;&gt; next(my_coro) # 2-&gt; coroutine started&gt;&gt;&gt; my_coro.send(42) # 3-&gt; coroutine received: 42Traceback (most recent call last): # 4 File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration yield is used in an expression; when the coroutine is designed just to receive data from the client it yields None—this is implicit because there is no expression to the right of the yield keyword. The first call is next(…) because the generator hasn’t started so it’s not waiting in a yield and we can’t send it any data initially. This call makes the yield in the coroutine body evaluate to 42; now the coroutine resumes and runs until the next yield or termination. In this case, control flows off the end of the coroutine body, which prompts the generator machinery to raise StopIteration, as usual. A coroutine can be in one of four states. You can determine the current state using the inspect.getgeneratorstate(…) function, which returns one of these strings: ‘GEN_CREATED’Waiting to start execution. ‘GEN_RUNNING’Currently being executed by the interpreter. ‘GEN_SUSPENDED’Currently suspended at a yield expression. ‘GEN_CLOSED’Execution has completed. Because the argument to the send method will become the value of the pending yield expression, it follows that you can only make a call like my_coro.send(42) if the coroutine is currently suspended. But that’s not the case if the coroutine has never been activated—when its state is ‘GEN_CREATED’. That’s why the first activation of a coroutine is always done with next(my_coro)—you can also call my_coro.send(None), and the effect is the same. If you create a coroutine object and immediately try to send it a value that is not None, this is what happens: 12345&gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro.send(1729)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: can't send non-None value to a just-started generator The initial call next(my_coro) is often described as “priming” the coroutine (i.e., advancing it to the first yield to make it ready for use as a live coroutine). A coroutine that yields twice 123456789101112131415161718192021222324&gt;&gt;&gt; def simple_coro2(a):... print('-&gt; Started: a =', a)... b = yield a... print('-&gt; Received: b =', b)... c = yield a + b... print('-&gt; Received: c =', c)... &gt;&gt;&gt; my_coro2=simple_coro2(14)&gt;&gt;&gt; from inspect import getgeneratorstate&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_CREATED'&gt;&gt;&gt; next(my_coro2)-&gt; Started: a = 1414&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_SUSPENDED'&gt;&gt;&gt; my_coro2.send(28) # 1-&gt; Received: b = 2842&gt;&gt;&gt; my_coro2.send(99) # 2-&gt; Received: c = 99Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration Send number 28 to suspended coroutine; the yield expression evaluates to 28 and that number is bound to b. The -&gt; Received: b = 28 message is displayed, the value of a + b is yielded (42), and the coroutine is suspended waiting for the value to be assigned to c. Send number 99 to suspended coroutine; the yield expression evaluates to 99 the number is bound to c. The -&gt; Received: c = 99 message is displayed, then the coroutine terminates, causing the generator object to raise StopIteration. It’s crucial to understand that the execution of the coroutine is suspended exactly at the yield keyword. As mentioned before, in an assignment statement, the code to the right of the = is evaluated before the actual assignment happens. This means that in a line like b = yield a, the value of b will only be set when the coroutine is activated later by the client code. It takes some effort to get used to this fact, but understanding it is essential to make sense of the use of yield in asynchronous programming. Execution of the simple_coro2 coroutine can be split in three phases: next(my_coro2) prints first message and runs to yield a, yielding number 14. my_coro2.send(28) assigns 28 to b, prints second message, and runs to yield a +b, yielding number 42. my_coro2.send(99) assigns 99 to c, prints third message, and the coroutine ter‐minates. Example: Coroutine to Compute a Running Average123456789101112131415161718&gt;&gt;&gt; def averager():... total= 0.0... count = 0... average = None... while True: # 1... term = yield average # 2... total += term... count += 1... average = total / count... &gt;&gt;&gt; coro_avg = averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(30)20.0&gt;&gt;&gt; coro_avg.send(5)15.0 This infinite loop means this coroutine will keep on accepting values and producing results as long as the caller sends them. This coroutine will only terminate when the caller calls .close() on it, or when it’s garbage collected because there are no more references to it. The yield statement here is used to suspend the coroutine, produce a result to the caller, and—later—to get a value sent by the caller to the coroutine, which resumes its infinite loop. The call next(coro_avg) makes the coroutine advance to the yield, yielding the initial value for average, which is None, so it does not appear on the console. At this point, the coroutine is suspended at the yield, waiting for a value to be sent. The line coro_avg.send(10) provides that value, causing the coroutine to activate, assigning it to term, updating the total, count, and average variables, and then starting another iteration in the while loop, which yields the average and waits for another term. Decorators for Coroutine PrimingYou can’t do much with a coroutine without priming it: we must always remember to call next(my_coro) before my_coro.send(x). To make coroutine usage more convenient, a priming decorator is sometimes used. coroutil.py: decorator for priming coroutine 123456789101112131415161718192021222324252627&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def coroutine(func):... @wraps(func)... def primer(*args, **kwargs):... gen= func(*args, **kwargs)... next(gen)... return gen... return primer... &gt;&gt;&gt; @coroutine... def averager():... total= 0.0... count = 0... average= None... while True:... term= yield average... total += term... count += 1... average = total/count... &gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; getgeneratorstate(coro_avg)'GEN_SUSPENDED'&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(30)20.0 Coroutine Termination and Exception HandlingAn unhandled exception within a coroutine propagates to the caller of the next or send that triggered it. How an unhandled exception kills a coroutine12345678910&gt;&gt;&gt; coro_avg.send('spam') # 1Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 8, in averagerTypeError: unsupported operand type(s) for +=: 'float' and 'str'&gt;&gt;&gt; coro_avg.send(60) # 2Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration Sending a nonnumeric value causes an exception inside the coroutine. Because the exception was not handled in the coroutine, it terminated. Any attempt to reactivate it will raise StopIteration. Since Python 2.5, generator objects have two methods that allow the client to explicitly send exceptions into the coroutine—throw and close: generator.throw(exc_type[, exc_value[, traceback]])Causes the yield expression where the generator was paused to raise the exception given. If the exception is handled by the generator, flow advances to the next yield, and the value yielded becomes the value of the generator.throw call. If the exception is not handled by the generator, it propagates to the context of the caller. generator.close()Causes the yield expression where the generator was paused to raise a GeneratorExit exception. No error is reported to the caller if the generator does not handle that exception or raises StopIteration—usually by running to completion. When receiving a GeneratorExit, the generator must not yield a value, otherwise a RuntimeError is raised. If any other exception is raised by the generator, it propagates to the caller. _coro_exc_demo.py: test code for studying exception handling in a coroutine_ 1234567891011121314151617181920212223&gt;&gt;&gt; class DemoException(Exception):... '''An exception type for the demonstration.'''... &gt;&gt;&gt; def demo_exc_handling():... print('-&gt; coroutine started')... while True:... try:... x = yield... except DemoException:... print('*** DemoException handled. Continuing...')... else:... print('-&gt; coroutine received: &#123;!r&#125;'.format(x))... raise RuntimeError('This line should never run.')... &gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.close()&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_CLOSED'&gt;&gt;&gt; _Throwing DemoException into demo_exc_handling does not break it_ 12345678910&gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.throw(DemoException)*** DemoException handled. Continuing...&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_SUSPENDED'&gt;&gt;&gt; Coroutine terminates if it can’t handle an exception thrown into it 123456789101112&gt;&gt;&gt; exc_coro= demo_exc_handling()&gt;&gt;&gt; next(exc_coro)-&gt; coroutine started&gt;&gt;&gt; exc_coro.send(11)-&gt; coroutine received: 11&gt;&gt;&gt; exc_coro.throw(ZeroDivisionError)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 5, in demo_exc_handlingZeroDivisionError&gt;&gt;&gt; getgeneratorstate(exc_coro)'GEN_CLOSED' If it’s necessary that some cleanup code is run no matter how the coroutine ends, you need to wrap the relevant part of the coroutine body in a try/finally block. _coro_finally_demo.py: use of try/finally to perform actions on coroutine termination_ 123456789101112131415class DemoException(Exception): '''An exception type for the demonstration.'''def demo_finally(): print('-&gt; coroutine started') try: while True: try: x = yield except DemoException: print('*** DemoException handled. Continuing...') else: print('-&gt; coroutine received: &#123;!r&#125;'.format(x)) finally: print('-&gt; coroutine ending') Returning a Value from a Coroutinecoroaverager2.py: code for an averager coroutine that returns a result 12345678910111213141516from collections import namedtupleResult= namedtuple('Result', 'count average')def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: # 1 break total += term count += 1 average = total / count return Result(count, average) # 2 In order to return a value, a coroutine must terminate normally; this is why this version of averager has a condition to break out of its accumulating loop. Return a namedtuple with the count and average. Before Python 3.3, it was a syntax error to return a value in a generator function. coroaverager2.py: doctest showing the behavior of averager 12345678910&gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)&gt;&gt;&gt; coro_avg.send(30)&gt;&gt;&gt; coro_avg.send(None)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration: Result(count=2, average=20.0)# Sending None terminates the loop, causing the coroutine to end by returning the result. As usual, the # generator object raises StopIteration. The value attribute of the exception carries the value returned. Note that the value of the return expression is smuggled to the caller as an attribute of the StopIteration exception. This is a bit of a hack, but it preserves the existing behavior of generator objects: raising StopIteration when exhausted. Catching StopIteration lets us get the value returned by averager 123456789101112&gt;&gt;&gt; coro_avg= averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)&gt;&gt;&gt; coro_avg.send(30)&gt;&gt;&gt; try:... coro_avg.send(None)... except StopIteration as exc:... result= exc.value... &gt;&gt;&gt; resultResult(count=2, average=20.0)&gt;&gt;&gt; Using yield fromThe first thing to know about yield from is that it is a completely new language construct. It does so much more than yield that the reuse of that keyword is arguably misleading. Similar constructs in other languages are called await, and that is a much better name because it conveys a crucial point: when a generator gen calls yield from subgen(), the subgen takes over and will yield values to the caller of gen; the caller will in effect drive subgen directly. Meanwhile gen will be blocked, waiting until subgen terminates. Chaining iterables with yield from 12345678&gt;&gt;&gt; def chain(*iterables):... for it in iterables:... yield from it...&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; t = tuple(range(3))&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2] The first thing the yield from x expression does with the x object is to call iter(x) to obtain an iterator from it. This means that x can be any iterable. However, if replacing nested for loops yielding values was the only contribution of yield from, this language addition wouldn’t have had a good chance of being accepted. The real nature of yield from cannot be demonstrated with simple iterables; it requires the mind-expanding use of nested generators. That’s why PEP 380, which introduced yield from, is titled “Syntax for Delegating to a Subgenerator.” The main feature of yield from is to open a bidirectional channel from the outermost caller to the innermost subgenerator, so that values can be sent and yielded back and forth directly from them, and exceptions can be thrown all the way in without adding a lot of exception handling boilerplate code in the intermediate coroutines. This is what enables coroutine delegation in a way that was not possible before. delegating generatorThe generator function that contains the yield from expression. subgeneratorThe generator obtained from the part of the yield from expression. This is the “subgenerator” mentioned in the title of PEP 380: “Syntax for Delegating to a Subgenerator.” callerPEP 380 uses the term “caller” to refer to the client code that calls the delegating generator. Depending on context, I use “client” instead of “caller,” to distinguish from the delegating generator, which is also a “caller” (it calls the subgenerator) coroaverager3.py: using yield from to drive averager and report statistics 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from collections import namedtupleResult= namedtuple('Result', 'count average')# the subgeneratordef averager(): total= 0.0 count= 0 average= None while True: term= yield # Each value sent by the client code in main will be bound to term here. if term is None: # The crucial terminating condition. Without it, a yield # from calling this coroutine will block forever. break total+= term count+= 1 average= total/ count return Result(count, average) # The returned Result will be the value of the yield from expression in grouper# the delegating generatordef grouper(results, key): while True:# Each iteration in thisloop creates a new instance of averager; # each is a generator object operating as a coroutine. results[key] = yield from averager() # Whenever grouper is sent a value, it’s piped into the averager instance # by the yield from. grouper will be suspended here as long as the # averager instance is consuming values sent by the client. When an # averager instance runs to the end, the value it returns is bound to # results[key]. The while loop then proceeds to create another averager # instance to consume more values.# the client codedef main(data): results=&#123;&#125; for key, values in data.items(): group= grouper(results, key) # group is a generator objectresulting from calling grouper with the # results dict to collect the results, and a particular key. It will # operate as a coroutine. next(group) # Prime the coroutine. for value in values: group.send(value) # Send each value into the grouper. That value ends up in the term = # yield line of averager; grouper never has a chance to see it. group.send(None) # Sending None into grouper causes the current averager instance to # terminate, and allows grouper to run again, which creates another # averager for the next group of values report(results)def report(results): for key, result in sorted(results.items()): group, unit = key.split(';') print('&#123;:2&#125; &#123;:5&#125; averaging &#123;:.2f&#125;&#123;&#125;'.format( result.count, group, result.average, unit ))data = &#123; 'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],&#125;if __name__ == '__main__': main(data) What would happen if we omitted the call group.send(None) marked “important!” in main:• Each iteration of the outer for loop creates a new grouper instance named group; this is the delegating generator.• The call next(group) primes the grouper delegating generator, which enters its while True loop and suspends at the yield from, after calling the subgenerator averager.• The inner for loop calls group.send(value); this feeds the subgenerator averager directly. Meanwhile, the current group instance of grouper is suspended at the yield from.• When the inner for loop ends, the group instance is still suspended at the yield from, so the assignment to results[key] in the body of grouper has not happened yet.• Without the last group.send(None) in the outer for loop, the averager subgenerator never terminates, the delegating generator group is never reactivated, and the assignment to results[key] never happens.• When execution loops back to the top of the outer for loop, a new grouper instance is created and bound to group. The previous grouper instance is garbage collected (together with its own unfinished averager subgenerator instance). Example 16-17 demonstrates the simplest arrangement of yield from, with only one delegating generator and one subgenerator. Because the delegating generator works as a pipe, you can connect any number of them in a pipeline: one delegating generator uses yield from to call a subgenerator, which itself is a delegating generator calling another subgenerator with yield from, and so on. Eventually this chain must end in a simple generator that uses just yield, but it may also end in any iterable object. Every yield from chain must be driven by a client that calls next(…) or .send(…) on the outermost delegating generator. This call may be implicit, such as a for loop. The Meaning of yield from• Any values that the subgenerator yields are passed directly to the caller of the delegating generator (i.e., the client code).• Any values sent to the delegating generator using send() are passed directly to the subgenerator. If the sent value is None, the subgenerator’s __next__() method is called. If the sent value is not None, the subgenerator’s send() method is called. If the call raises StopIteration, the delegating generator is resumed. Any other exception is propagated to the delegating generator.• return expr in a generator (or subgenerator) causes StopIteration(expr) to be raised upon exit from the generator.• The value of the yield from expression is the first argument to the StopIteration exception raised by the subgenerator when it terminates. The other two features of yield from have to do with exceptions and termination:• Exceptions other than GeneratorExit thrown into the delegating generator are passed to the throw() method of the subgenerator. If the call raises StopIteration, the delegating generator is resumed. Any other exception is propagated to the delegating generator.• If a GeneratorExit exception is thrown into the delegating generator, or the close() method of the delegating generator is called, then the close() method of the subgenerator is called if it has one. If this call results in an exception, it is propagated to the delegating generator. Otherwise, GeneratorExit is raised in the delegating generator.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C15_Context Managers and else Blocks]]></title>
    <url>%2F2018%2F05%2F25%2FC15-Context-Managers-and-else-Blocks%2F</url>
    <content type="text"><![CDATA[The with statement sets up a temporary context and reliably tears it down, under the control of a context manager object. This prevents errors and reduces boilerplate code, making APIs at the same time safer and easier to use. Python programmers are finding lots of uses for with blocks beyond automatic file closing. This is no secret, but it is an underappreciated language feature: the else clause can be used not only in if statements but also in for, while, and try statements. Do This, Then That: else Blocks Beyond ifforThe else block will run only if and when the for loop runs to completion (i.e., not if the for is aborted with a break). whileThe else block will run only if and when the while loop exits because the condition became falsy (i.e., not when the while is aborted with a break). tryThe else block will only run if no exception is raised in the try block. The official docs also state: “Exceptions in the else clause are not handled by the precedingexcept clauses.” In all cases, the else clause is also skipped if an exception or a return, break, or continue statement causes control to jump out of the main block of the compound statement. Context Managers and with BlocksThe context manager protocol consists of the __enter__ and __exit__ methods. At the start of the with, __enter__ is invoked on the context manager object. The role of the finally clause is played by a call to __exit__ on the context manager object at the end of the with block. mirror.py123456789101112131415161718192021222324class LookingGlass: def __enter__(self): import sys self.original_write= sys.stdout.write sys.stdout.write= self.reverse_write return 'JABBERWOCKY' def reverse_write(self, text): self.original_write(text[::-1]) #Python calls __exit__ with None, None, None if all went well; if an #exception is raised, the three arguments get the exception data, as #described next. def __exit__(self, exc_type, exc_value, traceback): import sys sys.stdout.write= self.original_write if exc_type is ZeroDivisionError: print('Please DO NOT divide by zero!') return True #return True to tell the interpreter that the exception was handled. #If __exit__ returns None or anything but True, any exception raised in #the with block will be propagated. The interpreter calls the __enter__ method with no arguments—beyond the implicit self. The three arguments passed to __exit__ are: exc_typeThe exception class (e.g., ZeroDivisionError). exc_valueThe exception instance. Sometimes, parameters passed to the exception constructor—such as the error message—can be found in exc_value.args. tracebackA traceback object. Exercising LookingGlass without a with block 1234567891011121314&gt;&gt;&gt; from mirror import LookingGlass &gt;&gt;&gt; manager = LookingGlass() &gt;&gt;&gt; manager &lt;mirror.LookingGlass object at 0x2a578ac&gt; &gt;&gt;&gt; monster = manager.__enter__() &gt;&gt;&gt; monster == 'JABBERWOCKY' eurT &gt;&gt;&gt; monster 'YKCOWREBBAJ' &gt;&gt;&gt; manager &gt;ca875a2x0 ta tcejbo ssalGgnikooL.rorrim&lt; &gt;&gt;&gt; manager.__exit__(None, None, None) &gt;&gt;&gt; monster 'JABBERWOCKY' The contextlib UtilitiesclosingA function to build context managers out of objects that provide a close() method but don’t implement the __enter__/__exit__ protocol. suppressA context manager to temporarily ignore specified exceptions. @contextmanagerA decorator that lets you build a context manager from a simple generator function, instead of creating a class and implementing the protocol. ContextDecoratorA base class for defining class-based context managers that can also be used as function decorators, running the entire function within a managed context. ExitStackA context manager that lets you enter a variable number of context managers. When the with block ends, ExitStack calls the stacked context managers’ __exit__ methods in LIFO order (last entered, first exited). Use this class when you don’t know beforehand how many context managers you need to enter in your with block; for example, when opening all files from an arbitrary list of files at the same time. Using @contextmanagerThe @contextmanager decorator reduces the boilerplate of creating a context manager: instead of writing a whole class with __enter__/__exit__ methods, you just implement a generator with a single yield that should produce whatever you want the __enter__ method to return. In a generator decorated with @contextmanager, yield is used to split the body of the function in two parts: everything before the yield will be executed at the beginning of the while block when the interpreter calls __enter__; the code after yield will run when __exit__ is called at the end of the block. _mirror_gen.py_ 12345678910111213141516171819202122import contextlib@contextlib.contextmanagerdef looking_glass(): import sys original_write= sys.stdout.write #Define custom reverse_write function; original_write will be available in #the closure. def reverse_write(text): original_write(text[::-1]) sys.stdout.write = reverse_write #Yield the value that will be bound to the target variable in the as clause #of the with statement. This function pauses at this point while the body of #the with executes. yield 'JABBERWOCKY' #When control exits the with block in any way, execution continues after #the yield; here the original sys.stdout.write is restored. sys.stdout.write = original_write _Test driving the looking_glass context manager function_ 123456789&gt;&gt;&gt; from mirror_gen import looking_glass &gt;&gt;&gt; with looking_glass() as what: ... print('Alice, Kitty and Snowdrop') ... print(what) ... pordwonS dna yttiK ,ecilA YKCOWREBBAJ &gt;&gt;&gt; what 'JABBERWOCKY' Essentially the contextlib.contextmanager decorator wraps the function in a class that implements the __enter__ and __exit__ methods. The __enter__ method of that class: Invokes the generator function and holds on to the generator object—let’s call it gen. Calls next(gen) to make it run to the yield keyword. Returns the value yielded by next(gen), so it can be bound to a target variable in the with/as form. When the with block terminates, the __exit__ method: Checks an exception was passed as exc_type; if so, gen.throw(exception) is invoked, causing the exception to be raised in the yield line inside the generator function body. Otherwise, next(gen) is called, resuming the execution of the generator function body after the yield. _mirror_gen_exc.py: generator-based context manager implementing ex‐ception handling_ 1234567891011121314151617181920import contextlib@contextlib.contextmanagerdef looking_glass(): import sys original_write= sys.stdout.write def reverse_write(text): original_write(text[::-1]) sys.stdout.write= reverse_write msg='' try: yield 'JABBERWOCKY' except ZeroDivisionError: msg= 'Please DO NOT divide by zero!' finally: sys.stdout.write= original_write if msg: print(msg)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C14_Iterables, Iterators, and Generators_02]]></title>
    <url>%2F2018%2F05%2F25%2FC14-Iterables-Iterators-and-Generators-02%2F</url>
    <content type="text"><![CDATA[The ArithmeticProgression class 1234567891011121314151617class ArithmeticProgression: def __init__(self, begin, step, end=None): self.begin=begin self.step=step self.end=end def __iter__(self): #This line produces a result value equal to self.begin, but coerced to #the type of the subsequent additions. result=type(self.begin+self.step)(self.begin) forever=self.end is None index = 0 while forever or result &lt; self.end: yield result index +=1 result =self.begin + self.step * index _The aritprog_gen generator function_ 12345678def aritprog_gen(begin, step, end=None): result= type(begin + step)(begin) forever=end is None index = 0 while forever or result &lt; end: yield result index +=1 result = begin + step * index Arithmetic Progression with itertoolsitertools.count function returns a generator that produces numbers. Without arguments, it produces a series of integers starting with 0. But you can provide optional start and step values to achieve a result very similar to our aritprog_gen functions: 123456789&gt;&gt;&gt; import itertools&gt;&gt;&gt; gen=itertools.count(1,.5)&gt;&gt;&gt; next(gen)1&gt;&gt;&gt; next(gen)1.5&gt;&gt;&gt; &gt;&gt;&gt; next(gen)2.0 itertools.count never stops, so if you call list(count()), Python will try to build a list larger than available memory and your machine will be very grumpy long before the call fails. On the other hand, there is the itertools.takewhile function: it produces a generator that consumes another generator and stops when a given predicate evaluates to False. So we can combine the two and write this: 123&gt;&gt;&gt; gen=itertools.takewhile(lambda n: n &lt; 3, itertools.count(1, .5))&gt;&gt;&gt; list(gen)[1, 1.5, 2.0, 2.5] _aritprog_v3.py_ 12345678import itertoolsdef aritprog_gen(begin, step, end= None): first= type(begin+ step)(begin) ap_gen=itertools.count(first, step) if end is not None: ap_gen= itertools.takewhile(lambda n: n &lt; end, ap_gen) return ap_gen Generator Functions in the Standard LibraryFiltering generator functions examples 12345678910111213141516&gt;&gt;&gt; def vowel(c): return c.lower() in 'aeiou'... &gt;&gt;&gt; list(filter(vowel, 'Aardvardk'))['A', 'a', 'a']&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.filterfalse(vowel, 'Aardvark'))['r', 'd', 'v', 'r', 'k']&gt;&gt;&gt; list(itertools.dropwhile(vowel, 'Aardvark'))['r', 'd', 'v', 'a', 'r', 'k']&gt;&gt;&gt; list(itertools.takewhile(vowel, 'Aardvark'))['A', 'a']&gt;&gt;&gt; list(itertools.compress('Aardvark', (1,0,1,1,0,1)))['A', 'r', 'd', 'a']&gt;&gt;&gt; list(itertools.islice('Aardvark', 1,7,2))['a', 'd', 'a']&gt;&gt;&gt; itertools.accumulate generator function examples 12345678910111213&gt;&gt;&gt; sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.accumulate(sample)) #[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]&gt;&gt;&gt; list(itertools.accumulate(sample, min)) #[5, 4, 2, 2, 2, 2, 2, 0, 0, 0]&gt;&gt;&gt; list(itertools.accumulate(sample, max)) #[5, 5, 5, 8, 8, 8, 8, 8, 9, 9]&gt;&gt;&gt; import operator&gt;&gt;&gt; list(itertools.accumulate(sample, operator.mul)) #[5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]&gt;&gt;&gt; list(itertools.accumulate(range(1, 11), operator.mul))[1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800] Mapping generator function examples1234567891011121314151617&gt;&gt;&gt; list(enumerate('albatroz', 1)) #[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')]&gt;&gt;&gt; import operator&gt;&gt;&gt; list(map(operator.mul, range(11), range(11))) #[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&gt;&gt;&gt; list(map(operator.mul, range(11), [2, 4, 8])) #[0, 4, 16]&gt;&gt;&gt; list(map(lambda a, b: (a, b), range(11), [2, 4, 8])) # [(0, 2), (1, 4), (2, 8)]&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.starmap(operator.mul, enumerate('albatroz', 1))) #['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz']&gt;&gt;&gt; sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]&gt;&gt;&gt; list(itertools.starmap(lambda a, b: b/a,... enumerate(itertools.accumulate(sample), 1))) #[5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333,5.0, 4.375, 4.888888888888889, 4.5] Merging generator function examples 123456789101112131415&gt;&gt;&gt; list(itertools.chain('ABC', range(2))) #['A', 'B', 'C', 0, 1]&gt;&gt;&gt; list(itertools.chain(enumerate('ABC'))) #chain does nothing useful when called with a single iterable[(0, 'A'), (1, 'B'), (2, 'C')]&gt;&gt;&gt; list(itertools.chain.from_iterable(enumerate('ABC'))) #[0, 'A', 1, 'B', 2, 'C']&gt;&gt;&gt; list(zip('ABC', range(5))) #[('A', 0), ('B', 1), ('C', 2)]&gt;&gt;&gt; list(zip('ABC', range(5), [10, 20, 30, 40])) #[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5))) #[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5), fillvalue='?')) #[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)] itertools.product generator function examples 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; list(itertools.product('ABC', range(2))) #[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]&gt;&gt;&gt; suits = 'spades hearts diamonds clubs'.split()&gt;&gt;&gt; list(itertools.product('AK', suits)) #[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'),('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')]&gt;&gt;&gt; list(itertools.product('ABC')) #[('A',), ('B',), ('C',)]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2)) #[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'),('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]#The repeat=N keyword argument tells product to consume each input iterable N times.&gt;&gt;&gt; list(itertools.product(range(2), repeat=3))[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),(1, 0, 1), (1, 1, 0), (1, 1, 1)]&gt;&gt;&gt; rows = itertools.product('AB', range(2), repeat=2)&gt;&gt;&gt; for row in rows: print(row)...('A', 0, 'A', 0)('A', 0, 'A', 1)('A', 0, 'B', 0)('A', 0, 'B', 1)('A', 1, 'A', 0)('A', 1, 'A', 1)('A', 1, 'B', 0)('A', 1, 'B', 1)('B', 0, 'A', 0)('B', 0, 'A', 1)('B', 0, 'B', 0)('B', 0, 'B', 1)('B', 1, 'A', 0)('B', 1, 'A', 1)('B', 1, 'B', 0)('B', 1, 'B', 1) Generator functions that expand each input item into multiple output items count, cycle, and repeat 12345678910111213141516171819&gt;&gt;&gt; ct = itertools.count() #&gt;&gt;&gt; next(ct) #0&gt;&gt;&gt; next(ct), next(ct), next(ct) #(1, 2, 3)&gt;&gt;&gt; list(itertools.islice(itertools.count(1, .3), 3)) #[1, 1.3, 1.6]&gt;&gt;&gt; cy = itertools.cycle('ABC') #&gt;&gt;&gt; next(cy)'A'&gt;&gt;&gt; list(itertools.islice(cy, 7)) #['B', 'C', 'A', 'B', 'C', 'A', 'B']&gt;&gt;&gt; rp = itertools.repeat(7) #&gt;&gt;&gt; next(rp), next(rp)(7, 7)&gt;&gt;&gt; list(itertools.repeat(8, 4)) #[8, 8, 8, 8]&gt;&gt;&gt; list(map(operator.mul, range(11), itertools.repeat(5))) #[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50] Combinatoric generator functions yield multiple values per input item123456789&gt;&gt;&gt; list(itertools.combinations('ABC', 2)) #All combinations of len()==2 from the items in 'ABC'[('A', 'B'), ('A', 'C'), ('B', 'C')]&gt;&gt;&gt; list(itertools.combinations_with_replacement('ABC', 2)) #including combinations with repeated items.[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]&gt;&gt;&gt; list(itertools.permutations('ABC', 2)) #All permutations of len()==2 from the items in 'ABC'[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2)) #Cartesian product from 'ABC' and 'ABC' (that’s the effect of repeat=2)[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] Rearranging generator functions Note that itertools.groupby assumes that the input iterable is sorted by the grouping criterion, or at least that the items are clustered by that criterion—even if not sorted. itertools.groupby 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; list(itertools.groupby('LLLLAAGGG')) #[('L', &lt;itertools._grouper object at 0x102227cc0&gt;),('A', &lt;itertools._grouper object at 0x102227b38&gt;),('G', &lt;itertools._grouper object at 0x102227b70&gt;)]&gt;&gt;&gt; for char, group in itertools.groupby('LLLLAAAGG'): #... print(char, '-&gt;', list(group))...L -&gt; ['L', 'L', 'L', 'L']A -&gt; ['A', 'A',]G -&gt; ['G', 'G', 'G']&gt;&gt;&gt; animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear',... 'bat', 'dolphin', 'shark', 'lion']&gt;&gt;&gt; animals.sort(key=len) #To use groupby, the input should be sorted;&gt;&gt;&gt; animals['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark','giraffe', 'dolphin']&gt;&gt;&gt; for length, group in itertools.groupby(animals, len): #... print(length, '-&gt;', list(group))...3 -&gt; ['rat', 'bat']4 -&gt; ['duck', 'bear', 'lion']5 -&gt; ['eagle', 'shark']7 -&gt; ['giraffe', 'dolphin']&gt;&gt;&gt; for length, group in itertools.groupby(reversed(animals), len): #... print(length, '-&gt;', list(group))...7 -&gt; ['dolphin', 'giraffe']5 -&gt; ['shark', 'eagle']4 -&gt; ['lion', 'bear', 'duck']3 -&gt; ['bat', 'rat'] itertools.tee yields multiple generators, each yielding every item of the input generator123456789101112131415&gt;&gt;&gt; list(itertools.tee('ABC'))[&lt;itertools._tee object at 0x10222abc8&gt;, &lt;itertools._tee object at 0x10222ac08&gt;]&gt;&gt;&gt; g1, g2 = itertools.tee('ABC')&gt;&gt;&gt; next(g1)'A'&gt;&gt;&gt; next(g2)'A'&gt;&gt;&gt; next(g2)'B'&gt;&gt;&gt; list(g1)['B', 'C']&gt;&gt;&gt; list(g2)['C']&gt;&gt;&gt; list(zip(*itertools.tee('ABC')))[('A', 'A'), ('B', 'B'), ('C', 'C')] New Syntax in Python 3.3: yield from a homemade implementation of a chaining generator: 123456789101112131415161718&gt;&gt;&gt; def chain(*iterables):... for it in iterables:... for i in it:... yield i... &gt;&gt;&gt; s='ABC'&gt;&gt;&gt; t=tuple(range(3))&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2]&gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2]&gt;&gt;&gt; def chain(*iterables):... for i in iterables:... yield from i... &gt;&gt;&gt; list(chain(s, t))['A', 'B', 'C', 0, 1, 2] As you can see, yield from i replaces the inner for loop completely. The use of yield from in this example is correct, and the code reads better, but it seems like mere syntactic sugar. Besides replacing a loop, yield from creates a channel connecting the inner generator directly to the client of the outer generator. This channel becomes really important when generators are used as coroutines and not only produce but also consume values from the client code. Iterable Reducing FunctionsBuilt-in functions that read iterables and return single values 12345678910111213141516171819&gt;&gt;&gt; all([1, 2, 3])True&gt;&gt;&gt; all([1, 0, 3])False&gt;&gt;&gt; all([])True&gt;&gt;&gt; any([1, 2, 3])True&gt;&gt;&gt; any([1, 0, 3])True&gt;&gt;&gt; any([0, 0.0])False&gt;&gt;&gt; any([])False&gt;&gt;&gt; g = (n for n in [0, 0.0, 7, 8])&gt;&gt;&gt; any(g)True&gt;&gt;&gt; next(g)8 Another built-in that takes an iterable and returns something else is sorted. Unlike reversed, which is a generator function, sorted builds and returns an actual list. After all, every single item of the input iterable must be read so they can be sorted, and the sorting happens in a list, therefore sorted just returns that list after it’s done. I mention sorted here because it does consume an arbitrary iterable. Of course, sorted and the reducing functions only work with iterables that eventually stop. Otherwise, they will keep on collecting items and never return a result. A Closer Look at the iter Functioniter has another trick: it can be called with two arguments to create an iterator from a regular function or any callable object. In this usage, the first argument must be a callable to be invoked repeatedly (with no arguments) to yield values, and the second argument is a sentinel: a marker value which, when returned by the callable, causes the iterator to raise StopIteration instead of yielding the sentinel. The following example shows how to use iter to roll a six-sided die until a 1 is rolled: 12345678910111213&gt;&gt;&gt; def d6():... return randint(1, 6)...&gt;&gt;&gt; d6_iter = iter(d6, 1)&gt;&gt;&gt; d6_iter&lt;callable_iterator object at 0x00000000029BE6A0&gt;&gt;&gt;&gt; for roll in d6_iter:... print(roll)...4363 reads lines from a file until a blank line is found or the end of file is reached: 123with open('mydata.txt') as fp: for line in iter(fp.readline, ''): process_line(line)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C14_Iterables, Iterators, and Generators_01]]></title>
    <url>%2F2018%2F05%2F25%2FC14-Iterables-Iterators-and-Generators-01%2F</url>
    <content type="text"><![CDATA[Every collection in Python is iterable, and iterators are used internally to support:• for loops• Collection types construction and extension• Looping over text files line by line• List, dict, and set comprehensions• Tuple unpacking• Unpacking actual parameters with * in function callssentence.py: A Sentence as a sequence of words 123456789101112131415161718192021import reimport reprlibRE_WORD=re.compile('\w+')class Sentence: def __init__(self, text): self.text = text self.words=RE_WORD.findall(text) def __getitem__(self, index): return self.words[index] def __len__(self): return len(self.words) # reprlib.repr is a utility function to generate abbreviated string # representations of data structures that can be very large. def __repr__(self): return 'Sentence(%s)'% reprlib.repr(self.text) Why Sequences Are Iterable: The iter FunctionWhenever the interpreter needs to iterate over an object x, it automatically calls iter(x). The iter built-in function: Checks whether the object implements __iter__, and calls that to obtain an iterator. If __iter__ is not implemented, but __getitem__ is implemented, Python creates an iterator that attempts to fetch items in order, starting from index 0 (zero). If that fails, Python raises TypeError, usually saying “C object is not iterable,” where C is the class of the target object. That is why any Python sequence is iterable: they all implement __getitem__. In fact,the standard sequences also implement __iter__, and yours should too, because the special handling of __getitem__ exists for backward compatibility reasons and may be gone in the future. In the goose-typing approach, the definition for an iterable is simpler but not as flexible: an object is considered iterable if it implements the __iter__ method. No subclassing or registration is required, because abc.Iterable implements the __subclasshook__. 12345678910&gt;&gt;&gt; class Foo:... def __iter__(self):... pass...&gt;&gt;&gt; from collections import abc&gt;&gt;&gt; issubclass(Foo, abc.Iterable)True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; isinstance(f, abc.Iterable)True However, note that our initial Sentence class does not pass the issubclass(Sentence, abc.Iterable) test, even though it is iterable in practice. Iterables Versus IteratorsiterableAny object from which the iter built-in function can obtain an iterator. Objects implementing an __iter__ method returning an iterator are iterable. Sequences are always iterable; as are objects implementing a __getitem__ method that takes 0-based indexes. It’s important to be clear about the relationship between iterables and iterators: Python obtains iterators from iterables. 123456789101112&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; it = iter(s) # Build an iterator it from the iterable.&gt;&gt;&gt; while True:... try:... print(next(it)) # Repeatedly call next on the iterator to obtain the next item.... except StopIteration: # The iterator raises StopIteration when there are no further items.... del it # Release reference to it—the iterator object is discarded.... break #...ABC StopIteration signals that the iterator is exhausted. This exception is handled internally in for loops and other iteration contexts like list comprehensions, tuple unpacking, etc. The standard interface for an iterator has two methods: __next__Returns the next available item, raising StopIteration when there are no more items. __iter__Returns self; this allows iterators to be used where an iterable is expected, for example, in a for loop. This is formalized in the collections.abc.Iterator ABC, which defines the __next__ abstract method, and subclasses Iterable—where the abstract __iter__ method is defined. The best way to check if an object x is an iterator is to call isinstance(x, abc.Iterator). Thanks to Iterator.__subclasshook__, this test works even if the class of x is not a real or virtual subclass of Iterator. iteratorAny object that implements the__next__ no-argument method that returns the next item in a series or raises StopIteration when there are no more items. Python iterators also implement the __iter__ method so they are iterable as well. A Classic Iterator_sentence_iter.py_ 123456789101112131415161718192021222324252627282930313233import reimport reprlibRE_WORD=re.compile('\w+')class Sentence: def __init__(self, text): self.text=text self.words=RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return SentenceIterator(self.words)class SentenceIterator: def __init__(self, words): self.words=words self.index=0 def __next__(self): try: word=self.words[self.index] except IndexError: raise StopIteration() self.index += 1 return word def __iter__(self): return self Note that implementing__iter__ in SentenceIterator is not actually needed for this example to work, but the it’s the right thing to do: iterators are supposed to implement both __next__ and __iter__, and doing so makes our iterator pass the issubclass(Sen tenceInterator, abc.Iterator) test. If we had subclassed SentenceIterator from abc.Iterator, we’d inherit the concrete abc.Iterator.__iter__ method. Making Sentence an Iterator: Bad IdeaA common cause of errors in building iterables and iterators is to confuse the two. To be clear: iterables have an __iter__ method that instantiates a new iterator every time. Iterators implement a __next__ method that returns individual items, and an __iter__ method that returns self. Therefore, iterators are also iterable, but iterables are not iterators. It may be tempting to implement __next__ in addition to __iter__ in the Sentence class, making each Sentence instance at the same time an iterable and iterator over itself. But this is a terrible idea. It’s also a common anti-pattern, according to Alex Martelli who has a lot of experience with Python code reviews. A Generator Function _sentence_gen.py_ 123456789101112131415161718192021import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text self.words= RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for word in self.words: yield word return #This return is not needed; the function can just “fall-through” and return#automatically. Either way, a generator function doesn’t raise StopIteration: #it simply exits when it’s done producing values. __iter__ is a generator function which, when called, builds a generator object that implements the iterator interface, so the SentenceIterator class is no longer needed. How a Generator Function WorksAny Python function that has the yield keyword in its body is a generator function: a function which, when called, returns a generator object. In other words, a generator function is a generator factory. 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; def gen_123():... yield 1... yield 2... yield 3... &gt;&gt;&gt; gen_123&lt;function gen_123 at 0x05155C90&gt;&gt;&gt;&gt; gen_123()&lt;generator object gen_123 at 0x05157540&gt; #when invoked, gen_123() returns a generator object.#Generators are iterators that produce the values of the expressions passed to yield.# To iterate, the for machinery does the equivalent of g = iter(gen_123()) to get a generator object, and # then next(g) at each iteration.&gt;&gt;&gt; for i in gen_123(): print(i)... 123&gt;&gt;&gt; g=gen_123()&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)2&gt;&gt;&gt; next(g)3&gt;&gt;&gt; next(g)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration When the generator function body runs to the end, the generator object raises StopIteration. The for loop machinery catches that exception, and the loop terminates cleanly. A Lazy ImplementationThe Iterator interface is designed to be lazy: next(my_iterator) produces one item at a time. The opposite of lazy is eager: lazy evaluation and eager evaluation are actual technical terms in programming language theory. Our Sentence implementations so far have not been lazy because the __init__ eagerly builds a list of all words in the text, binding it to the self.words attribute. This will entail processing the entire text, and the list may use as much memory as the text itself. The re.finditer function is a lazy version of re.findall which, instead of a list, returns a generator producing re.MatchObject instances on demand. If there are many matches, re.finditer saves a lot of memory. Using it, our third version of Sentence is now lazy: it only produces the next word when it is needed. _sentence_gen2.py_ 1234567891011121314151617181920import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): #finditer builds an iterator over the matches of RE_WORD on self.text, # yielding MatchObject instances. for match in RE_WORD.finditer(self.text): yield match.group() #match.group() extracts the actual matched text from the #MatchObject instance A Generator ExpressionSimple generator functions like the one in the previous Sentence class can be replaced by a generator expression. A generator expression can be understood as a lazy version of a list comprehension: it does not eagerly build a list, but returns a generator that will lazily produce the items on demand. In other words, if a list comprehension is a factory of lists, a generator expression is a factory of generators. _sentence_genexp.py: Sentence implemented using a generator expression_ 123456789101112131415import reimport reprlibRE_WORD= re.compile('\w+')class Sentence: def __init__(self, text): self.text= text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text))]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C13_Operator Overloading: Doing It Right]]></title>
    <url>%2F2018%2F05%2F25%2FC13-Operator-Overloading-Doing-It-Right%2F</url>
    <content type="text"><![CDATA[We cannot overload operators for the built-in types. We cannot create new operators, only overload existing ones. A few operators can’t be overloaded: is, and, or, not (but the bitwise &amp;, |, ~, can). Unary OperatorsIt’s easy to support the unary operators. Simply implement the appropriate special method, which will receive just one argument: self. Use whatever logic makes sense in your class, but stick to the fundamental rule of operators: always return a new object. In other words, do not modify self, but create and return a new instance of a suitable type. In the case of - and +, the result will probably be an instance of the same class as self; for +, returning a copy of self is the best approach most of the time. For abs(…), the result should be a scalar number. As for ~, it’s difficult to say what would be a sensible result if you’re not dealing with bits in an integer, but in an ORM it could make sense to return the negation of an SQL WHERE clause, for example. _vector_v6.py: unary operators - and + added_ 123456def __abs__(self): return math.sqrt(sum(x * x for x in self)) def __neg__(self): return Vector(-x for x in self) def __pos__(self): #To compute +v, build a new Vector with every component of self. return Vector(self) Overloading + for Vector Addition1234**_Vector.add method, take #1_**def __add__(self, other): pairs=itertools.zip_longest(self, other, fillvalue = 0.0) return Vector(a +b for a, b in pairs) To support operations involving objects of different types, Python implements a special dispatching mechanism for the infix operator special methods. Given an expression a+ b, the interpreter will perform these steps: If a has __add__, call a.__add__(b) and return result unless it’s NotImplemented. If a doesn’t have __add__, or calling it returns NotImplemented, check if b has __radd__, then call b.__radd__(a) and return result unless it’s NotImplemented. If b doesn’t have __add__, or calling it returns NotImplemented, raise TypeError with an unsupported operand types message. The __radd__ method is called the “reflected” or “reversed” version of __add__. I prefer to call them “reversed” special methods. Three of this book’s technical reviewers—Alex,Anna, and Leo—told me they like to think of them as the “right” special methods, because they are called on the righthand operand. Therefore, to make the mixed-type additions work, we need to implement the Vector.__radd__ method, which Python will invoke as a fall back if the left operand does not implement __add__ or if it does but returns NotImplemented to signal that it doesn’t know how to handle the right operand. Vector.add and radd methods1234567def __add__(self, other): pairs=itertools.zip_longest(self, other, fillvalue = 0.0) return Vector(a +b for a, b in pairs)def __radd__(self, other): return self + other The methods work with Vector objects, or any iterable with numeric items, such as a Vector2d, a tuple of integers, or an array of floats. But if provided with a noniterable object,__add__ fails with a message that is not very helpful. Vector.add method needs an iterable operand 123456&gt;&gt;&gt; v1 + 1Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "vector_v6.py", line 328, in __add__ pairs = itertools.zip_longest(self, other, fillvalue=0.0)TypeError: zip_longest argument #2 must support iteration Vector.add method needs an iterable with numeric items 12345678910&gt;&gt;&gt; v1 + 'ABC'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "vector_v6.py", line 329, in __add__ return Vector(a + b for a, b in pairs) File "vector_v6.py", line 243, in __init__ self._components = array(self.typecode, components) File "vector_v6.py", line 329, in &lt;genexpr&gt; return Vector(a + b for a, b in pairs)TypeError: unsupported operand type(s) for +: 'float' and 'str' In the spirit of duck typing, we will refrain from testing the type of the other operand, or the type of its elements. We’ll catch the exceptions and return NotImplemented.By returning NotImplemented, you leave the door open for the implementer of the other operand type to perform the operation when Python tries the reversed method call. If the reverse method call returns NotImplemented, then Python will raise issue TypeError with a standard error message like “unsupported operand type(s) for +: Vector and str.” _vector_v6.py: operator + methods added to vector_v5.py_12345678910def __add__(se;f, other): try: pairs=itertools.zip_longest(self, other, fillvalue= 0.0) return Vector(a+b for a, b in pairs) except TypeError: return NotImplementeddef __radd__(self, other): return self + other Overloading * for Scalar MultiplicationWe could use the same duck typing technique and catch aTypeError in __mul__, but there is another, more explicit way that makes sense in this situation: goose typing. We use isinstance() to check the type of scalar, but instead of hardcoding some concrete types, we check against the numbers.Real ABC, which covers all the types we need, and keeps our implementation open to future numeric types that declare themselves actual or virtual subclasses of the numbers.Real ABC. _vector_v7.py: operator * methods added_ 12345678910111213141516171819202122from array import arrayimport reprlibimport mathimport functoolsimport operatorsimport itertoolsimport numbersclass Vector: typecode='d' def __init__(self, components): self._components=array(self.typecode, compoents) def __mul__(self, scalar): if isinstance(scalar, number.Real): return Vector(n*scalar for n in self) else: return NotImplemented def __rmul__(self, scalar): return self * scalar Rich Comparison OperatorsThe handling of the rich comparison operators ==, !=, &gt;, &lt;, &gt;=, &lt;= by the Python interpreter is similar to what we just saw, but differs in two important aspects: The same set of methods are used in forward and reverse operator calls. The rules are summarized in Table 13-2. For example, in the case of ==, both the forward and reverse calls invoke __eq__, only swapping arguments; and a forward call to __gt__ is followed by a reverse call to __lt__ with the swapped arguments. In the case of == and !=, if the reverse call fails, Python compares the object IDs instead of raising TypeError. _vector_v8.py: improved eq in the Vector class_ 12345678910111213141516171819def __eq__(self, other): if isinstance(other, Vector): return (len(self) == len(other) and all(a==b for a,b in zip(self, other))) else: return NotImplemented**_Same comparisons_:**&gt;&gt;&gt; va = Vector([1.0, 2.0, 3.0])&gt;&gt;&gt; vb = Vector(range(1, 4))&gt;&gt;&gt; va == vb #True&gt;&gt;&gt; vc = Vector([1, 2])&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; v2d = Vector2d(1, 2)&gt;&gt;&gt; vc == v2d #Same result as before, but why? Explanation coming up.True&gt;&gt;&gt; t3 = (1, 2, 3)&gt;&gt;&gt; va == t3 #False Here is what happens in the example with a Vector and a Vector2d, step by step: To evaluate vc == v2d, Python calls Vector.__eq__(vc, v2d). Vector.__eq__(vc, v2d) verifies that v2d is not a Vector and returns NotImplemented. Python gets NotImplemented result, so it tries Vector2d.__eq__(v2d, vc). Vector2d.__eq__(v2d, vc) turns both operands into tuples an compares them: the result is True. As for the comparison between Vector and tuple, the actual steps are: To evaluate va == t3, Python calls Vector.__eq__(va, t3). Vector.__eq__(va, t3) verifies that t3 is not a Vector and returns NotImplemented. Python gets NotImplemented result, so it tries tuple.__eq__(t3, va). tuple.__eq__(t3, va) has no idea what a Vector is, so it returns NotImplemented. In the special case of ==, if the reversed call returns NotImplemented, Python compares object IDs as a last resort. How about !=? We don’t need to implement it because the fallback behavior of the __ne__ inherited from object suits us: when __eq__ is defined and does not return NotImplemented, __ne__ returns that result negated. If a class does not implement the in-place operators., the augmented assignment operators are just syntactic sugar: a += b is evaluated exactly as a = a +b. That’s the expected behavior for immutable types, and if you have __add__ then += will work with no additional code. However, if you do implement an in-place operator method such as __iadd__, that method is called to compute the result of a += b. As the name says, those operators are expected to change the lefthand operand in place, and not create a new object as the result. bingoaddable.py: AddableBingoCage extends BingoCage to support + and +=123456789101112131415161718192021222324import itertoolsfrom tombola import Tombolafrom bingo import BingoCageclass AddableBingoCage(BingoCage): def __add__(self, other): if isinstance(other, Tombola): return AddableBingoCage(self.inspect()+other.inspect()) else: return NotImplemented def __iadd__(self, other): if isinstance(other, Tombola): other_iterable=other.inspect() else: try: other_iterable=iter(other) except TypeError: self_cls=type(self).__name__ msg="right operand in += must be &#123;!r&#125; or an iterable" raise TypeError(msg.format(self_cls)) self.load(other_iterable) return self __add__The result is produced by calling the constructor AddableBingoCage to build a new instance. __iadd__The result is produced by returning self, after it has been modified.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C12_Inheritance: For Good or For Worse_02]]></title>
    <url>%2F2018%2F05%2F25%2FC12-Inheritance-For-Good-or-For-Worse-02%2F</url>
    <content type="text"><![CDATA[Distinguish Interface Inheritance from Implementation InheritanceWhen dealing with multiple inheritance, it’s useful to keep straight the reasons why subclassing is done in the first place. The main reasons are: Inheritance of interface creates a subtype, implying an “is-a” relationship. Inheritance of implementation avoids code duplication by reuse. In practice, both uses are often simultaneous, but whenever you can make the intent clear, do it. Inheritance for code reuse is an implementation detail, and it can often be replaced by composition and delegation. On the other hand, interface inheritance is the backbone of a framework. Make Interfaces Explicit with ABCsIn modern Python, if a class is designed to define an interface, it should be an explicit ABC. Use Mixins for Code ReuseIf a class is designed to provide method implementations for reuse by multiple unrelated subclasses, without implying an “is-a” relationship, it should be an explicit mixin class.Conceptually, a mixin does not define a new type; it merely bundles methods for reuse. A mixin should never be instantiated, and concrete classes should not inherit only from a mixin. Each mixin should provide a single specific behavior, implementing few and very closely related methods. Make Mixins Explicit by NamingThere is no formal way in Python to state that a class is a mixin, so it is highly recommended that they are named with a …Mixin suffix. Tkinter does not follow this advice, but if it did, XView would be XViewMixin, Pack would be PackMixin, and so on. An ABC May Also Be a Mixin; The Reverse Is Not TrueBecause an ABC can implement concrete methods, it works as a mixin as well. An ABC also defines a type, which a mixin does not. And an ABC can be the sole base class of any other class, while a mixin should never be subclassed alone except by another, more specialized mixin—not a common arrangement in real code. One restriction applies to ABCs and not to mixins: the concrete methods implemented in an ABC should only collaborate with methods of the same ABC and its superclasses.This implies that concrete methods in an ABC are always for convenience, because everything they do, a user of the class can also do by calling other methods of the ABC. Don’t Subclass from More Than One Concrete ClassConcrete classes should have zero or at most one concrete superclass.In other words, all but one of the superclasses of a concrete class should be ABCs or mixins. For example, in the following code, if Alpha is a concrete class, then Beta and Gamma must be ABCs or mixins: 123class MyConcreteClass(Alpha, Beta, Gamma): """This is a concrete class: it can be instantiated.""" # ... more code ... Provide Aggregate Classes to UsersIf some combination of ABCs or mixins is particularly useful to client code, provide a class that brings them together in a sensible way. Grady Booch calls this an aggregate class. 12345class Widget(BaseWidget, Pack, Place, Grid): """Internal class. Base class for a widget which can be positioned with the geometry managers Pack, Place or Grid.""" pass The body of Widget is empty, but the class provides a useful service: it brings together four superclasses so that anyone who needs to create a new widget does not need to remember all those mixins, or wonder if they need to be declared in a certain order in a class statement. “Favor Object Composition Over Class Inheritance.”Favoring composition leads to more flexible designs. For example, in the case of the tkinter.Widget class, instead of inheriting the methods from all geometry managers, widget instances could hold a reference to a geometry manager, and invoke its methods. After all, a Widget should not “be” a geometry manager, but could use the services of one via delegation. Then you could add a new geometry manager without touching the widget class hierarchy and without worrying about name clashes. Even with single inheritance, this principle enhances flexibility, because subclassing is a form of tight coupling, and tall inheritance trees tend to be brittle. Composition and delegation can replace the use of mixins to make behaviors available to different classes, but cannot replace the use of interface inheritance to define a hierarchy of types.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C12_Inheritance: For Good or For Worse_01]]></title>
    <url>%2F2018%2F05%2F25%2FC12-Inheritance-For-Good-or-For-Worse-01%2F</url>
    <content type="text"><![CDATA[It was not possible to subclass built-in types such as list or dict. Since then, it can be done but there is a major caveat: the code of the built-ins (written in C) does not call special methods overridden by user-defined classes.1234567891011121314151617181920&gt;&gt;&gt; class DoppelDict(dict):... def __setitem__(self, key, value):... super().__setitem__(key, [value] * 2)... &gt;&gt;&gt; dd = DoppelDict(one=1)&gt;&gt;&gt; dd&#123;'one': 1&#125;#The __init__ method inherited from dict clearly ignored that __setitem__ was overridden: the value of #'one' is not duplicated.&gt;&gt;&gt; dd['two'] = 2&gt;&gt;&gt; dd&#123;'one': 1, 'two': [2, 2]&#125;#The [] operator calls our __setitem__ and works as expected&gt;&gt;&gt; dd.update(three=3)&gt;&gt;&gt; dd&#123;'one': 1, 'two': [2, 2], 'three': 3&#125;# The update method from dict does not use our version of __setitem__ either This built-in behavior is a violation of a basic rule of object-oriented programming: the search for methods should always start from the class of the target instance (self), even when the call happens inside a method implemented in a superclass. If you subclass collections.UserDict instead of dict, the issues exposed are both fixed.123456789101112131415&gt;&gt;&gt; import collections&gt;&gt;&gt; class DoppleDict2(collections.UserDict):... def __setitem__(self,key,value):... super().__setitem__(key,[value]*2)... &gt;&gt;&gt; dd=DoppleDict2(one=1)&gt;&gt;&gt; dd&#123;'one': [1, 1]&#125;&gt;&gt;&gt; dd['two']=2&gt;&gt;&gt; dd&#123;'one': [1, 1], 'two': [2, 2]&#125;&gt;&gt;&gt; dd.update(three=3)&gt;&gt;&gt; dd&#123;'one': [1, 1], 'two': [2, 2], 'three': [3, 3]&#125; To summarize: the problem described in this section applies only to method delegation within the C language implementation of the built-in types, and only affects user defined classes derived directly from those types. If you subclass from a class coded in Python, such as UserDict or MutableMapping, you will not be troubled by this. Multiple Inheritance and Method Resolution Order 123456789101112131415161718192021222324class A: def ping(self): print('ping:',self)class B(A): def pong(self): print('pong:',self)class C(A): def pong(self): print('PONG:',self)class D(B,C): def ping(self): super().ping() print('post-ping:',self) def pingpong(self): self.ping() super().ping() self.pong() super().pong() C.pong(self) Two ways of invoking method pong on an instance of class D123456&gt;&gt;&gt; from diamond import *&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.pong() #pong: &lt;diamond.D object at 0x10066c278&gt;&gt;&gt;&gt; C.pong(d) #PONG: &lt;diamond.D object at 0x10066c278&gt; The ambiguity of a call like d.pong() is resolved because Python follows a specific order when traversing the inheritance graph. That order is called MRO: Method Resolution Order. Classes have an attribute called __mro__ holding a tuple of references to the superclasses in MRO order, from the current class all the way to the object class. For the D class, this is the __mro__. 12&gt;&gt;&gt; D.__mro__(&lt;class 'diamond.D'&gt;, &lt;class 'diamond.B'&gt;, &lt;class 'diamond.C'&gt;,&lt;class 'diamond.A'&gt;, &lt;class 'object'&gt;) The recommended way to delegate method calls to superclasses is the super() built-in function.It’s safest and more future-proof to use super(), especially when calling methods on a framework, or any class hierarchies you do not control. The five calls made by pingpong: 123456789&gt;&gt;&gt; from diamond import D&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.pingpong()ping: &lt;diamond.D object at 0x10bf235c0&gt; #post-ping: &lt;diamond.D object at 0x10bf235c0&gt;ping: &lt;diamond.D object at 0x10bf235c0&gt; #pong: &lt;diamond.D object at 0x10bf235c0&gt; #pong: &lt;diamond.D object at 0x10bf235c0&gt; #PONG: &lt;diamond.D object at 0x10bf235c0&gt; #]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C11_ABCs in the Standard Library]]></title>
    <url>%2F2018%2F05%2F25%2FC11-ABCs-in-the-Standard-Library%2F</url>
    <content type="text"><![CDATA[ABCs are available in the standard library. Most are defined in the collections.abc module, but there are others. You can find ABCs in the numbers and io packages ABCs in collections.abc Iterable, Container, and SizedEvery collection should either inherit from these ABCs or at least implement compatible protocols. Iterable supports iteration with __iter__, Container supports the in operator with __contains__, and Sized supports len() with __len__. Sequence, Mapping, and SetThese are the main immutable collection types, and each has a mutable subclass. A detailed diagram for MutableSequence is in Figure 11-2; for MutableMapping and MutableSet, there are diagrams in Chapter 3. MappingViewIn Python 3, the objects returned from the mapping methods .items(), .keys(), and .values() inherit from ItemsView, ValuesView, and ValuesView, respectively. The first two also inherit the rich interface of Set. Callable and HashableThese ABCs are not so closely related to collections, but collections.abc was the first package to define ABCs in the standard library, and these two were deemed important enough to be included. I’ve never seen subclasses of either Callable or Hashable. Their main use is to support the insinstance built-in as a safe way of determining whether an object is callable or hashable. IteratorNote that iterator subclasses Iterable. The Numbers Tower of ABCsThe numbers package defines the so-called “numerical tower” ,where Number is the topmost superclass, Complex is its immediate subclass, and so on, down to Integral: Number Complex Real Rational Integral So if you need to check for an integer, use isinstance(x, numbers.Integral) to accept int, bool (which subclasses int) or other integer types that may be provided by external libraries that register their types with the numbers ABCs. If, on the other hand, a value can be a floating-point type, you write isinstance(x, numbers.Real), and your code will happily take bool, int, float, fractions.Fraction, or any other noncomplex numerical type provided by an external library, such as NumPy. Defining and Using an ABCThe Tombola ABC has four methods. The two abstract methods are:.load(…): put items into the container..pick(): remove one item at random from the container, returning it. The concrete methods are:.loaded(): return True if there is at least one item in the container. .inspect(): return a sorted tuple built from the items currently in the container, without changing its contents (its internal ordering is not preserved). tombola.py: Tombola is an ABC with two abstract methods and two concrete methods 123456789101112131415161718192021222324252627282930import abcclass Tombola(abc.ABC): #To define an ABC, subclass abc.ABC @abc.abstractclassmethod def load(self, iterable): """Add items from an iterable""" @abc.abstractclassmethod def pick(self): ''' Remove item at random, returning it. This method should raise LookUpError when the instance is empty ''' def loaded(self): #An ABC may include concrete methods. ''' return True if there is at least 1 item, False otherwise ''' #Concrete methods in an ABC must rely only on the interface defined by the ABC return bool(self.inspect()) def inspect(self): '''Return a sorted tuple with the items currently inside''' items=[] while True: try: items.append(self.pick()) except LookupError: break self.load(items) return tuple(sorted(items)) ABC Syntax DetailsThe best way to declare an ABC is to subclass abc.ABC or any other ABC. However, the abc.ABC class is new in Python 3.4, so if you are using an earlier version of Python—and it does not make sense to subclass another existing ABC—then you must use the metaclass= keyword in the class statement, pointing to abc.ABCMeta (not abc.ABC). The metaclass= keyword argument was introduced in Python 3. In Python 2, you must use the __metaclass__ class attribute. Subclassing the Tombola ABCThis BingoCage implements the required abstract methods load and pick, inherits loaded from Tombola, overrides inspect, and adds __call__. bingo.py: BingoCage is a concrete subclass of Tombola 1234567891011121314151617181920212223242526272829303132333435363738394041424344import randomfrom tombola import Tombolaclass BingoCage(Tombola): def __init__(self): self._randomizer=random.SystemRandom() self._items=[] self.load(items) def load(self, items): self._items.extend(items) self._randomizer.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError(&apos;pick from empty BingoCage&apos;) def __call__(self): self.pick()from tombola import Tombolaclass BingoCage(Tombola): def __init__(self): self._randomizer=random.SystemRandom() self._items=[] self.load(items) def load(self, items): self._items.extend(items) self._randomizer.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError(&apos;pick from empty BingoCage&apos;) def __call__(self): self.pick() A Virtual Subclass of TombolaVirtual subclasses do not inherit from their registered ABCs, and are not checked for conformance to the ABC interface at any time, not even when they are instantiated. It’s up to the subclass to actually implement all the methods needed to avoid runtime errors. The register method is usually invoked as a plain function, but it can also be used as a decorator. We use the decorator syntax and implement TomboList, a virtual subclass of Tombola. tombolist.py: class TomboList is a virtual subclass of Tombola 12345678910111213141516171819202122from random import randrangefrom tombola import Tombola#Tombolist is registered as a virtual subclass of Tombola.@Tombola.registerclass TomboList(list): def pick(self): if self: position=randrange(len(self)) return self.pop(position) else: raise LookupError('pop from empty TomboList') #Tombolist.load is the same as list.extend. load=list.extend def loaded(self): return bool(self) def inspect(self): return tuple(sorted(self)) Note that because of the registration, the functions issubclass and isinstance act as if TomboList is a subclass of Tombola: 1234567&gt;&gt;&gt; from tombola import Tombola&gt;&gt;&gt; from tombolist import TomboList&gt;&gt;&gt; issubclass(TomboList, Tombola)True&gt;&gt;&gt; t = TomboList(range(100))&gt;&gt;&gt; isinstance(t, Tombola)True However, inheritance is guided by a special class attribute named __mro__—the Method Resolution Order. It basically lists the class and its superclasses in the order Python uses to search for methods. If you inspect the mro of TomboList, you’ll see that it lists only the “real” superclasses—list and object: 12&gt;&gt;&gt; TomboList.__mro__(&lt;class 'tombolist.TomboList'&gt;, &lt;class 'list'&gt;, &lt;class 'object'&gt;) Tombola is not in Tombolist.__mro__, so Tombolist does not inherit any methods from Tombola. How the Tombola Subclasses Were Testedtest the Tombola example: __subclasses__()Method that returns a list of the immediate subclasses of the class. The list does not include virtual subclasses. _abc_registryData attribute—available only in ABCs—that is bound to a WeakSet with weak references to registered virtual subclasses of the abstract class. Usage of register in PracticeEven if register can now be used as a decorator, it’s more widely deployed as a function to register classes defined elsewhere. For example, in the source code for the collections.abc module, the built-in types tuple, str, range, and memoryview are registered as virtual subclasses of Sequence like this: 1234Sequence.register(tuple)Sequence.register(str)Sequence.register(range)Sequence.register(memoryview)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C11_Interfaces: From Protocols to ABCs]]></title>
    <url>%2F2018%2F05%2F25%2FC11-Interfaces-From-Protocols-to-ABCs%2F</url>
    <content type="text"><![CDATA[_vector2d_v0.py: x and y are public data attributes_ 123456789class Vector2d: typecode='d' def __init__(self,x, y): self.x=float(x) self.y=float(y) def __iter__(self): return (i for i in (self.x, self.y)) _vector2d_v3.py: x and y reimplemented as properties_1234567891011121314151617class Vector2d: typecode='d' def __init__(self,x,y): self.__x=float(x) self.__y=float(y) @property def x(self): return self.__x @property def y(self): return self.__y def __iter__(self): return (i for i in (self.x,self.y)) Python digs SequencePartial sequence protocol implementation with \_getitem__: enough for item access, iteration, and the in operator_ 12345678910111213&gt;&gt;&gt; class Foo:... def __getitem__(self, pos):... return range(0,30,10)[pos]... &gt;&gt;&gt; f=Foo()&gt;&gt;&gt; for i in f: print(i)... 01020&gt;&gt;&gt; 20 in fTrue&gt;&gt;&gt; There is no method __iter__ yet Foo instances are iterable because—as a fallback—when Python sees a __getitem__ method, it tries to iterate over the object by calling that method with integer indexes starting with 0. Because Python is smart enough to iterate over Foo instances, it can also make the in operator work even if Foo has no __contains__ method: it does a full scan to check if an item is present. In summary, given the importance of the sequence protocol, in the absence __iter__ and __contains__ Python still manages to make iteration and the in operator work by invoking __getitem__. Monkey-Patching to Implement a Protocol at Runtime random.shuffle cannot handle FrenchDeck 123456789&gt;&gt;&gt; from random import shuffle&gt;&gt;&gt; from frenchdeck import FrenchDeck&gt;&gt;&gt; deck = FrenchDeck()&gt;&gt;&gt; shuffle(deck)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File ".../python3.3/random.py", line 265, in shuffle x[i], x[j] = x[j], x[i]TypeError: 'FrenchDeck' object does not support item assignment FrenchDeck object does not support item assignment.” The problem is that shuffle operates by swapping items inside the collection, and FrenchDeck only implements the immutable sequence protocol. Mutable sequences must also provide a __setitem__ method. Monkey patching FrenchDeck to make it mutable and compatible with random.shuffle 12345678&gt;&gt;&gt; def set_card(deck, position, card):... deck._cards[position] = card...&gt;&gt;&gt; FrenchDeck.__setitem__ = set_card&gt;&gt;&gt; shuffle(deck)&gt;&gt;&gt; deck[:5][Card(rank='3', suit='hearts'), Card(rank='4', suit='diamonds'), Card(rank='4',suit='clubs'), Card(rank='7', suit='hearts'), Card(rank='9', suit='spades')] The trick is that set_card knows that the deck object has an attribute named _cards, and _cards must be a mutable sequence. The set_card function is then attached to the FrenchDeck class as the __setitem__ special method. This is an example of monkey patching: changing a class or module at runtime, without touching the source code. ABCs are meant to encapsulate very general concepts, abstractions, introduced by a framework—thingslike “a sequence” and “an exact number.” [Readers] most likely don’t need to write any new ABCs, just use existing ones correctly, to get 99.9% of the benefits without serious risk of misdesign. Subclassing an ABCFrenchDeck2 is explicitly declared a subclass of collections.MutableSequence. frenchdeck2.py:12345678910111213141516171819202122232425262728293031import collectionsCard=collections.namedtuple('Card',['rank','suit'])class FrenchDeck2(collections.MutableSequence): ranks=[str(n) for n in range(2,11)+list('JQKA')] suits='spades diamonds clubs hearts'.split() def __init__(self): self._cards=[Card(rank, suit) for suit in self.suits for rank in self.ranks ] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] def __setitem__(self, position, value): self._cards[position]=value #subclassing MutableSequence forces us to implement __delitem__, an #abstract method of that ABC. def __delitem__(self,position): del self._cards[position] #required to implement insert, the third abstract method of #MutableSequence. def insert(self, position, value): self._cards.insert(position, value) Python does not check for the implementation of the abstract methods at import time, but only at runtime when we actually try to instantiate FrenchDeck2. Then, if we fail to implement any abstract method, we get a TypeError exception. From Sequence, FrenchDeck2 inherits the following ready-to-use concrete methods: __contains__, __iter__, __reversed__, index, and count. From MutableSequence, it gets append, reverse, extend, pop, remove, and __iadd__.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C10_Sequence Hacking, Hashing, and Slicing]]></title>
    <url>%2F2018%2F05%2F25%2FC10-Sequence-Hacking-Hashing-and-Slicing%2F</url>
    <content type="text"><![CDATA[A Slice-Aware __getitem___Part of vector_v2.py: __len__ and __getitem_\ methods added to Vector class from vector_v1.py_ 1234567891011def __len__(self): return len(self._components)def __getitem__(self, index): cls = type(self) if isinstance(index, slice): return cls(self._components[index]) elif isinstance(index, numbers.Integral): return self._components[index] else: msg = '&#123;cls.__name__&#125; indices must be integers' raise TypeError(msg.format(cls=cls)) Dynamic Attribute AccessThe __getattr__ method is invoked by the interpreter when attribute lookup fails. In simple terms,given the expression my_obj.x, Python checks if the my_obj instance has an attribute namedx; if not, the search goes to the class (my_obj.__class__), and then up the inheritancegraph. If the x attribute is not found, then the __getattr__ method defined in the class ofmy_obj is called with self and the name of the attribute as a string. _Part of vector_v3.py: __getattr__method added to Vector class from vector_v2.py_12345678910shortcut_names = 'xyzt'def __getattr__(self, name): cls = type(self)if len(name) == 1: pos = cls.shortcut_names.find(name)if 0 &lt;= pos &lt; len(self._components): return self._components[pos]msg = '&#123;.__name__!r&#125; object has no attribute &#123;!r&#125;'raise AttributeError(msg.format(cls, name)) _Part of vector_v3.py: __setattr__ method in Vector class_ 12345678910111213def __setattr__(self, name, value): cls = type(self) if len(name) == 1: if name in cls.shortcut_names: error = 'readonly attribute &#123;attr_name!r&#125;' elif name.islower(): error = "can't set attributes 'a' to 'z' in &#123;cls_name!r&#125;" else: error = '' if error: msg = error.format(cls_name=cls.__name__, attr_name=name) raise AttributeError(msg) super().__setattr__(name, value) Hashing and a Faster ==_Part of vector_v4.py: two imports and __hash__ method added to Vector class from vector_v3.py_ 12345678910111213141516171819202122from array import arrayimport reprlibimport mathimport functools #import operator class Vector: typecode = 'd' # many lines omitted in book listing... def __eq__(self, other): # if len(self) != len(other): # return False for a, b in zip(self, other): # if a != b: # return False return True # or you can just write as: return len(self) == len(other) and all(a == b for a, b in zip(self, other)) def __hash__(self): hashes = (hash(x) for x in self._components) # return functools.reduce(operator.xor, hashes, 0) # Formatting12345678910111213141516171819def angle(self, n): r = math.sqrt(sum(x * x for x in self[n:])) a = math.atan2(r, self[n-1]) if (n == len(self) - 1) and (self[-1] &lt; 0): return math.pi * 2 - a else: return a def angles(self): return (self.angle(n) for n in range(1, len(self))) def __format__(self, fmt_spec=''): if fmt_spec.endswith('h'): # hyperspherical coordinates fmt_spec = fmt_spec[:-1] coords = itertools.chain([abs(self)], self.angles()) outer_fmt = '&lt;&#123;&#125;&gt;' else: coords = self outer_fmt = '(&#123;&#125;)' components = (format(c, fmt_spec) for c in coords) return outer_fmt.format(', '.join(components))]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C09_A Pythonic Object]]></title>
    <url>%2F2018%2F05%2F25%2FC09-A-Pythonic-Object%2F</url>
    <content type="text"><![CDATA[Object RepresentationsEvery object-oriented language has at least one standard way of getting a string representation fromany object. Python has two:repr() Return a string representing the object as the developer wants to see it. str() Return a string representing the object as the user wants to see it. There are two additional special methods to support alternative representations of objects:__bytes__ and __format__. The __bytes__ method is analogous to __str_\: it’scalled by bytes() to get the object represented as a byte sequence. Regarding **\_format__,both the built-in function format() and the str.format()** method call it to get string displays ofobjects using special formatting codes. _vector2d_v1.py_ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from array import arrayimport mathclass Vector2d: typecode='d' def __init__(self, x, y): self.x = float(x) self.y = float(y) #__iter__ makes a Vector2d iterable; this is what makes #unpacking work (e.g,x, y = my_vector). def __iter__(self): return (i for i in (self.x, self.y)) #because Vector2d is iterable, *self feeds the x and y #components to format. def __repr__(self): class_name= type(self).__name__ return '&#123;&#125;(&#123;!r&#125;, &#123;!r&#125;)'.format(class_name, *self) #From an iterable Vector2d, it’s easy to build a tuple #for display as an ordered pair. def __str__(self): return str(tuple(self)) def __bytes__(self): return (bytes([ord(self.typecode)])+ bytes(array(self.typecode, self)) ) def __eq__(self, other): return tuple(self) == tuple(other) def __abs__(self): return math.hypot(self.x+ self.y) def __bool__(self): return bool(abs(self)) #No self argument; instead, the class itself is passed as #cls. @classmethod def frombytes(cls, octets): typecode=chr(octets[0]) #Read the typecode from the first byte. #Create a memoryview from the octets binary sequence and use the #typecode to cast it memv=memoryview(octets[1:].cast(typecode)) #Unpack the memoryview resulting from the cast into the pair of #arguments needed for the constructor. return cls(*memv) classmethod Versus staticmethodclassmethod changes the way the method is called, so it receives the class itself as the firstargument, instead of an instance. Its most common use is for alternative constructors, like frombytes in_vector2d_v1.py_. staticmethod decorator changes a method so that it receives no special first argument. In essence, astatic method is just like a plain function that happens to live in a class body, instead of beingdefined at the module level. Formatted DisplaysThe format() built-in function and the str.format() method delegate the actual formatting to eachtype by calling their .__format__(format_spec) method. The format_spec is a formattingspecifier, which is either the second argument in format(my_obj, format_spec), or Whatever appearsafter the colon in a replacement field delimited with {} inside a format string used with str.format(). A few built-in types have their own presentation codes in the Format Specification MiniLanguage. Forexample—among several other codes—the int type supports b and x for base 2 and base 16 output,respectively, while float implements f for a fixed-point display and % for a percentage display: 12345&gt;&gt;&gt; format(42, 'b')'101010'&gt;&gt;&gt; format(2/3, '.1%')'66.7%' The classes in the datetime module use the same format codes in the strftime() functions and in their__format__ methods. Here are a couple examples using the format() built-in and thestr.format() method: 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; format(now, '%H:%M:%S')'18:49:05'&gt;&gt;&gt; "It's now &#123;:%I:%M %p&#125;".format(now)"It's now 06:49 PM" If a class has no __format__, the method inherited from object returns str(my_object). 123456789&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; format(v1)&apos;(3.0, 4.0)&apos;# if you pass a format specifier, object.__format__ raises TypeError:&gt;&gt;&gt; format(v1, &apos;.3f&apos;)Traceback (most recent call last): ...TypeError: non-empty format string passed to object.__format Vector2d.format method, now with polar coordinates1234567891011121314def angle(self): return math.atan2(self.y, self.x)def __format__(self, fmt_spec=''): if fmt_spec.endswith('p'): fmt_spec=fmt_spec[:-1] coords=(abs(self), self.angle()) outer_fmt='&lt;&#123;&#125; &#123;&#125;&gt;' else: coords=self outer_fmt='(&#123;&#125;, &#123;&#125;)' components=(format(c, fmt_spec) for c in self) return outer_fmt.format(*components) A Hashable Vector2dVector2d instances are unhashable, so we can’t put them in a set 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; hash(v1)Traceback (most recent call last): ...TypeError: unhashable type: 'Vector2d'&gt;&gt;&gt; set([v1])Traceback (most recent call last): ...TypeError: unhashable type: 'Vector2d'class Vector2d: typecode='d' #Use exactly two leading underscores to make an attribute #private def __init__(self,x,y): self.__x=float(x) self.__y=float(y) #The @property decorator marks the getter method of a #property. @property def x(self): return self.__x @property def y(self): return self.__y def __iter__(self): return (i for i in (self.x, self.y)) def __hash__(self): return hash(self.x) ^ hash(self.y)&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; v2 = Vector2d(3.1, 4.2)&gt;&gt;&gt; hash(v1), hash(v2)(7, 384307168202284039)&gt;&gt;&gt; set([v1, v2])&#123;Vector2d(3.1, 4.2), Vector2d(3.0, 4.0)&#125; Private and “Protected” Attributes in PythonConsider this scenario: someone wrote a class named Dog that uses a mood instance attribute internally, without exposing it. You need to subclass Dog as Beagle. If you create your own mood instance attribute without being aware of the name clash, you will clobber the mood attribute used by the methods inherited from Dog. This would be a pain to debug. To prevent this, if you name an instance attribute in the form __mood (two leading underscores andzero or at most one trailing underscore), Python stores the name in the instance __dict__ prefixedwith a leading underscore and the class name, so in the Dog class,__mood becomes_Dog__mood, and in Beagle it’s _Beagle__mood. This language feature goes by thelovely name of name mangling. The single underscore prefix has no special meaning to the Python interpreter when used in attributenames, but it’s a very strong convention among Python programmers that you should not access suchattributes from outside the class. Saving Space with the slots Class AttributeBy default, Python stores instance attributes in a per-instance dict named __dict__.If youare dealing with millions of instances with few attributes, the __slots__ class attribute can save alot of memory, by letting the interpreter store the instance attributes in a tuple instead of a dict. A __slots__ attribute inherited from a superclass has no effect. Python only takes into account__slots__ attributes defined in each class individually. By defining __slots__ in the class, you are telling the interpreter: “These are all the instanceattributes in this class.” Python then stores them in a tuple-like structure in each instance, avoiding thememory overhead of the per-instance_\dict__. This can make a huge difference in memory usage ifyour have millions of instances active at the same time. The __weakref__ attribute is necessary for an object to support weak references. That attribute ispresent by default in instances of user-defined classes. However, if the class defines __slots__, andyou need the instances to be targets of weak references, then you need to include __weakref__among the attributes named in __slots__. To summarize, __slots__ may provide significant memory savings if properly used, but there are afew caveats: You must remember to redeclare __slots__ in each subclass, because the inherited attribute is ignored by the interpreter. Instances will only be able to have the attributes listed in __slots__, unless you include ‘_\dict__‘ in __slots__ (but doing so may negate the memory savings). Instances cannot be targets of weak references unless you remember to include ‘__weakref__‘ in __slots__. Overriding Class AttributesA distinctive feature of Python is how class attributes can be used as default values for instanceattributes. 1234567891011121314151617&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; v1 = Vector2d(1.1, 2.2) #Set typecode to 'f' in the v1 instance.&gt;&gt;&gt; dumpd = bytes(v1)&gt;&gt;&gt; dumpdb'd\x9a\x99\x99\x99\x99\x99\xf1?\x9a\x99\x99\x99\x99\x99\x01@'&gt;&gt;&gt; len(dumpd) #17&gt;&gt;&gt; v1.typecode = 'f' #&gt;&gt;&gt; dumpf = bytes(v1)&gt;&gt;&gt; dumpfb'f\xcd\xcc\x8c?\xcd\xcc\x0c@'&gt;&gt;&gt; len(dumpf) #9# Vector2d.typecode is unchanged; only the v1 instance uses typecode 'f'.&gt;&gt;&gt; Vector2d.typecode #'d' If you want to change a class attribute you must set it on the class directly, not through aninstance. You could change the default typecode for all instances (that don’t have their owntypecode) by doing this: Vector2d.typecode = ‘f’ The ShortVector2d is a subclass of Vector2d, which only overwrites the default typecode123456789&gt;&gt;&gt; from vector2d_v3 import Vector2d&gt;&gt;&gt; class ShortVector2d(Vector2d): #... typecode = 'f'...&gt;&gt;&gt; sv = ShortVector2d(1/11, 1/27) #&gt;&gt;&gt; svShortVector2d(0.09090909090909091, 0.037037037037037035) #&gt;&gt;&gt; len(bytes(sv)) #9]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C08_Object References, Mutability, and Recycling_02]]></title>
    <url>%2F2018%2F05%2F25%2FC08-Object-References-Mutability-and-Recycling-02%2F</url>
    <content type="text"><![CDATA[Function Parameters as ReferencesA function may change any mutable object it receives1234567891011121314151617181920212223&gt;&gt;&gt; def f(a, b):... a += b... return a...&gt;&gt;&gt; x = 1&gt;&gt;&gt; y = 2&gt;&gt;&gt; f(x, y)3&gt;&gt;&gt; x, y(1, 2)&gt;&gt;&gt; a = [1, 2]&gt;&gt;&gt; b = [3, 4]&gt;&gt;&gt; f(a, b)[1, 2, 3, 4]&gt;&gt;&gt; a, b([1, 2, 3, 4], [3, 4])&gt;&gt;&gt; t = (10, 20)&gt;&gt;&gt; u = (30, 40)&gt;&gt;&gt; f(t, u)(10, 20, 30, 40)&gt;&gt;&gt; t, u((10, 20), (30, 40)) Mutable Types as Parameter Defaults: Bad IdeaIf a default value is a mutable object, and you change it, the change will affect every future call of thefunction.12345678910111213141516171819class HauntedBus: def __init__(self, passengers=[]):#This assignment makes self.passengers an aliasfor passengers, which isitself an alias for the default list, #when no passengers argument is given. if passengers is None: self.passengers=passengers else: self.passengers = list(passengers)# When the methods .remove() and .append() are used with self.passengers we are actually mutating the #default list, which is an attribute of the function object def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) eerie behavior of the HauntedBus12345678910111213141516171819202122232425262728&gt;&gt;&gt; bus1 = HauntedBus(['Alice', 'Bill'])&gt;&gt;&gt; bus1.passengers['Alice', 'Bill']&gt;&gt;&gt; bus1.pick('Charlie')&gt;&gt;&gt; bus1.drop('Alice')&gt;&gt;&gt; bus1.passengers['Bill', 'Charlie']&gt;&gt;&gt; bus2 = HauntedBus()&gt;&gt;&gt; bus2.pick('Carrie')&gt;&gt;&gt; bus2.passengers['Carrie']&gt;&gt;&gt; bus3 = HauntedBus()&gt;&gt;&gt; bus3.passengers['Carrie']&gt;&gt;&gt; bus3.pick('Dave')&gt;&gt;&gt; bus2.passengers['Carrie', 'Dave']&gt;&gt;&gt; bus2.passengers is bus3.passengers # bus2.passengers and bus3.passengers refer to the same listTrue&gt;&gt;&gt; bus1.passengers # bus1.passengers is a distinct list['Bill', 'Charlie']&gt;&gt;&gt; dir(HauntedBus.__init__) # doctest: +ELLIPSIS['__annotations__', '__call__', ..., '__defaults__', ...]&gt;&gt;&gt; HauntedBus.__init__.__defaults__(['Carrie', 'Dave'],)&gt;&gt;&gt; HauntedBus.__init__.__defaults__[0] is bus2.passengersTrue The issue with mutable defaults explains why None is often used as the default value for parameters that may receive mutable values. Defensive Programming with Mutable ParametersIf your function receives a dict and needs to modify it while processing it, should this side effect bevisible outside of the function or not? Actually it depends on the context. It’s really a matter of aligningthe expectation of the coder of the function and that of the caller. A simple class to show the perils of mutating received arguments 123456789101112131415161718192021222324252627282930313233343536class TwilightBus: def __init__(self, passengers=None): if passengers is None: self.passengers=[] else: #this assignment makes self.passengers an alias for # passengers, which is itself an alias for the # actual argument passed to __init__ self.passengers=passengers #When the methods .remove() and .append() are used with #self.passengers, we are actually mutating the original #list received as argument to the constructor. def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name)# Passengers disappear when dropped by a TwilightBus&gt;&gt;&gt; basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']&gt;&gt;&gt; bus = TwilightBus(basketball_team)&gt;&gt;&gt; bus.drop('Tina')&gt;&gt;&gt; bus.drop('Pat')&gt;&gt;&gt; basketball_team #The dropped passengers vanished from the basketball team!['Sue', 'Maya', 'Diana']# The fix is simple: in __init__, when the passengers parameter is provided, self.passengers should be # initialized with a copy of itdef __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) del and Garbage CollectionThe del statement deletes names, not objects.An object may be garbage collected as result of adel command, but only if the variable deleted holds the last reference to the object, or if the objectbecomes unreachable. Rebinding a variable may also cause the number of references to an object toreach zero, causing its destruction. There is a __del__ special method, but it does not cause the disposal of the instance, and shouldnot be called by your code. __del__ invoked by the Python interpreter when the instance is aboutto be destroyed to give it a chance to release external resources. In CPython, the primary algorithm for garbage collection is reference counting. Essentially, each objectkeeps count of how many references point to it. As soon as that refcount reaches zero, the object isimmediately destroyed: CPython calls the __del__ method on the object (if defined) and thenfrees the memory allocated to the object. In CPython 2.0, a generational garbage collection algorithmwas added to detect groups of objects involved in reference cycles—which may be unreachable evenwith outstanding references to them, when all the mutual references are contained within the group.Other implementations of Python have more sophisticated garbage collectors that do not rely onreference counting, which means the __del__ method may not be called immediately when thereare no more references to the object. Using weakref.finalize to register a callback function to be called when an object is destroyed Weak ReferencesWeak references to an object do not increase its reference count. The object that is the target of areference is called the referent. Therefore, we say that a weak reference does not prevent thereferent from being garbage collected. A weak reference is a callable that returns the referenced object or None if the referent is no more 1234567891011121314151617181920212223242526&gt;&gt;&gt; import weakref&gt;&gt;&gt; a_set = &#123;0, 1&#125;&gt;&gt;&gt; wref = weakref.ref(a_set)&gt;&gt;&gt; wref&lt;weakref at 0x100637598; to 'set' at 0x100636748&gt;#Invoking wref() returns the referenced object, &#123;0, 1&#125;. Because this is a console session, the result &#123;0, 1&#125; is # bound to the _ variable.&gt;&gt;&gt; wref()&#123;0, 1&#125;# a_set no longer refers to the &#123;0, 1&#125; set, so its reference count is decreased. But the _ variable still refers # to it.&gt;&gt;&gt; a_set = &#123;2, 3, 4&#125;&gt;&gt;&gt; wref()&#123;0, 1&#125;# When this expression is evaluated, &#123;0, 1&#125; lives, therefore wref() is not None. But _ is then bound to the # resulting value, False. Now there are no more strong references to &#123;0, 1&#125;.&gt;&gt;&gt; wref() is NoneFalse# Because the &#123;0, 1&#125; object is now gone, this last call to wref() returns None.&gt;&gt;&gt; wref() is NoneTrue The WeakValueDictionary SkitThe class WeakValueDictionary implements a mutable mapping where the values are weak referencesto objects. When a referred object is garbage collected elsewhere in the program, the corresponding keyis automatically removed from WeakValueDictionary. This is commonly used for caching. 123456789101112131415161718192021222324class Cheese: def __init__(self, kind): self.kind = kind def __repr__(self): return 'Cheese(%r)' % self.kind&gt;&gt;&gt; import weakref&gt;&gt;&gt; stock = weakref.WeakValueDictionary()&gt;&gt;&gt; catalog = [Cheese('Red Leicester'), Cheese('Tilsit'),... Cheese('Brie'), Cheese('Parmesan')]...# The stock mapsthe name of the cheese to aweak reference to the cheese instance in the catalog.&gt;&gt;&gt; for cheese in catalog: ... stock[cheese.kind] = cheese...&gt;&gt;&gt; sorted(stock.keys())['Brie', 'Parmesan', 'Red Leicester', 'Tilsit']&gt;&gt;&gt; del catalog&gt;&gt;&gt; sorted(stock.keys())['Parmesan']&gt;&gt;&gt; del cheese&gt;&gt;&gt; sorted(stock.keys())[]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[春天的一些照片]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%98%A5%E5%A4%A9%E7%9A%84%E4%B8%80%E4%BA%9B%E7%85%A7%E7%89%87%2F</url>
    <content type="text"><![CDATA[照片非出自本人，双休日这么好的日子，宅着，看看nba, 看看番。 2018年3月，同事去某个植物园，可能是上海植物园，陶堰情操，拍拍花草，感觉比以前拍的好看点。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>photo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态博客太好用！！！]]></title>
    <url>%2F2018%2F04%2F22%2F%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E5%A4%AA%E5%A5%BD%E7%94%A8%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
    <content type="text"><![CDATA[用hexo搭的这个博客， 太好搭了，就用了node.js和git, 不需要vps, 省钱。之前flask搭的那个博客，要先看完书，然后找了一个别人写好的框架，一步一步网上谷歌，废了九牛二虎之力才弄好， 弄完后眼睛那个酸。这个flask博客还是蛮好看的，羡慕别人怎么就可以写的出这么好的框架。 hexo博客优点：样式简洁美观上传文章超级方便 hexo n, hexo g, hexo s, hexo d 不过发现图片总是404错误，于是在vps上搭了个图床，第一次发现宝塔这个工具，真是强大方便。。。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>吐槽</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C08_Object References, Mutability, and Recycling_01]]></title>
    <url>%2F2018%2F04%2F22%2FC08-Object-References-Mutability-and-Recycling-01%2F</url>
    <content type="text"><![CDATA[The distinction between objects and their names. A name is not the object; a name is a separate thing. Variables Are Not BoxesIt’s better to think of Python variables as labels attached to objects Variables a and b hold references to the same list, not copies of the list12345&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; b[1, 2, 3, 4] With reference variables, it makes much more sense to say that the variable is assigned to an object,and not the other way around. After all, the object is created before the assignment. The righthand side of an assignment happens first(Variables are assigned to objects only after the objects are created) 1234567891011121314&gt;&gt;&gt; class Gizmo:... def __init__(self):... print('Gizmo id: %d' % id(self))...&gt;&gt;&gt; x = Gizmo()Gizmo id: 4301489152# a second Gizmo was actually instantiated before the multiplication was attempted.&gt;&gt;&gt; y = Gizmo() * 10 Gizmo id: 4301489432Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int' Identity, Equality, and Aliasescharles and lewis refer to the same object123456789&gt;&gt;&gt; charles = &#123;'name': 'Charles L. Dodgson', 'born': 1832&#125;&gt;&gt;&gt; lewis = charles # lewis is an alias for charles.&gt;&gt;&gt; lewis is charlesTrue&gt;&gt;&gt; id(charles), id(lewis) (4300473992, 4300473992)&gt;&gt;&gt; lewis['balance'] = 950&gt;&gt;&gt; charles&#123;'name': 'Charles L. Dodgson', 'balance': 950, 'born': 1832&#125; alex and charles compare equal, but alex is not charles #alex refers to an object that is a replica of the object assigned to charles.12345&gt;&gt;&gt; alex = &#123;'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950&#125;&gt;&gt;&gt; alex == charlesTrue&gt;&gt;&gt; alex is not charlesTrue In The Python Language Reference, “3.1. Objects, values and types” states: Every object has an identity, a type and a value. An object’s identity never changes onceit has been created; you may think of it as the object’s address in memory. The is operatorcompares the identity of two objects; the id() function returns an integer representingits identity. In CPython, id() returns the memory address of the object, but it may be something else in anotherPython interpreter. The key point is that the ID is guaranteed to be a unique numeric label, and it willnever change during the life of the object. Choosing Between == and isThe == operator compares the values of objects (the data they hold), while is compares theiridentities. checking whether a variable is bound to Nonex is None The is operator is faster than ==, because it cannot be overloaded, so Python does not have tofind and invoke special methods to evaluate it, and computing is as simple as comparing two integer IDs.In contrast, a == b is syntactic sugar for a.__eq__(b). The __eq__ method inherited fromobject compares object IDs, so it produces the same result as is. The Relative Immutability of TuplesTuples, like most Python collections—lists, dicts, sets, etc.—hold references toobjects.If the referenced items are mutable, they may change even if the tuple itself does not. Inother words, the immutability of tuples really refers to the physical contents of the tuple datastructure (i.e., the references it holds), and does not extend to the referenced objects.1234567891011121314&gt;&gt;&gt; t1 = (1, 2, [30, 40])&gt;&gt;&gt; t2 = (1, 2, [30, 40])&gt;&gt;&gt; t1 == t2True&gt;&gt;&gt; id(t1[-1])4302515784&gt;&gt;&gt; t1[-1].append(99)&gt;&gt;&gt; t1(1, 2, [30, 40, 99])&gt;&gt;&gt; id(t1[-1]) #The identity of t1[-1] has not changed, only its value.4302515784&gt;&gt;&gt; t1 == t2False Copies Are Shallow by Default12345678&gt;&gt;&gt; l1 = [3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 = list(l1) #list(l1) creates a copy of l1.&gt;&gt;&gt; l2[3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 == l1True&gt;&gt;&gt; l2 is l1 #The copies are equal. But refer to two different objectsFalse For lists and other mutable sequences, the shortcut l2 = l1[:] also makes a copy. Using the constructor or [:] produces a shallow copy (i.e., the outermost container is duplicated,but the copy is filled with references to the same items held by the original container). This savesmemory and causes no problems if all the items are immutable. But if there are mutable items, this maylead to unpleasant surprises. Making a shallow copy of a list containing another list 123456789101112131415161718192021l1 = [3, [66, 55, 44], (7, 8, 9)]l2 = list(l1) #l1.append(100) #l1[1].remove(55) #print('l1:', l1)print('l2:', l2)l2[1] += [33, 22] #l2[2] += (10, 11) #print('l1:', l1)print('l2:', l2)#ouput:l1: [3, [66, 44], (7, 8, 9), 100]l2: [3, [66, 44], (7, 8, 9)]#For a mutable object like the list referred by l2[1], the operator += changes the list in place. This change #is visible at l1[1], which is an alias for l2[1].l1: [3, [66, 44, 33, 22], (7, 8, 9), 100]# += on a tuple creates a new tuple and rebinds the variable l2[2] here. This is the same as doing l2[2] = # l2[2] + (10, 11). Now the tuples in the last position of l1 and l2 are no longer the same object. l2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)] Deep and Shallow Copies of Arbitrary ObjectsThe copy module provides the deepcopy and copy functions that return deep and shallowcopies of arbitrary objects.1234567891011121314class Bus: def __init__(self, passengers=None): if passengers is None: self.passengers=[] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) Effects of using copy versus deepcopy1234567891011121314151617&gt;&gt;&gt; import copy&gt;&gt;&gt; bus1 = Bus(['Alice', 'Bill', 'Claire', 'David'])&gt;&gt;&gt; bus2 = copy.copy(bus1)&gt;&gt;&gt; bus3 = copy.deepcopy(bus1)#Using copy and deepcopy, we create three distinct Bus instances.&gt;&gt;&gt; id(bus1), id(bus2), id(bus3)(4301498296, 4301499416, 4301499752)&gt;&gt;&gt; bus1.drop('Bill')&gt;&gt;&gt; bus2.passengers['Alice', 'Claire', 'David']&gt;&gt;&gt; id(bus1.passengers), id(bus2.passengers), id(bus3.passengers)(4302658568, 4302658568, 4302657800)&gt;&gt;&gt; bus3.passengers['Alice', 'Bill', 'Claire', 'David'] Cyclic references: b refers to a, and then is appended to a; deepcopy still manages to copy a 123456789&gt;&gt;&gt; a=[10,20]&gt;&gt;&gt; b=[a,30]&gt;&gt;&gt; a.append(b)&gt;&gt;&gt; a[10, 20, [[...], 30]]&gt;&gt;&gt; from copy import deepcopy&gt;&gt;&gt; c=deepcopy(a)&gt;&gt;&gt; c[10, 20, [[...], 30]]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Stacked Decorators_Parameterized Decorators]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Stacked-Decorators-Parameterized-Decorators%2F</url>
    <content type="text"><![CDATA[Stacked DecoratorsWhen two decorators @d1 and @d2 are applied to a function f in that order, the result isthe same as f = d1(d2(f)). Parameterized DecoratorsWhen parsing a decorator in source code, Python takes the decorated function and passes it as the firstargument to the decorator function. So how do you make a decorator accept other arguments? Theanswer is: make a decorator factory that takes those arguments and returns a decorator, which is thenapplied to the function to be decorated._registration_param.py_12345678910111213141516171819202122registry=set()def register(active=True): def decorate(func): print('running register (active=%s) -&gt; decorate(%s)')%(active, func) if active: registry.add(func) else: registry.discard(func) return func return decorate@register(active=False)def f1(): print('running f1()')@register(active=True)def f2(): print('running f2()')def f3(): print('running f3()') _Using the registration_param module_123456789101112131415&gt;&gt;&gt; from registration_param import *running register(active=False)-&gt;decorate(&lt;function f1 at 0x10073c1e0&gt;)running register(active=True)-&gt;decorate(&lt;function f2 at 0x10073c268&gt;)&gt;&gt;&gt; registry #&#123;&lt;function f2 at 0x10073c268&gt;&#125;&gt;&gt;&gt; register()(f3) #running register(active=True)-&gt;decorate(&lt;function f3 at 0x10073c158&gt;)&lt;function f3 at 0x10073c158&gt;&gt;&gt;&gt; registry #&#123;&lt;function f3 at 0x10073c158&gt;, &lt;function f2 at 0x10073c268&gt;&#125;&gt;&gt;&gt; register(active=False)(f2) #running register(active=False)-&gt;decorate(&lt;function f2 at 0x10073c268&gt;)&lt;function f2 at 0x10073c268&gt;&gt;&gt;&gt; registry #&#123;&lt;function f3 at 0x10073c158&gt;&#125; _clockdeco_param.py_12345678910111213141516171819202122232425262728293031323334import timeDEFAULT_FMT='[&#123;elasped: 0.8f&#125;s &#123;name&#125;(&#123;args&#125;) -&gt; &#123;result&#125;]'def clock(fmt=DEFAULT_FMT): def decorate(func): def clocked(*_args): t0=time.time() _result=func(*_args) elapsed=time.time()- t0 name=func.__name__ #_args holds the actual arguments of clocked args=', '.join(repr(arg) for arg in _args) result=repr(_result) #Using **locals() here allows any local variable of #clocked to be referenced in the fmt. print(fmt.format(**locals())) return _result return clocked return decorateif __name__=='__main__': @clock() def snooze(seconds): time.sleep(seconds) for i in range(3): snooze(.123)# output:$ python3 clockdeco_param.py[0.12412500s] snooze(0.123) -&gt; None[0.12411904s] snooze(0.123) -&gt; None[0.12410498s] snooze(0.123) -&gt; None _clockdeco_param_demo1.py_ 12345678910111213import timefrom clockdeco_param import clock@clock('&#123;name&#125;: &#123;elapsed&#125;s')def snooze(seconds): time.sleep(seconds)for i in range(3): snooze(.123)# output:$ python3 clockdeco_param_demo1.pysnooze: 0.12414693832397461ssnooze: 0.1241159439086914ssnooze: 0.12412118911743164s _clockdeco_param_demo2.py_12345678910111213import timefrom clockdeco_param import clock@clock('&#123;name&#125;(&#123;args&#125;) dt=&#123;elapsed:0.3f&#125;s')def snooze(seconds): time.sleep(seconds)for i in range(3): snooze(.123)# output:$ python3 clockdeco_param_demo2.pysnooze(0.123) dt=0.124ssnooze(0.123) dt=0.124ssnooze(0.123) dt=0.124s]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Implementing a Simple Decorator]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Implementing-a-Simple-Decorator%2F</url>
    <content type="text"><![CDATA[clockdeco.py: 12345678910import timedef clock(func): def clocked(*args): t0=time.perf_counter() result=func(*args) elapsed=time.perf_counter()- t0 name=func.__name__ arg_str=', '.join(repr(arg) for arg in args) print('[%0.8fs] %s(%s) -&gt; %r'%(elapsed, name, arg_str, result)) return clocked _clockdeco_demo.py:_ 123456789101112131415161718192021import timefrom clockdeco import clock@clockdef snooze(seconds): time.sleep(seconds)@clockdef factorial(n): return 1 if n ==1 else n * factorial(n-1)if __name__ == '__main__': print('*'*40, 'Calling snooze(.123)') snooze(.123) print('*'*40, 'Calling factorial(6)') print('6!= ', factorial(6))&gt;&gt;&gt; import clockdeco_demo&gt;&gt;&gt; clockdeco_demo.factorial.__name__'clocked' So factorial now actually holds a reference to the clocked function. The typical behavior of a decorator: it replaces the decorated function with a new function that accepts the same arguments and (usually) returns whatever the decorated function was supposed to return, while also doing some extra processing. functools.wrapsThe clock decorator has a few shortcomings: it does not support keyword arguments, and it masksthe __name__ and __doc__ of the decorated function. clockdeco2.py1234567891011121314151617181920import timeimport functoolsdef clock(func): @functools.wraps(func) def clocked(*args, **kwargs): t0=time.time() result=func(*args, **kwargs) elapsed=time.time() - t0 name=func.__name__ arg_lst=[] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs=['%s = %s' %(k,w) for k,w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str=', '.join(arg_lst) print('[%0.8fs] %s(%s) -&gt; %r'%(elapsed, name, arg_str, result)) return result return clocked functools.lru_cacheA very practical decorator is functools.lru_cache. It implements memoization: an optimizationtechnique that works by saving the results of previous invocations of an expensive function, avoidingrepeat computations on previously used arguments. The letters LRU stand for Least Recently Used,meaning that the growth of the cache is limited by discarding the entries that have not been read for awhile. _fibo_demo.py_1234567891011121314151617181920212223242526272829303132333435from clockdeco import clock@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n-2) + fibonacci(n-1)if __name__=='__main__': print(fibonacci(6))# $ python3 fibo_demo.py# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00007892s] fibonacci(2) -&gt; 1# [0.00000095s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00003815s] fibonacci(2) -&gt; 1# [0.00007391s] fibonacci(3) -&gt; 2# [0.00018883s] fibonacci(4) -&gt; 3# [0.00000000s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000119s] fibonacci(1) -&gt; 1# [0.00004911s] fibonacci(2) -&gt; 1# [0.00009704s] fibonacci(3) -&gt; 2# [0.00000000s] fibonacci(0) -&gt; 0# [0.00000000s] fibonacci(1) -&gt; 1# [0.00002694s] fibonacci(2) -&gt; 1# [0.00000095s] fibonacci(1) -&gt; 1# [0.00000095s] fibonacci(0) -&gt; 0# [0.00000095s] fibonacci(1) -&gt; 1# [0.00005102s] fibonacci(2) -&gt; 1# [0.00008917s] fibonacci(3) -&gt; 2# [0.00015593s] fibonacci(4) -&gt; 3# [0.00029993s] fibonacci(5) -&gt; 5# [0.00052810s] fibonacci(6) -&gt; 8# 8 _fibo_demo_lru.py_12345678910111213141516import functools from clockdeco import clock@functools.lru_cache()@clockdef fibnacci(n): if n &lt; 2: return n return fibnacci(n-2)+ fibnacci(n-1)$ python3 fibo_demo_lru.py# [0.00000119s] fibonacci(0) -&gt; 0# [0.00000119s] fibonacci(1) -&gt; 1# [0.00010800s] fibonacci(2) -&gt; 1# [0.00000787s] fibonacci(3) -&gt; 2# [0.00016093s] fibonacci(4) -&gt; 3# [0.00001216s] fibonacci(5) -&gt; 5# [0.00025296s] fibonacci(6) -&gt; 8 lru_cache can be tuned by passing two optional arguments. Its full signature is: functools.lru_cache(maxsize=128, typed=False) The maxsize argument determines how many call results are stored. After the cache is full, older results are discarded to make room. For optimal performance, maxsize should be a power of 2. The typed argument, if set to True, stores results of different argument types separately, i.e., distinguishing between float and integer arguments that are normally considered equal, like 1 and 1.0. By the way, because lru_cache uses a dict to store the results, and the keys are made from the positional and keyword arguments used in the calls, all the arguments taken by the decorated function must be hashable. functools.singledispatchIf you decorate a plain function with @singledispatch, it becomes a generic function: a group offunctions to perform the same operation in different ways, depending on the type of the first argument. 123456789101112131415161718192021222324252627from functools import singledispatchfrom collections import abcimport numbersimport html@singledispatchdef htmlize(obj): content=html.escape(repr(obj)) return '&lt;pre&gt;&#123;&#125;&lt;/pre&gt;'.format(content)@htmlize.register(str)def _(text): content=html.escape(text).replace('\n','&lt;br&gt;\n') return '&lt;p&gt;&#123;0&#125;&lt;/p&gt;'.format(content)#numbers.Integral is a virtual superclass of int.@htmlize.register(numbers.Integral)def _(n): return '&lt;pre&gt;&#123;0&#125; (0x&#123;0:x&#125;)&lt;/pre&gt;'.format(n) #stack several register decorators to support different types #with the same function@htmlize.register(tuple)@htmlize.register(abc.MutableSequence)def _(seq): inner='&lt;/li&gt;\n&lt;li&gt;'.join(htmlize(item) for item in seq) return '&lt;ul&gt;\n&lt;li&gt;'+inner+'&lt;/li&gt;\n&lt;/ul&gt;']]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Function Decorators and Closures_02]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Function-Decorators-and-Closures-02%2F</url>
    <content type="text"><![CDATA[ClosuresA closure is a function with an extended scope that encompasses nonglobal variables referencedin the body of the function but not defined there. It does not matter whether the function is anonymousor not; what matters is that it can access nonglobal variables that are defined outside of its body.A class to calculate a running average12345678910class Averager(): def __init__(self): self.series=[] def __call__(self, new_value): self.series.append(new_value) total=sum(self.series) return total / len(self.series) A higher-order function to calculate a running average12345678910def make_averager(): series=[] def averager(new_value): series.append(new_value) total=sum(series) return total / len(series) return averager When invoked, make_averager returns an averager function object. Each time anaverager is called, it appends the passed argument to the series, and computes the currentaverage, as shown: 1234567&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 _inspecting the function created by make_average_ 1234&gt;&gt;&gt; avg.__code__.co_varnames('new_value', 'total')&gt;&gt;&gt; avg.__code__.co_freevars('series',) The binding for series is kept in the __closure__ attribute of the returned function avg. Each item inavg.__closure__corresponds to a name in avg.__closure__**.co_free vars. These items are cells, and they have an attribute called cell_contents** where the actual value can be found. A closure is a function that retains the bindings of the free variables that exist when the function isdefined, so that they can be used later when the function is invoked and the defining scope is no longeravailable. nonlocal Declaration123456789101112131415161718def make_averager(): count=0 total=0 def averager(new_value): count += 1 total += new_value return total / count return averager&gt;&gt;&gt; avg= make_averager()&gt;&gt;&gt; avg(10)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 6, in averagerUnboundLocalError: local variable 'count' referenced before assignment The problem is that the statement count += 1 actually means the same as count = count + 1,when count is a number or any immutable type. So we are actually assigning to count in the bodyof averager, and that makes it a local variable. The same problem affects the total variable. We did not have this problem in series because we never assigned to the series name; we onlycalled series.append and invoked sum and len on it. So we took advantage of the fact that lists are mutable. But with immutable types like numbers, strings, tuples, etc., all you can do is read, but never update. Ifyou try to rebind them, as in count = count + 1, then you are implicitly creating a local variablecount. It is no longer a free variable, and therefore it is not saved in the closure. To work around this, the nonlocal declaration was introduced in Python 3. It lets you flag a variable asa free variable even when it is assigned a new value within the function. If a new value is assigned to anonlocal variable, the binding stored in the closure is changed. Calculate a running average without keeping all history (fixed with the use of nonlocal) 123456789def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_Function Decorators and Closures01]]></title>
    <url>%2F2018%2F04%2F22%2FC07-Function-Decorators-and-Closures01%2F</url>
    <content type="text"><![CDATA[decoratorA decorator is a callable that takes another function as argument (the decorated function). Thedecorator may perform some processing with the decorated function, and returns it or replaces it withanother function or callable object. 1234567891011&gt;&gt;&gt; def deco(func):... def inner():... print('running inner')... return inner&gt;&gt;&gt; @deco... def target():... print('running target()')&gt;&gt;&gt; target()running inner&gt;&gt;&gt; target&lt;function deco.&lt;locals&gt;.inner at 0x05CDF348&gt; The first crucial fact about decorators is that they have the power to replace the decoratedfunction with a different one. The second crucial fact is that they are executed immediately when a moduleis loaded. This is explained next. When Python Executes Decorators123456789101112131415161718192021222324252627282930313233# _registration.py_registry=[]def register(func): print('running register(%s)'%func) registry.append(func) return func@registerdef f1(): print('running f1()')@registerdef f2():print('running f2()')def f3():print('running f3()')def main(): print('running main()') print('registry -&gt;', registry) f1() f2() f3()if __name__ == '__main__': main()#If registration.py is imported&gt;&gt;&gt; importregistrationrunning register(&lt;function f1 at 0x10063b1e0&gt;)running register(&lt;function f2 at 0x10063b268&gt;)&gt;&gt;&gt; registration.registry[&lt;function f1 at 0x10063b1e0&gt;, &lt;function f2 at 0x10063b268&gt;] decorators are executed as soon as the module is imported, but the decorated functions only runwhen they are explicitly invoked. This highlights the difference between what Pythonistas call importtime and runtime. Considering how decorators are commonly employed in real code, above example is unusual in two ways: The decorator function is defined in the same module as the decorated functions. A real decorator is usually defined in one module and applied to functions in other modules. The register decorator returns the same function passed as argument. In practice, most decorators define an inner function and return it. Decorator-Enhanced Strategy Pattern1234567891011121314151617181920212223242526272829303132promos=[]def promotion(promo_func): promos.append(promo_func) return promo_func@promotiondef fidelity(order): """5% discount for customers with 1000 or more fidelity points""" return order.total() *0.05 if order.customer.fidelity &gt;=1000 else 0@promotiondef bulk_item(order): """10% discount for each LineItem with 20 or more units""" discount=0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * 0.1 return discount@promotiondef large_order(order): """7% discount for orders with 10 or more distinct items""" distinct_items=&#123;item.product for item in order.cart&#125; if len(distinct_items) &gt;= 10: return order.total() * 0.07 return 0def best_promo(order): """Select best discount available""" return max(promo(order) for promo in promos) Most decorators do change the decorated function. They usually do it by defining an inner function andreturning it to replace the decorated function. Code that uses inner functions almost always depends onclosures to operate correctly. Variable Scope Rules12345678910111213&gt;&gt;&gt; b=6&gt;&gt;&gt; def f2(a):... print(a)... print(b)... b=9... &gt;&gt;&gt; f2(3)3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in f2UnboundLocalError: local variable 'b' referenced before assignment When Python compiles the body of the function, it decides that b is a local variable because it is assigned within the function. Python does not require you to declare variables, but assumes that a variable assigned in thebody of a function is local. This is much better than the behavior of JavaScript, which doesnot require variable declarations either, but if you do forget to declare that a variable is local (withvar), you may clobber a global variable without knowing. If we want the interpreter to treat b as a global variable in spite of the assignment within the function,we use the global declaration： 1234567891011121314&gt;&gt;&gt; def f3(a):... global b... print(a)... print(b)... b=9... &gt;&gt;&gt; f3(3)36&gt;&gt;&gt; b9&gt;&gt;&gt; f3(3)39]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_design pattern- Command]]></title>
    <url>%2F2018%2F04%2F22%2FC06-design-pattern-Command%2F</url>
    <content type="text"><![CDATA[Command is another design pattern that can be simplified by the use of functions passed as arguments. MacroCommand12345678class MacroCommand: def __init__(self, commands): self.commands=list(commands) def __call__(self): for command in self.commands: command() More advanced uses of the Command pattern—to support undo, for example—may require morethan a simple callback function. Even then, Python provides a couple of alternatives that deserveconsideration: A callable instance like MacroCommand can keep whatever state is necessary, and provide extra methods in addition to __call__. A closure can be used to hold the internal state of a function between calls.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_Strategy Design and Function-Oriented Strategy]]></title>
    <url>%2F2018%2F04%2F22%2FC06-Strategy-Design-and-Function-Oriented-Strategy%2F</url>
    <content type="text"><![CDATA[Strategy Designpromotion.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from abc import ABC, abstractmethodfrom collections import namedtupleCustomer=namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self,product,quantity,price): self.product=product self.quantity=quantity self.price=price def total(self): return self.price * self.quantityclass Order: def __init__(self, customer,cart, promotion=None): self.customer=customer self.cart=list(cart) self.promotion=promotion def total(self): if not hasattr(self,'__total'): self.__total=sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount=0 else: discount=self.promotion.discount(self) return self.total()- discount def __repr__(self): fmt='&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total, self.due())class Promotion(ABC): @abstractmethod def discount(self,order): ''' return discount as a positive dollar amount '''class FidelityPromo(Promotion): ''' 5% discount for customers with 1000 or more fidelity points ''' def discount(self, order): return order.total() *0.05 if order.customer.fidelity &gt;=1000 else 0class BulkItemPromo(Promotion): ''' 10% discount for each LineItem with 20 or more units ''' def discount(self, order): discount=0 for item in order.cart: if item.quantity &gt;= 20: discount+= item.total() * 0.1 return discountclass LargeOrderPromo(Promotion): ''' "7% discount for orders with 10 or more distinct items ''' def discount(self, order): distinct_items=&#123;item.products for item in order.cart&#125; if len(distinct_items) &gt; 10: return order.total() * 0.07 return 0 Sample usage of Order class with different promotions applied12345678910111213141516171819&gt;&gt;&gt; joe = Customer('John Doe', 0) &gt;&gt;&gt; ann = Customer('Ann Smith', 1100) &gt;&gt;&gt; cart = [LineItem('banana', 4, .5), ... LineItem('apple', 10, 1.5), ... LineItem('watermellon', 5, 5.0)] &gt;&gt;&gt; Order(joe, cart, FidelityPromo()) &lt;Order total: 42.00 due: 42.00&gt; &gt;&gt;&gt; Order(ann, cart, FidelityPromo()) &lt;Order total: 42.00 due: 39.90&gt; &gt;&gt;&gt; banana_cart = [LineItem('banana', 30, .5), ... LineItem('apple', 10, 1.5)] &gt;&gt;&gt; Order(joe, banana_cart, BulkItemPromo()) &lt;Order total: 30.00 due: 28.50&gt; &gt;&gt;&gt; long_order = [LineItem(str(item_code), 1, 1.0) ... for item_code in range(10)] &gt;&gt;&gt; Order(joe, long_order, LargeOrderPromo()) &lt;Order total: 10.00 due: 9.30&gt; &gt;&gt;&gt; Order(joe, cart, LargeOrderPromo()) &lt;Order total: 42.00 due: 42.00&gt; Function-Oriented Strategy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from collections import namedtupleCustomer = namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self, product, quantity, price): self.product = product self.quantity = quantity self.price = price def total(self): return self.price * self.quantityclass Order: # the Context def __init__(self, customer, cart, promotion=None): self.customer = customer self.cart = list(cart) self.promotion = promotion def total(self): if not hasattr(self, '__total'): self.__total = sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount = 0 else: discount = self.promotion(self) # &lt;1&gt; return self.total() - discount def __repr__(self): fmt = '&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total(), self.due())# &lt;2&gt;def fidelity_promo(order): # &lt;3&gt; """5% discount for customers with 1000 or more fidelity points""" return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0def bulk_item_promo(order): """10% discount for each LineItem with 20 or more units""" discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discountdef large_order_promo(order): """7% discount for orders with 10 or more distinct items""" distinct_items = &#123;item.product for item in order.cart&#125; if len(distinct_items) &gt;= 10: return order.total() * .07 return 0# END STRATEGY Sample usage of Order class with promotions as functions1234567&gt;&gt;&gt; joe = Customer('John Doe', 0) &gt;&gt;&gt; ann = Customer('Ann Smith', 1100) &gt;&gt;&gt; cart = [LineItem('banana', 4, .5), ... LineItem('apple', 10, 1.5), ... LineItem('watermellon', 5, 5.0)] &gt;&gt;&gt; Order(joe, cart, fidelity_promo) &lt;Order total: 42.00 due: 42.00&gt; best_promo1234promos=[fidelity_promo, bulk_item_promo, large_order_promo]def best_promo(order): return max(promo(order) for promo in promos) globals()globals()Return a dictionary representing the current global symbol table. This is always the dictionary of the current module (inside a function or method, this is the module where it is defined, not the module from which it is called). 123promos=[globals()[name] for name in globals() if name.endswith('_promo') and name != 'best_promo']def best_promo(order): return max(promo(order) for promo in promos) create a module and put all the strategy functions there, except for best_promoThe list of strategy functions is built by introspection of a separate module called promotions. 1234promos=[func for name, func in inspect.getmembsers(promotions, inspect.isfunction)]def best_promo(order): return max(promo(order) for promo in promos) The function inspect.getmembers returns the attributes of an object—in this case, the promotions module—optionally filtered by a predicate (a boolean function). We use inspect.isfunction to get only the functions from the module.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_functional programming]]></title>
    <url>%2F2018%2F04%2F22%2FC05-functional-programming%2F</url>
    <content type="text"><![CDATA[The operator Module123&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fact(n):... return reduce(lambda a,b : a*b, ranage(1,n+1)) To save you the trouble of writing trivial anonymous functions like lambda a, b:a*b, the operator module provides function equivalents for dozens of arithmetic operators.123&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; def fact(n):... return reduce(mul, range(1,n+1)) itemgetter and attrgetteritemgetter itemgetter(1) does the same as lambda fields: fields[1]: create a function that, given a collection, returns the item at index 1. Because itemgetter uses the [ ] operator, it supports not only sequences but also mappings andany class that implements __getitem__. 1234567891011121314151617&gt;&gt;&gt; metro_data = [... ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),... ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),... ]&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; for city in sorted(metro_data, key=itemgetter(1)):... print(city)('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))('Tokyo', 'JP', 36.933, (35.689722, 139.691667))# pass multiple index arguments to itemgetter, the function it builds will returntuples with the extracted values&gt;&gt;&gt; cc_name = itemgetter(1, 0)&gt;&gt;&gt; for city in metro_data:... print(cc_name(city))('JP', 'Tokyo')('IN', 'Delhi NCR') attrgetter attrgetter creates functions to extract object attributes by name. If you pass attrgetter severalattribute names as arguments, it also returns a tuple of values. In addition, if any argument name contains a . (dot), attrgetter navigates through nested objects to retrieve the attribute.123456789101112131415161718192021222324&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; LatLong = namedtuple('LatLong', 'lat long') #&gt;&gt;&gt; Metropolis = namedtuple('Metropolis', 'name cc pop coord') # &gt;&gt;&gt; metro_areas = [Metropolis(name, cc, pop, LatLong(lat, long)) #... for name, cc, pop, (lat, long) in metro_data]&gt;&gt;&gt; metro_areas[0]Metropolis(name='Tokyo', cc='JP', pop=36.933, coord=LatLong(lat=35.689722,long=139.691667))&gt;&gt;&gt; metro_areas[0].coord.lat #Reach into element metro_areas[0] to get its latitude.35.689722&gt;&gt;&gt; from operator import attrgetter&gt;&gt;&gt; name_lat = attrgetter('name', 'coord.lat') #Define an attrgetter to retrieve the name and the coord.lat nested attribute&gt;&gt;&gt;&gt;&gt;&gt; for city in sorted(metro_areas, key=attrgetter('coord.lat')): # Use attrgetter again to sort list of cities by latitude.... print(name_lat(city)) #...('Sao Paulo', -23.547778)('Mexico City', 19.433333)('Delhi NCR', 28.613889)('Tokyo', 35.689722)('New York-Newark', 40.808611) methodcallermethodcaller creates a function on the fly. The function it creates calls a method by name on theobject given as argument. 12345678&gt;&gt;&gt; from operator import methodcaller&gt;&gt;&gt; s = 'The time has come'&gt;&gt;&gt; upcase = methodcaller('upper')&gt;&gt;&gt; upcase(s)'THE TIME HAS COME'&gt;&gt;&gt; hiphenate = methodcaller('replace', ' ', '-')&gt;&gt;&gt; hiphenate(s)'The-time-has-come' functools.partialfunctools.partial is a higher-order function that allows partial application of a function. Given a function, a partial application produces a new callable with some of the arguments of the original function fixed. partial takes a callable as first argument, followed by an arbitrary number of positional and keywordarguments to bind. 1234567&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; triple=partial(mul, 3)&gt;&gt;&gt; triple(7)21&gt;&gt;&gt; list(map(triple, range(1,10)))[3, 6, 9, 12, 15, 18, 21, 24, 27] 1234567891011&gt;&gt;&gt; tag&lt;function tag at 0x03DFA300&gt;&gt;&gt;&gt; picture=partial(tag, 'img', cls='pic-frame')&gt;&gt;&gt; picturefunctools.partial(&lt;function tag at 0x03DFA300&gt;, 'img', cls='pic-frame')&gt;&gt;&gt; picture.func&lt;function tag at 0x03DFA300&gt;&gt;&gt;&gt; picture.args('img',)&gt;&gt;&gt; picture.keywords&#123;'cls': 'pic-frame'&#125; The functools.partialmethod function (new in Python 3.4) does the same job as partial, but isdesigned to work with methods.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_functions]]></title>
    <url>%2F2018%2F04%2F22%2FC05-functions%2F</url>
    <content type="text"><![CDATA[Higher-Order FunctionsReplacements for map, filter, and reduceFunctional languages commonly offer the map, filter, and reduce higher-order functions(sometimes with different names). A listcomp or a genexp does the job of map and filtercombined, but is more readable. 12345678&gt;&gt;&gt; list(map(fact, range(6)))[1, 1, 2, 6, 24, 120]&gt;&gt;&gt; [fact(n) for n in range(6)][1, 1, 2, 6, 24, 120]&gt;&gt;&gt; list(map(factorial, filter(lambda n: n % 2, range(6))))[1, 6, 120]&gt;&gt;&gt; [factorial(n) for n in range(6) if n % 2][1, 6, 120] map and filter return generators—a form of iterator—so their direct substitute is now a generator expression . The reduce function was demoted from a built-in in Python 2 to the functools module in Python 3.123456&gt;&gt;&gt; from functools import reduce #Starting with Python 3.0, reduce is not a built-in&gt;&gt;&gt; from operator import add&gt;&gt;&gt; reduce(add, range(100))4950&gt;&gt;&gt; sum(range(100)) Other reducing built-ins are all and any:all(iterable)Returns True if every element of the iterable is truthy; all([ ]) returns True.any(iterable)Returns True if any element of the iterable is truthy; any([ ]) returns False. Anonymous FunctionsThe best use of anonymous functions is in the context of an argument list. 12&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=lambda word: word[::-1]) Callable ObjectsThe call operator (i.e., ()) may be applied to other objects beyond user-defined functions. To determinewhether an object is callable, use the callable() built-in function. User-Defined Callable TypesArbitrary Python objects may also be made to behave like functions. Implementing a __call__instance method is all it takes. 123456789101112131415import randomclass BingoCage: def __init__(self, items): self._items=list(items) random.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError('pick from empty BingoCage') def __call__(self): return self.pick() A class implementing *__call__**is an easy way to create function-like objects that havesome internal state that must be kept across invocations. An example is a decorator. Decorators must befunctions, but it is sometimes convenient to be able to “remember” something between calls of thedecorator. Function Introspection123456&gt;&gt;&gt; dir(factorial)['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__'] __dict__Like the instances of a plain user-defined class, a function uses the__dict__attribute to store userattributes assigned to it. Listing attributes of functions that don’t exist in plain instances1234567&gt;&gt;&gt; class C: pass #&gt;&gt;&gt; obj = C() #&gt;&gt;&gt; def func(): pass #&gt;&gt;&gt; sorted(set(dir(func)) - set(dir(obj))) #['__annotations__', '__call__', '__closure__', '__code__', '__defaults__','__get__', '__globals__', '__kwdefaults__', '__name__', '__qualname__'] Positional and Keyword-Only ParametersUsing* and ** to “explode” iterables and mappings into separate arguments whenwe call a function. __defaults__ 123456789101112131415def clip(text, max_len=80): """Return text clipped at the last space before or after max_len """ end = None if len(text) &gt; max_len: space_before = text.rfind(' ', 0, max_len) if space_before &gt;= 0: end = space_before else: space_after = text.rfind(' ', max_len) if space_after &gt;= 0: end = space_after if end is None: # no spaces were found end = len(text) return text[:end].rstrip() The values of __defaults__, __code__.co_varnames, and__code__.co_argcount for the clip function:123456789&gt;&gt;&gt; from clip import clip&gt;&gt;&gt; clip.__defaults__(80,)&gt;&gt;&gt; clip.__code__ # doctest: +ELLIPSIS&lt;code object clip at 0x...&gt;&gt;&gt;&gt; clip.__code__.co_varnames(&apos;text&apos;, &apos;max_len&apos;, &apos;end&apos;, &apos;space_before&apos;, &apos;space_after&apos;)&gt;&gt;&gt; clip.__code__.co_argcount2 Extracting the function signature:123456789101112&gt;&gt;&gt; from clip import clip&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; sig = signature(clip)&gt;&gt;&gt; sig # doctest: +ELLIPSIS&lt;inspect.Signature object at 0x...&gt;&gt;&gt;&gt; str(sig)'(text, max_len=80)'&gt;&gt;&gt; for name, param in sig.parameters.items():... print(param.kind, ':', name, '=', param.default)...POSITIONAL_OR_KEYWORD : text = &lt;class 'inspect._empty'&gt;POSITIONAL_OR_KEYWORD : max_len = 80 The kind attribute holds one of five possible values from the _ParameterKind class:POSITIONAL_OR_KEYWORDA parameter that may be passed as a positional or as a keyword argument (most Python functionparameters are of this kind).VAR_POSITIONALA tuple of positional parameters.VAR_KEYWORDA dict of keyword parameters.KEYWORD_ONLYA keyword-only parameter (new in Python 3).POSITIONAL_ONLYA positional-only parameter; currently unsupported by Python function declaration syntax. inspect.Signature 12345678910111213141516171819def tag(name,*content, cls=None, **attrs): pass&gt;&gt;&gt; import inspect&gt;&gt;&gt; sig = inspect.signature(tag)&gt;&gt;&gt; my_tag = &#123;'name': 'img', 'title': 'Sunset Boulevard',... 'src': 'sunset.jpg', 'cls': 'framed'&#125;&gt;&gt;&gt; bound_args = sig.bind(**my_tag)&gt;&gt;&gt; bound_args&lt;inspect.BoundArguments object at 0x...&gt;&gt;&gt;&gt; for name, value in bound_args.arguments.items(): #Iterate over the items in bound_args.arguments, which is an OrderedDict, to display the names and # values of the arguments.... print(name, '=', value)...name = imgcls = framedattrs = &#123;'title': 'Sunset Boulevard', 'src': 'sunset.jpg'&#125;&gt;&gt;&gt; del my_tag['name']&gt;&gt;&gt; bound_args = sig.bind(**my_tag)Traceback (most recent call last): Function AnnotationsEach argument in the function declaration may have an annotation expression preceded by :. If there is a default value, the annotation goes between the argument name and the = sign. To annotate the return value, add -&gt; and another expression between the ) and the : at the tail of the function declaration. The expressions may be of any type. 12345678# clip_annot.pydef clip(text:str, max_len:'int &gt; 0' =80) -&gt;str: end=None if len(text) &gt; max_len: pass&gt;&gt;&gt; from clip_annot import clip&gt;&gt;&gt; clip.__annotations__&#123;'text': &lt;class 'str'&gt;, 'max_len': 'int &gt; 0', 'return': &lt;class 'str'&gt;&#125; Extracting annotations from the function signatureThe signature function returns a Signature object, which has a return_annotation attribute and aparameters dictionary mapping parameter names to Parameter objects. Each Parameter object has itsown annotation attribute. 12345678910&gt;&gt;&gt; from clip_annot import clip&gt;&gt;&gt; from inspect import signature&gt;&gt;&gt; sig = signature(clip)&gt;&gt;&gt; sig.return_annotation&lt;class 'str'&gt;&gt;&gt;&gt; for param in sig.parameters.values():... note = repr(param.annotation).ljust(13)... print(note, ':', param.name, '=', param.default)&lt;class 'str'&gt; : text = &lt;class 'inspect._empty'&gt;'int &gt; 0' : max_len = 80]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_bytes, bytesarray, struct module, memory views]]></title>
    <url>%2F2018%2F04%2F22%2FC04-bytes-bytesarray-struct-module-memory-views%2F</url>
    <content type="text"><![CDATA[The Unicode standard explicitly separates the identity of characters from specific byte representations: The identity of a character The actual bytes that represent a character depend on the encoding in use. Although binary sequences are really sequences of integers, their literal notation reflects the fact that ASCII text is often embedded in them. Therefore, three different displays are used, depending on each byte value: For bytes in the printable ASCII range—from space to ~—the ASCII character itself is used. For bytes corresponding to tab, newline, carriage return, and \, the escape sequences \t, \n, \r, and \ are used. For every other byte value, a hexadecimal escape sequence is used (e.g., \x00 is the null byte). Binary sequences have a class method that str doesn’t have, called fromhex, which builds a binary sequence by parsing pairs of hex digits optionally separated by spaces:12&gt;&gt;&gt; bytes.fromhex('31 4B CE A9')b'1K\xce\xa9' Building a binary sequence from a buffer-like object is a low-level operation that may involve type casting.12345&gt;&gt;&gt; import array&gt;&gt;&gt; numbers = array.array('h', [-2, -1, 0, 1, 2])&gt;&gt;&gt; octets = bytes(numbers)&gt;&gt;&gt; octetsb'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00' Structs and Memory ViewsThe struct module provides functions to parse packed bytes into a tuple of fields of different types and to perform the opposite conversion, from a tuple into packed bytes. Struct is used with bytes, bytearray, and memoryview objects. 123&gt;&gt;&gt; import struct&gt;&gt;&gt; fmt = '&lt;3s3sHH' #struct format: &lt; little-endian; 3s3s two sequences of 3 bytes; HH two 16-bitintegers. 123456789101112&gt;&gt;&gt; with open('filter.gif', 'rb') as fp:... img = memoryview(fp.read()) #...&gt;&gt;&gt; header = img[:10] # another memoryview by slicing the first one; no bytes are copied here&gt;&gt;&gt; bytes(header) #Convert to bytes for display only; 10 bytes are copied hereb'GIF89a+\x02\xe6\x00'&gt;&gt;&gt; struct.unpack(fmt, header) #Unpack memoryview into tuple of: type, version, width, and height(b'GIF', b'89a', 555, 230)&gt;&gt;&gt; del header #Delete references to release the memory associated with the memoryviewinstances&gt;&gt;&gt; del img Discover the Encoding of a Byte SequenceIf you omit the encoding argument when opening a file, the default is given by locale.getpreferredencoding() (‘cp1252’ on Windows). It is the default for opening text files and for sys.stdout/stdin/stderr when they are redirected to files. The encoding of sys.stdout/stdin/stderr is given by the PYTHONIOENCODING environment variable, if present, otherwise it is either inherited from the console or defined by locale.getpreferredencoding() if the output/input is redirected to/from a file. sys.getdefaultencoding() is used internally by Python to convert binary data to/from str; this happens less often in Python 3, but still happens. sys.getfilesystemencoding() is used to encode/decode filenames (not file contents). It is used when open() gets a str argument for the filename; if the filename is given as a bytes argument, it is passed unchanged to the OS API. The Python Unicode HOWTO says: “on Windows, Python uses the name mbcs to refer to whatever the currently configured encoding is.” The acronym MBCS stands for Multi Byte Character Set, which for Microsoft are the legacy variable-width encodings like gb2312 or Shift_JIS, but not UTF-8.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_set, hash tables]]></title>
    <url>%2F2018%2F04%2F22%2FC03-set-hash-tables%2F</url>
    <content type="text"><![CDATA[set, forzensetA set is a collection of unique objects. A basic use case is removing duplication: 12345&gt;&gt;&gt; l = ['spam', 'spam', 'eggs', 'spam']&gt;&gt;&gt; set(l)&#123;'eggs', 'spam'&#125;&gt;&gt;&gt; list(set(l))['eggs', 'spam'] There is no special syntax to represent frozenset literals—they must be created by calling the constructor. 12&gt;&gt;&gt; frozenset(range(10))frozenset(&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;) There’s no literal notation for the empty set, so we must remember to write set(). hashA hash table is a sparse array (i.e., an array that always has empty cells). In standard data structure texts,the cells in a hash table are often called “buckets.” In a dict hash table, there is a bucket for each item, andit contains two fields: a reference to the key and a reference to the value of the item. Because all bucketshave the same size, access to an individual bucket is done by offset. To put an item in a hash table, the first step is to calculate the hash value of the item key, which is donewith the hash() built-in function. Hashes and equalityThe hash() built-in function works directly with built-in types and falls back to calling __hash__ for user-defined types. If two objects compare equal, their hash values must also be equal, otherwise the hashtable algorithm does not work. To be effective as hash table indexes, hash values should scatter around the index space as much aspossible. This means that, ideally, objects that are similar but not equal should have hash values that differwidely. hash table algorithmTo fetch the value at my_dict[search_key], Python calls hash(search_key) to obtain the hash value ofsearch_key and uses the least significant bits of that number as an offset to look up a bucket in the hashtable (the number of bits used depends on the current size of the table). If the found bucket is empty,KeyError is raised.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re module_compile, finditer, group, match]]></title>
    <url>%2F2018%2F04%2F22%2Fre-module-compile-finditer-group-match%2F</url>
    <content type="text"><![CDATA[使用 re 模块有两种方式： 使用 re.compile 函数生成一个 Pattern 对象，然后使用 Pattern 对象的一系列方法对文本进行匹配查找； 直接使用 re.match, re.search 和 re.findall 等函数直接对文本匹配查找。 re 模块的一般使用步骤如下： 使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象 通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果（一个 Match 对象） 最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作 Python 的正则匹配默认是贪婪匹配。 compile12import repattern=re.compile(r'\w+') pattern的常用方法： match 方法 search 方法 findall 方法 finditer 方法 split 方法 sub 方法 subn 方法 matchmatch 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。123456789101112&gt;&gt;&gt; pattern=re.compile(r'\d+')&gt;&gt;&gt; m=pattern.match(''one12twothree34four', 3, 10') #从'1'的位置开始匹配，正好匹配&gt;&gt;&gt; print(m) # 返回一个 Match 对象&lt;_sre.SRE_Match object at 0x10a42aac0&gt;&gt;&gt;&gt; m.group(0) # 可省略 0'12'&gt;&gt;&gt; m.start(0) # 可省略 03&gt;&gt;&gt; m.end(0) # 可省略 05&gt;&gt;&gt; m.span(0) # 可省略 0(3, 5) 当匹配成功时返回一个 Match 对象，其中： group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)； start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0； end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0； span([group]) 方法返回 (start(group), end(group))。 123456&gt;&gt;&gt; pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I) # re.I 表示忽略大小写&gt;&gt;&gt; m = pattern.match('Hello World Wide Web')&gt;&gt;&gt; m.group(0) # 返回匹配成功的整个子串'Hello World'&gt;&gt;&gt; m.groups() # 等价于 (m.group(1), m.group(2), ...)('Hello', 'World') finditerfinditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器12for match in pattern.finditer(hello 234 ref'): print(match.group(), m1.span())]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_Generic Mapping Types, hashable]]></title>
    <url>%2F2018%2F04%2F22%2FC03-Generic-Mapping-Types-hashable%2F</url>
    <content type="text"><![CDATA[hashableAn object is hashable if it has a hash value which never changes during its lifetime (it needs a hash()method), and can be compared to other objects (it needs an eq() method). Hashable objects whichcompare equal must have the same hash value.The atomic immutable types (str, bytes, numeric types) are all hashable. A frozen set is always hashable,because its elements must be hashable by definition. A tuple is hashable only if all its items are hashable. User-defined types are hashable by default because their hash value is their id() and they all compare notequal. If an object implements a custom eq that takes into account its internal state, it may be hashable only if all its attributes are immutable. ways to build a dict1234567&gt;&gt;&gt; a = dict(one=1, two=2, three=3)&gt;&gt;&gt; b = &#123;'one': 1, 'two': 2, 'three': 3&#125;&gt;&gt;&gt; c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))&gt;&gt;&gt; d = dict([('two', 2), ('one', 1), ('three', 3)])&gt;&gt;&gt; e = dict(&#123;'three': 3, 'one': 1, 'two': 2&#125;)&gt;&gt;&gt; a == b == c == d == eTrue dict comprehension A dictcomp builds a dict instance by producing key:value pair from any iterable. dict.setdefault12345678910111213141516import sysimport reWORD_RE = re.compile('\w+')index = &#123;&#125;with open(sys.argv[1], encoding='utf-8') as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) # this is ugly; coded like this to make a point occurrences = index.get(word, []) occurrences.append(location) index[word] = occurrences for word in sorted(index, key=str.upper): print(word, index[word]) 123456789with open(sys.argv[1], encoding='utf-8') as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) index.setdefault(word, []).append(location) #Get the list of occurrences for word, or set it to [ ] if not found; setdefault returns the value, so it can be #updated without requiring a second search. collections.defaultdictA defaultdict is configured to create items on demand whenever a missing key is searched. When instantiating a defaultdict, you provide a callable that is used to produce a default value whenever__getitem__ is passed a nonexistent key argument. given an empty defaultdict created as dd = defaultdict(list), if ‘new-key’ is not in dd, the expressiondd[‘new-key’] does the following steps: Calls list() to create a new list. Inserts the list into dd using ‘new-key’ as key. Returns a reference to that list 123456789101112131415161718import sysimport reimport collectionsword_re=re.compile('\w+')index=collections.defaultdict(list) #Create a defaultdict with the list constructor as default_factory.If no default_factory is provided, the usual KeyError is raised for missing keys.with open(sys.argv[1],encoding='utf-8') as fp: for line_no, line in enumerate(fp,1): for match in word_re.finditer(line): word=match.group() column_no=match.start()+1 location=(line_no,column_no) index[word].append(location) #If word is not initially in the index, the default_factory is called to produce the missing value, which in this #case is an empty list that is then assigned to index[word] and returned, so the .append(location) #operation always succeeds. __missing__If you subclass dict and provide a __missing__ method, the standard dict.getitemwill call itwhenever a key is not found, instead of raising KeyError. The __missing__ method is just called by __getitem__ (i.e., forthe d[k] operator). The presence of a __missing__ method has no effect on the behavior of other methods that look up keys, such as get or__contains__ (which implements the in operator). This is why the default_factory of defaultdict worksonly with__getitem__, 12345678910111213141516class StrKeyDict0(dict): def __missing__(self,key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def get(self,key,default=None): try: return self[key] #The get method delegates to __getitem__ by using the self[key] notation; that gives the opportunity for our __missing__ to act. except KeyError: return default def __contains__(self,key): #we do not check for the key in the usual Pythonic way—k in my_dict—becausestr(key) in self would recursively call __contains__. We avoid this by explicitly looking up the key in self.keys(). return key in self.keys() or str(key) in self.keys() collections.OrderedDict collections.ChainMap collections.Counter collections.UserDictUserDict it’s preferable to subclass from UserDict rather than from dict is that the built-in has someimplementation shortcuts that end up forcing us to override methods that we can just inherit fromUserDict with no problems. Note that UserDict does not inherit from dict, but has an internal dict instance, called data, which holdsthe actual items. This avoids undesired recursion when coding special methods like __setitem__, andsimplifies the coding of __contains__.1234567891011class StrKeyDict(collections.UserDict): def __missing__(self,key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def __contains__(self, key): return str(key) in self.data def __setitem__(self, key, item): self.data[str(key)] = item UserDict subclasses MutableMapping, the remaining methods that make StrKeyDict a full-fledgedmapping are inherited from UserDict, MutableMapping, or Mapping. The latter have several usefulconcrete methods, in spite of being abstract base classes (ABCs). The following methods are worth noting: MutableMapping.update This powerful method can be called directly but is also used by __init__ to load the instance from other mappings, from iterables of (key, value) pairs, and keyword arguments. Because it uses self[key] = value to add items, it ends up calling our implementation of __setitem__. Mapping.get In StrKeyDict0 (Example 3-7), we had to code our own get to obtain results con‐sistent with getitem, but in Example 3-8 we inherited Mapping.get, which isimplemented exactly like StrKeyDict0.get (see Python source code). MappingProxyTypeThe types module provides a wrapper class called MappingProxyType, which, given a mapping, returns amappingproxy instance that is a read-only but dynamic view of the original mapping. This means thatupdates to the original mapping can be seen in the mappingproxy, but changes cannot be made throughit. 12345678910111213141516&gt;&gt;&gt; from types import MappingProxyType&gt;&gt;&gt; d = &#123;1: 'A'&#125;&gt;&gt;&gt; d_proxy = MappingProxyType(d)&gt;&gt;&gt; d_proxymappingproxy(&#123;1: 'A'&#125;)&gt;&gt;&gt; d_proxy[1] # Items in d can be seen through d_proxy.'A'&gt;&gt;&gt; d_proxy[2] = 'x' # Changes cannot be made through d_proxy.Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment&gt;&gt;&gt; d[2] = 'B'&gt;&gt;&gt; d_proxy # d_proxy is dynamic: any change in d is reflectedmappingproxy(&#123;1: 'A', 2: 'B'&#125;)&gt;&gt;&gt; d_proxy[2]'B']]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C_deque]]></title>
    <url>%2F2018%2F04%2F22%2FC-deque%2F</url>
    <content type="text"><![CDATA[Inserting and removing from the left of a list (the 0-index end) is costly because the entire list must be shifted. The class collections.deque is a thread-safe double-ended queue designed for fast inserting and removingfrom both ends. It is also the way to go if you need to keep a list of “last seen items” or something like that, because a deque can be bounded—i.e., created with a maximum length—and then, when it is full, itdiscards items from the opposite end when you append new ones.The append and popleft operations are atomic, so deque is safe to use as a LIFO queue in multithreadedapplications without the need for using locks.12345678910111213141516171819&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; dq = deque(range(10), maxlen=10)&gt;&gt;&gt; dqdeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.rotate(3)&gt;&gt;&gt; dqdeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)&gt;&gt;&gt; dq.rotate(-4)&gt;&gt;&gt; dqdeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)&gt;&gt;&gt; dq.appendleft(-1) #Appending to a deque that is full discards items from the other end&gt;&gt;&gt; dqdeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.extend([11, 22, 33])&gt;&gt;&gt; dqdeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)&gt;&gt;&gt; dq.extendleft([10, 20, 30, 40])&gt;&gt;&gt; dqdeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Array, Memory Views]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Array-Memory-Views%2F</url>
    <content type="text"><![CDATA[An array does not actually hold full-fledged float objects, but only the packed bytes representing theirmachine values—just like an array in the C language. On the other hand, if you are constantly adding andremoving items from the ends of a list as a FIFO or LIFO data structure, a deque (double-ended queue)works faster. array.arrayIf the list will only contain numbers, an array.array is more efficient than a list: it supports all mutablesequence operations (including .pop, .insert, and .extend), and additional methods for fast loading andsaving such as .frombytes and .tofile. When creating an array, you provide a typecode,A letter to determine the underlying C type used to store each item in the array. For example, b is thetypecode for signed char. If you create an array(‘b’), then each item will be stored in a single byte andinterpreted as an integer from –128 to 127. For large sequences of numbers, this saves a lot of memory.And Python will not let you put any number that does not match the type for the array.12345678910111213141516&gt;&gt;&gt; from array import array&gt;&gt;&gt; from random import random&gt;&gt;&gt; floats = array('d', (random() for i in range(10**7)))&gt;&gt;&gt; floats[-1]0.07802343889111107&gt;&gt;&gt; fp = open('floats.bin', 'wb')&gt;&gt;&gt; floats.tofile(fp)&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2 = array('d') &gt;&gt;&gt; fp = open('floats.bin', 'rb')&gt;&gt;&gt; floats2.fromfile(fp, 10**7)&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2[-1]0.07802343889111107&gt;&gt;&gt; floats2 == floatsTrue As of Python 3.4, the array type does not have an in-place sort method like list.sort(). If you need to sortan array, use the sorted function to rebuild it sorted:1a=array.array(typecode, sorted(a)) memoryviewA memoryview is essentially a generalized NumPy array structure in Python itself (without the math). Itallows you to share memory between data-structures (things like PIL images, SQLlite databases, NumPyarrays, etc.) without first copying. This is very important for large data sets. memoryview.cast method lets you change the way multiple bytes are read or written as units withoutmoving bits around (just like the C cast operator). memoryview.cast returns yet another memoryviewobject, always sharing the same memory]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_list.sort and the sorted Built-In Function, bisect]]></title>
    <url>%2F2018%2F04%2F22%2FC02-list-sort-and-the-sorted-Built-In-Function-bisect%2F</url>
    <content type="text"><![CDATA[list.sort and sortedThe list.sort method sorts a list in place—that is, without making a copy. It returns None to remind us that itchanges the target object, and does not create a new list. This is an important Python API convention:functions or methods that change an object in place should return None to make it clear to the caller thatthe object itself was changed, and no new object was created.The built-in function sorted creates a new list and returns it. In fact, itaccepts any iterable object as anargument, including immutable sequences and generators. Regardless of the type of iterable given to sorted, it always returns a newly created list Both list.sort and sorted take two optional, keyword-only arguments:reverse: If True, the items are returned in descending order (i.e., by reversing the comparison of the items). The default is False. key: A one-argument function that will be applied to each item to produce its sorting key. For example, when sorting a list of strings, key=str.lower can be used to perform a case-insensitive sort, and key=len will sort the strings by character length. The default is the identity function (i.e., the items themselves are compared).bisectThe bisect module offers two main functions—bisect and insort. bisect(haystack, needle) does a binary search for needle in haystack—which must be a sorted sequence.You could use the result of bisect(haystack, needle) as the index argument to haystack.insert(index, needle)—however, using insort does both steps, and is faster. bisect finds insertion points for items in a sorted sequencebisect_right returns an insertion point after the existing item, and bisect_left returns the position of theexisting item, so insertion would occur before it.1234567891011121314151617181920212223242526import bisectimport sysHAYSTACK=[1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]NEEDLES=[0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]ROW_FMT='&#123;0:2d&#125; @ &#123;1:2d&#125; &#123;2&#125;&#123;0:&lt;2d&#125;'def demo(bisect_fn): for needle in reversed(NEEDLES): positioin=bisect_fn(HAYSTACK, needle) #Use the chosen bisect function to get the insertion point. offset=positioin * ' |' print(ROW_FMT.format(needle, positioin, offset))if __name__=='__main__': if sys.argv[-1] =='left': bisect_fn=bisect.bisect_left else: bisect_fn=bisect.bisect print('DEMO:', bisect_fn.__name__) print('haystack -&gt;',' '.join('%2d'% n for n in HAYSTACK )) demo(bisect_fn) 1234567&gt;&gt;&gt; import bisect&gt;&gt;&gt; def grade(score, breakpoints=[60,70,80,90],grades='FDCBA'):... i=bisect.bisect(breakpoints, score)... return grades[i]... &gt;&gt;&gt; [grade(score) for score in [33, 99, 77, 70, 89, 90, 100]]['F', 'A', 'C', 'C', 'B', 'A', 'A'] Inserting with bisect.insortinsort(seq, item) inserts item into seq so as to keep seq in ascending order12345678910111213141516171819import bisectimport randomSIZE=7random.seed(1729)my_list=[]for i in range(SIZE): new_item = random.randrange(SIZE*2) bisect.insort(my_list, new_item) print('%2d -&gt;' % new_item, my_list)#10 -&gt; [10]#0 -&gt; [0, 10]#6 -&gt; [0, 6, 10]#8 -&gt; [0, 6, 8, 10]#7 -&gt; [0, 6, 7, 8, 10]#2 -&gt; [0, 2, 6, 7, 8, 10]#10 -&gt; [0, 2, 6, 7, 8, 10, 10]]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Sequence Assignment +=, *=]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Sequence-Assignment%2F</url>
    <content type="text"><![CDATA[#The augmented assignment operators += and *= behave very differently depending on the first operand. The special method that makes += work is iadd (for “in-place addition”). However, if iadd is notimplemented, Python falls back to calling add.If a implements iadd, a += b will be called. In the case of mutable sequences (e.g., list, bytearray,array.array), a will be changed in place (i.e., the effect will be similar to a.extend(b)). However, when a doesnot implement iadd, the expression a += b has the same effect as a = a + b: the expression a + b isevaluated first, producing a new object, which is then bound to a. In general, for mutable sequences, it is a good bet that iadd is implemented and that += happens inplace. For immutable sequences, clearly there is no way for that to happen.1234567891011121314&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; id(l)4311953800&gt;&gt;&gt; l *= 2&gt;&gt;&gt; l[1, 2, 3, 1, 2, 3]&gt;&gt;&gt; id(l)4311953800&gt;&gt;&gt; t = (1, 2, 3)&gt;&gt;&gt; id(t)4312681568&gt;&gt;&gt; t *= 2&gt;&gt;&gt; id(t)4301348296]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Slice Objects]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Slice-Objects%2F</url>
    <content type="text"><![CDATA[The notation a :b :c is only valid within [ ] when used as the indexing or subscript operator, and it produces aslice object: slice(a, b, c).To evaluate the expression seq[start:stop:step], Python calls seq.getitem(slice(start, stop, step)). Assign to slices12345678910&gt;&gt;&gt; l=list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; l[2:5]=[20,30] #When the target of the assignment is a slice, the right side must be an iterable object, even if it has just one item.&gt;&gt;&gt; l[0, 1, 20, 30, 5, 6, 7, 8, 9]&gt;&gt;&gt; del l[5:7]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 8, 9] Building Lists of Lists1123456&gt;&gt;&gt; board = [['_'] * 3 for i in range(3)]&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; board[1][2] = 'X'&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']] Equivalent to:12345678910&gt;&gt;&gt; board = []&gt;&gt;&gt; for i in range(3):... row = ['_'] * 3 #... board.append(row)...&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; board[2][0] = 'X'&gt;&gt;&gt; board #[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']] 2123456&gt;&gt;&gt; weird_board = [['_'] * 3] * 3&gt;&gt;&gt; weird_board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; weird_board[1][2] = 'O'&gt;&gt;&gt; weird_board[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']] Equivalent to:1234row = ['_'] * 3board = []for i in range(3): board.append(row)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Tuple unpacking, namedtuple]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Tuple-unpacking-namedtuple%2F</url>
    <content type="text"><![CDATA[Tuple unpacking works with any iterable object. The only requirement is that the iterable yields exactly oneitem per variable in the receiving tuple, unless you use a star (*) to capture excess items. 123456789101112&gt;&gt;&gt; t=(20,8)&gt;&gt;&gt; divmod(t)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: divmod expected 2 arguments, got 1&gt;&gt;&gt; divmod(*t)(2, 4)&gt;&gt;&gt; import os&gt;&gt;&gt; _,filename=os.path.split('/hom/lubi/.ssh/if.pub')&gt;&gt;&gt; filename'if.pub' use * to grab excess items123456&gt;&gt;&gt; a,b,*rest=range(5)&gt;&gt;&gt; a,b,rest(0, 1, [2, 3, 4])&gt;&gt;&gt; *head,b,c=range(5)&gt;&gt;&gt; head,b,c([0, 1, 2], 3, 4) namedtuple Two parameters are required to create a named tuple: a class name and a list of field names, which can be given as an iterable of strings or as a single spacedelimited string. Data must be passed as positional arguments to the constructor (in contrast, the tuple constructor takes a single iterable). You can access the fields by name or position. 12345678910&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; City = namedtuple('City', 'name country population coordinates')&gt;&gt;&gt; tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667))&gt;&gt;&gt; tokyoCity(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722,139.691667))&gt;&gt;&gt; tokyo.population36.933&gt;&gt;&gt; tokyo[1]'JP' _fields is a tuple with the field names of the class. _make() allow you to instantiate a named tuple from an iterable; City(*delhi_data) would do the same. _asdict() returns a collections.OrderedDict built from the named tuple instance. 12345678910&gt;&gt;&gt; City._fields('name', 'country', 'population', 'coordinates')&gt;&gt;&gt; LatLong = namedtuple('LatLong', 'lat long')&gt;&gt;&gt; delhi_data = ('Delhi NCR', 'IN', 21.935, LatLong(28.613889, 77.208889))&gt;&gt;&gt; delhi = City._make(delhi_data)&gt;&gt;&gt; delhi._asdict()OrderedDict([('name', 'Delhi NCR'), ('country', 'IN'), ('population',21.935), ('coordinates', LatLong(lat=28.613889, long=77.208889))])&gt;&gt;&gt; for key, value in delhi._asdict().items(): print(key + ':', value)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_Generate Expressions]]></title>
    <url>%2F2018%2F04%2F22%2FC02-Generate-Expressions%2F</url>
    <content type="text"><![CDATA[To initialize tuples, arrays, and other types of sequences, you could also start from a listcomp, but a genexpsaves memory because it yields items one by one using the iterator protocol instead of building a whole listjust to feed another constructor.12345678910&gt;&gt;&gt; symbols = '$¢£¥€¤'&gt;&gt;&gt; tuple(ord(symbol) for symbol in symbols)(36, 162, 163, 165, 8364, 164)#If the generator expression is the single argument in a function call, there is no need to duplicate the #enclosing parentheses.&gt;&gt;&gt; import array&gt;&gt;&gt; array.array('I', (ord(symbol) for symbol in symbols))array('I', [36, 162, 163, 165, 8364, 164])#The array constructor takes two arguments, so the parentheses around the generator expression are mandatory.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C02_listcomp_speed.py]]></title>
    <url>%2F2018%2F04%2F22%2FC02-listcomp-speed-py%2F</url>
    <content type="text"><![CDATA[Container sequences and Flat sequencesContainer sequences:list, tuple, and collections.deque can hold items of different types. Flat sequences:str, bytes, bytearray, memoryview, and array.array hold items of one type. Container sequences hold references to the objects they contain, which may be of any type, while flatsequences physically store the value of each item within its own memory space, and not as distinctobjects. Mutable sequences and Immutable sequencesMutable sequences:list, bytearray, array.array, collections.deque, and memoryview Immutable sequences:tuple, str, and bytes map and filter were faster than the equivalent listcomps123456789101112131415161718import timeitTIMES = 10000SETUP = """symbols = '$¢£¥€¤'def non_ascii(c): return c &gt; 127"""def clock(label, cmd): res = timeit.repeat(cmd, setup=SETUP, number=TIMES) print(label, *('&#123;:.3f&#125;'.format(x) for x in res))clock('listcomp :', '[ord(s) for s in symbols if ord(s) &gt; 127]')clock('listcomp + func :', '[ord(s) for s in symbols if non_ascii(ord(s))]')clock('filter + lambda :', 'list(filter(lambda c: c &gt; 127, map(ord, symbols)))')clock('filter + func :', 'list(filter(non_ascii, map(ord, symbols)))')]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_repr__,__str__,__bool__]]></title>
    <url>%2F2018%2F04%2F22%2FC01-repr-str-bool%2F</url>
    <content type="text"><![CDATA[repr, strThe repr special method is called by the repr built-in to get the string representation of the object forinspection.The interactive console and debugger call repr on the results of the expressions evaluated. Contrast repr with str, which is called by the str() constructor and implicitly used by the printfunction. str should return a string suitable for display to end users.If you only implement one ofthese special methods, choose repr, because when no custom str is available, Python will callrepr as a fallback. boolBy default, instances of user-defined classes are considered truthy, unless either__bool__ or lenis implemented. Basically, bool(x) calls x.bool() and uses the result. If bool is not implemented,Python tries to invoke x.len(), and if that returns zero, bool returns False. Otherwise bool returns True.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_How Special Methods Are Used]]></title>
    <url>%2F2018%2F04%2F22%2FC01-How-Special-Methods-Are-Used%2F</url>
    <content type="text"><![CDATA[First thing to know about special methods is that they are meant to be called by thePython interpreter,and not by you. You don’t write my_object.len(). You writelen(my_object) and, if my_object is an instance of a user-defined class, then Python calls the len instance method you implemented. Special method call is implicit. For example, the statement for i in x: actually causes the invocation of iter(x), which in turn may call x.iter() if that is available.Normally, your code should not have many direct calls to special methods. Unless you are doing a lotof metaprogramming, you should be implementing special methods more often than invoking themexplicitly. The only special method that is frequently called by user code directly is init, to invokethe initializer of the superclass in your own init implementation. If you need to invoke a special method, it is usually better to call the related built-in function (e.g., len,iter, str, etc). These built-ins call the corresponding special method, but often provide other services and—for built-in types—are faster than method calls.]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>fluent python</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C29_mysqldump]]></title>
    <url>%2F2018%2F04%2F22%2FC29-mysqldump%2F</url>
    <content type="text"><![CDATA[备份 使用命令行实用程序mysqldump 转储所有数据库内容到某个外部文件。在进行常规备份前这个实用程序应该正常运行，以便能正确地备份转储文件。 可用命令行实用程序mysqlhotcopy 从一个数据库复制所有数据（并非所有数据库引擎都支持这个实用程序）。 可以使用MySQL的BACKUP TABLE 或SELECT INTO OUTFILE 转储所有数据到某个外部文件。这两条语句都接受将要创建的系统文件名，此系统文件必须不存在，否则会出错。数据可以用RESTORE TABLE 来复原。 为了保证所有数据被写到磁盘（包括索引数据），可能需要在进行备份前使用FLUSH TABLES 语句。 维护ANALYZE TABLE ，用来检查表键是否正确1ANALYZE TABLE orders; CHECK TABLE 用来针对许多问题对表进行检查。在MyISAM 表上还对索引进行检查。CHECK TABLE 支持一系列的用于MyISAM 表的方式。CHANGED 检查自最后一次检查以来改动过的表。EXTENDED 执行最彻底的检查，FAST 只检查未正常关闭的表，MEDIUM 检查所有被删除的链接并进行键检验，QUICK 只进行快速扫描。 如果MyISAM 表访问产生不正确和不一致的结果，可能需要用REPAIR TABLE 来修复相应的表。这条语句不应该经常使用，如果需要经常使用，可能会有更大的问题要解决。 如果从一个表中删除大量数据，应该使用OPTIMIZE TABLE 来收回所用的空间，从而优化表的性能 启动问题 mysqld –safe-mode 装载减去某些最佳配置的服务器； –verbose 显示全文本消息（为获得更详细的帮助消息与–help 联合使用）； –version 显示版本信息然后退出。 日志MySQL维护管理员依赖的一系列日志文件。主要的日志文件有以下几种。 错误日志。它包含启动和关闭问题以及任意关键错误的细节。此日志通常名为hostname.err ，位于data 目录中。此日志名可用–log-error 命令行选项更改。 查询日志。它记录所有MySQL活动，在诊断问题时非常有用。此日志文件可能会很快地变得非常大，因此不应该长期使用它。此日志通常名为hostname.log ，位于data 目录中。此名字可以用–log 命令行选项更改。 二进制日志。它记录更新过数据（或者可能更新过数据）的所有语句。此日志通常名为hostname-bin ，位于data 目录内。此名字可以用–log-bin 命令行选项更改。注意，这个日志文件是MySQL 5中添加的，以前的MySQL版本中使用的是更新日志。 缓慢查询日志。顾名思义，此日志记录执行缓慢的任何查询。这个日志在确定数据库何处需要优化很有用。此日志通常名为hostname-slow.log ，位于data 目录中。此名字可以用–log-slow-queries 命令行选项更改。 在使用日志时，可用FLUSH LOGS 语句来刷新和重新开始所有日志文件。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C28_用户管理]]></title>
    <url>%2F2018%2F04%2F22%2FC28-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[MySQL用户账号和信息存储在名为mysql 的MySQL数据库中。一般不需要直接访问mysql 数据库和表，但有时需要直接访问。需要直接访问它的时机之一是在需要获得所有用户账号列表时。12USE mysql;SELECT user FROM user; 创建用户账号一般来说CREATE USER 是最清楚和最简单的句子。此外，也可以通过直接插入行到user 表来增加用户，不过为安全起见，一般不建议这样做。MySQL用来存储用户账号信息的表（以及表模式等）极为重要，对它们的任何毁坏都可能严重地伤害到MySQL服务器。因此，相对于直接处理来说，最好是用标记和函数来处理这些表。123CREATE USER ben IDENTIFIED BY 'password';-- IDENTIFIED BY 指定的口令为纯文本，MySQL将在保存到user 表之前对其进行加密。为了作为散列值指-- 定口令，使用IDENTIFIED BY PASSWORD 。 RENAME USER1RENAME USER ben TO bforra; DROP USER1DROP USER bforra; 设置访问权限12SHOW GRANTS FOR bforra;--为看到赋予用户账号的权限 为设置权限，使用GRANT 语句。GRANT 要求你至少给出以下信息： 要授予的权限； 被授予访问权限的数据库或表； 用户名。每个GRANT 添加（或更新）用户的一个权限。MySQL读取所有授权，并根据它们确定权限。GRANT 的反操作为REVOKE 123456789GRANT SELECT ON crashcourse.* TO bforra;-- 此GRANT 允许用户在crashcourse.* （crashcourse 数据库的所有表）上使用SELECT 。通过只授予-- SELECT 访问权限，用户bforta 对crashcourse 数据库中的所有数据具有只读访问权限。GRANT SELECT, INSERT ON crashcourse.* TO beforta;-- 可通过列出各权限并用逗号分隔，将多条GRANT 语句串在一起REVOKE SELECT ON crashcourse.* FROM bfoora; -- 被撤销的访问权限必须存在,否则会报错。 GRANT 和REVOKE 可在几个层次上控制访问权限： 整个服务器，使用GRANT ALL 和REVOKE ALL ； 整个数据库，使用ON database.* ； 特定的表，使用ON database.table ； 特定的列； 特定的存储过程。###更改password新口令必须传递到Password() 函数进行加密。 1234SET PASSWORD FOR befora= PASSWORD('new password')SET PASSWORD = PASSWORD('new password')-- 设置自己的口令]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C27_CHARACTER SET, COLLATE]]></title>
    <url>%2F2018%2F04%2F22%2FC27-CHARACTER-SET-COLLATE%2F</url>
    <content type="text"><![CDATA[给表指定字符集和校对123456CREATE TABLE mytable( columnn1 INT, columnn2 VARCHAR(10)) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 如果指定CHARACTER SET 和COLLATE 两者，则使用这些值。 如果只指定CHARACTER SET ，则使用此字符集及其默认的校对（如SHOW CHARACTER SET 的结果中所示）。 如果既不指定CHARACTER SET ，也不指定COLLATE ，则使用数据库默认。 每个列设置1234567891011CREATE TABLE mytable( columnn1 INT, columnn2 VARCHAR(10), column3 VARCHAR(10) CHARACTER SET latin1 COLLATE latin1_general_ci) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci;SELECT * FROM customersORDER BY lastname, firstname COLLATE latin1_general_cs;-- 此SELECT 使用COLLATE 指定一个备用的校对顺序（在这个例子中，为区分大小写的校对）。这显然将-- 会影响到结果排序的次序。 COLLATE 还可以用于GROUP BY 、HAVING 、聚集函数、别名等。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C26_COMMIT ROLLBACK]]></title>
    <url>%2F2018%2F04%2F22%2FC26-COMMIT-ROLLBACK%2F</url>
    <content type="text"><![CDATA[MyISAM 和InnoDB 是两种最常使用的引擎。前者不支持明确的事务处理管理，而后者支持。 事务处理（transactionprocessing）可以用来维护数据库的完整性，它保证成批的MySQL操作要么完全执行，要么完全不执行。 事务（transaction） 指一组SQL语句； 回退（rollback） 指撤销指定SQL语句的过程； 提交（commit） 指将未存储的SQL语句结果写入数据库表； 保留点（savepoint） 指事务处理中设置的临时占位符（place-holder），你可以对它发布回退（与回退整个事务处理不同）。 标识事务的开始1START TRANSANCTION 使用ROLLBACKROLLBACK 只能在一个事务处理内使用（在执行一条START TRANSACTION 命令之后）。事务处理用来管理INSERT 、UPDATE 和DELETE 语句。你不能回退SELECT 语句。（这样做也没有什么意义。）你不能回退CREATE 或DROP 操作。事务处理块中可以使用这两条语句，但如果你执行回退，它们不会被撤销。123456789SELECT * FROM ordertotals;START TRANSACTION;DELETE FROM ordertotals;SELECT * FROM ordertotals;ROLLBACK;SELECT * FROM ordertotals-- 首先执行一条SELECT 以显示该表不为空。然后开始一个事务处理，用一条DELETE 语句删除ordertotals -- 中的所有行。另一条SELECT 语句验证ordertotals 确实为空。这时用一条ROLLBACK 语句回退START -- TRANSACTION 之后的所有语句，最后一条SELECT 语句显示该表不为空。 使用COMMIT一般的MySQL语句都是直接针对数据库表执行和编写的。这就是所谓的隐含提交（implicitcommit），即提交（写或保存）操作是自动进行的。 但是，在事务处理块中，提交不会隐含地进行。为进行明确的提交，使用COMMIT 语句。1234567-- 从系统中完全删除订单20010 。因为涉及更新两个数据库表orders 和orderItems ，所以使用事务处理-- 块来保证订单不被部分删除。最后的COMMIT 语句仅在不出错时写出更改。如果第一条DELETE 起作-- 用，但第二条失败，则DELETE 不会提交（实际上，它是被自动撤销的）。START TRANSACTION;DELETE FROM orderitems WHERE order_num = 20010;DELETE FROM orders WHERE order_num = 20010;COMMIT; 使用保留点简单的ROLLBACK 和COMMIT 语句就可以写入或撤销整个事务处理。但是，只是对简单的事务处理才能这样做，更复杂的事务处理可能需要部分提交或回退。 为了支持回退部分事务处理，必须能在事务处理块中合适的位置放置占位符。这样，如果需要回退，可以回退到某个占位符。123SAVEPOINT delete1;ROLLBACK TO delete1; 保留点在事务处理完成（执行一条ROLLBACK 或COMMIT ）后自动释放。自MySQL 5以来，也可以用RELEASE SAVEPOINT 明确地释放保留点。 更改默认的提交行为默认的MySQL行为是自动提交所有更改。换句话说，任何时候你执行一条MySQL语句，该语句实际上都是针对表执行的，而且所做的更改立即生效。为指示MySQL不自动提交更改:1SET autocommit = 0;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C25_TRIGGER]]></title>
    <url>%2F2018%2F04%2F22%2FC25-TRIGGER%2F</url>
    <content type="text"><![CDATA[触发器是MySQL响应以下任意语句而自动执行的一条MySQL语句（或位于BEGIN 和END 语句之间的一组语句）： DELETE； INSERT； UPDATE。 其他MySQL语句不支持触发器。只有表才支持触发器，视图不支持（临时表也不支持）。每个表每个事件每次只允许一个触发器。因此，每个表最多支持6个触发器（每条INSERT 、UPDATE 和DELETE 的之前和之后）。单一触发器不能与多个事件或多个表关联，所以，如果你需要一个对INSERT 和UPDATE 操作执行的触发器，则应该定义两个触发器。 触发器名必须在每个表中唯一，但不是在每个数据库中唯一。这表示同一数据库中的两个表可具有相同名字的触发器。这在其他每个数据库触发器名必须唯一的DBMS中是不允许的，最好是在数据库范围内使用唯一的触发器名。 如果BEFORE 触发器失败，则MySQL将不执行请求的操作。此外，如果BEFORE 触发器或语句本身失败，MySQL将不执行AFTER 触发器（如果有的话）。 在创建触发器时，需要给出4条信息： 唯一的触发器名； 触发器关联的表； 触发器应该响应的活动（DELETE 、INSERT 或UPDATE ）； 触发器何时执行（处理之前或之后）。 12345#对每个成功的插入，显示Product added 消息。CREATE TRIGGER newproduct AFTER INSERT ON productsFOR EACH ROW SELECT 'Product added';#删除触发器DROP TRIGGER newproduct; INSERT触发器 INSERT 触发器在INSERT 语句执行之前或之后执行。需要知道以下几点： 在INSERT 触发器代码内，可引用一个名为NEW 的虚拟表，访问被插入的行； 在BEFORE INSERT 触发器中，NEW 中的值也可以被更新（允许更改被插入的值）； 对于AUTO_INCREMENT 列，NEW 在INSERT 执行之前包含0 ，在INSERT 执行之后包含新的自动生成值。12345#MySQL生成一个新订单号并保存到order_num 中。触发器从NEW.order_num 取得这个值并返回它。此#触发器必须按照AFTER INSERT 执行，因为在BEFORE INSERT 语句执行之前，新order_num 还没有生成。#对于orders 的每次插入使用这个触发器将总是返回新的订单号。CREATE TRIGGER neworder AFTER INSERT ON ordersFOR EACH ROW SELECT NEW.order_num; DELETE触发器 DELETE 触发器在DELETE 语句执行之前或之后执行。需要知道以下两点： 在DELETE 触发器代码内，你可以引用一个名为OLD 的虚拟表，访问被删除的行； OLD 中的值全都是只读的，不能更新。123456CREATE TRIGGER deleteorder BEFORE DELETE ON ordersFOR EACH ROWBEGIN #使用BEGIN END 块的好处是触发器能容纳多条SQL语句 INSERT INTO archive_orders(order_num, order_date, cust_id) VALUES(OLD.order_num, OLD.order_date, OLD.cust_id);END; UPDATE触发器 UPDATE 触发器在UPDATE 语句执行之前或之后执行。需要知道以下几点： 在UPDATE 触发器代码中，你可以引用一个名为OLD 的虚拟表访问以前（UPDATE 语句前）的值，引用一个名为NEW 的虚拟表访问新更新的值； 在BEFORE UPDATE 触发器中，NEW 中的值可能也被更新（允许更改将要用于UPDATE 语句中的值）； OLD 中的值全都是只读的，不能更新。 12CREATE TRIGGER updatevendor BEFORE UPDATE ON vendorsFOR EACH ROW SET NEW.vend_state = Upper(NEW.vend_state); 与其他DBMS相比，MySQL5中支持的触发器相当初级。未来的MySQL版本中有一些改进和增强触发器支持的计划。 创建触发器可能需要特殊的安全访问权限，但是，触发器的执行是自动的。如果INSERT 、UPDATE 或DELETE 语句能够执行，则相关的触发器也能执行。 应该用触发器来保证数据的一致性（大小写、格式等）。在触发器中执行这种类型的处理的优点是它总是进行这种处理，而且是透明地进行，与客户机应用无关。 触发器的一种非常有意义的使用是创建审计跟踪。使用触发器，把更改（如果需要，甚至还有之前和之后的状态）记录到另一个表非常容易。 遗憾的是，MySQL触发器中不支持CALL 语句。这表示不能从触发器内调用存储过程。所需的存储过程代码需要复制到触发器内。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C29__getitem__, __setitem__]]></title>
    <url>%2F2018%2F04%2F22%2FC29-getitem-setitem%2F</url>
    <content type="text"><![CDATA[Indexing and SlicingIf defined in a class (or inherited by it), the getitem method is called automatically for instance-indexingoperations. When an instance X appears in an indexing expression like X[i], Python calls the getitemmethod inherited by the instance, passing X to the first argument and the index in brackets to the secondargument.123456789101112&gt;&gt;&gt; class Indexer:... def __getitem__(self, index):... return index ** 2...&gt;&gt;&gt; X = Indexer()&gt;&gt;&gt; X[2] # X[i] calls X.__getitem__(i)4&gt;&gt;&gt; for i in range(5):... print(X[i], end=' ') # Runs __getitem__(X, i) each time...0 1 4 9 16 slicingIn addition to indexing, getitem is also called for slice expressions. Slicing bounds are bundled up intoa slice object and passed to the list’s implementation of indexing. In fact, you can always pass a sliceobject manually—slice syntax is mostly syntactic sugar for indexing with a slice object.1234567891011121314151617181920212223242526272829&gt;&gt;&gt; L = [5, 6, 7, 8, 9]&gt;&gt;&gt; L[slice(2, 4)] # Slice with slice objects[7, 8]&gt;&gt;&gt; L[slice(1, None)][6, 7, 8, 9]&gt;&gt;&gt; L[slice(None, −1)][5, 6, 7, 8]&gt;&gt;&gt; L[slice(None, None, 2)][5, 7, 9]&gt;&gt;&gt; class Indexer:... data = [5, 6, 7, 8, 9]... def __getitem__(self, index): # Called for index or slice... print('getitem:', index)... return self.data[index] # Perform index or slice...&gt;&gt;&gt; X = Indexer()&gt;&gt;&gt; X[0] # Indexing sends __getitem__ an integergetitem: 05&gt;&gt;&gt; X[2:4] # Slicing sends __getitem__ a slice objectgetitem: slice(2, 4, None)[7, 8]&gt;&gt;&gt; X[1:]getitem: slice(1, None, None)[6, 7, 8, 9]&gt;&gt;&gt; X[:-1]getitem: slice(None, −1, None)[5, 6, 7, 8] Index IterationThe for statement works by repeatedly indexing a sequence from zero to higher indexes, until an out-of-bounds exception is detected. Because of that, getitem also turns out to be one way to overloaditeration in Python—if this method is defined, for loops call the class’s getitem each time through,with successively higher offsets.12345678910111213&gt;&gt;&gt; class stepper:... def __getitem__(self, i):... return self.data[i]...&gt;&gt;&gt; X = stepper() # X is a stepper object&gt;&gt;&gt; X.data = 'Spam'&gt;&gt;&gt;&gt;&gt;&gt; X[1] # Indexing calls __getitem__'p'&gt;&gt;&gt; for item in X: # for loops call __getitem__... print(item, end=' ') # for indexes items 0..N...S p a m Any class that supports for loops automatically supports all iteration contexts in Python. For example, thein membership test, list comprehensions, the map built-in, list and tuple assignments, and typeconstructors will also call getitem automatically, if it’s defined.123456789101112131415161718&gt;&gt;&gt; 'p' in X # All call __getitem__ tooTrue&gt;&gt;&gt; [c for c in X] # List comprehension['S', 'p', 'a', 'm']&gt;&gt;&gt; list(map(str.upper, X)) # map calls (use list() in 3.0)['S', 'P', 'A', 'M']&gt;&gt;&gt; (a, b, c, d) = X # Sequence assignments&gt;&gt;&gt; a, c, d('S', 'a', 'm')&gt;&gt;&gt; list(X), tuple(X), ''.join(X)(['S', 'p', 'a', 'm'], ('S', 'p', 'a', 'm'), 'Spam')&gt;&gt;&gt; X&lt;__main__.stepper object at 0x00A8D5D0&gt;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>learning python 5th</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C01_named tuple, random.choice]]></title>
    <url>%2F2018%2F04%2F14%2FC01-named-tuple-random-choice%2F</url>
    <content type="text"><![CDATA[namedtuplePython中的tuples（元组）是经常用来表示简单的数据结构，但它只能通过下标来访问其中的数据，这导致代码难于阅读和维护。Python的collections模块包含一个namedtuple()函数，用来创建一个tuple的子类，其可以通过属性名称访问tuple中的元素。1234567891011121314&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; NetworkAddress = namedtuple('NetworkAddress',['hostname','port'])&gt;&gt;&gt; a = NetworkAddress('www.python.org',80)&gt;&gt;&gt; a.hostname'www.python.org'&gt;&gt;&gt; a.port80&gt;&gt;&gt; host, port = a&gt;&gt;&gt; len(a)2&gt;&gt;&gt; type(a)&lt;class '__main__.NetworkAddress'&gt;&gt;&gt;&gt; isinstance(a, tuple)True 支持所有普通tuple的操作，而且增加了通过使用属性名访问其中数据的功能。但使用namedtuple访问属性值时，不如通过类那样高效。 1234567class Stock(object): def __init__(self,name,shares,price): self.name = name self.shares = shares self.price = price#可定义为Stock=namedtuple('Stock',['name', 'shares', 'price']) 作为字典的替代，因为字典存储需要更多的内存空间。 如果你需要构建一个非常大的包含字典的数据结构，那么使用命名元组会更加高效。 但是需要注意的是，不像字典那样，一个命名元组是不可更改的。 需要改变属性的值，可以使用命名元组实例的 _replace() 方法 123456789101112&gt;&gt;&gt; s = Stock('ACME', 100, 123.45)&gt;&gt;&gt; sStock(name='ACME', shares=100, price=123.45)&gt;&gt;&gt; s.shares = 75Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't set attributes._replace(shares=75)&gt;&gt;&gt; sStock(name='ACME', shares=75, price=123.45)&gt;&gt;&gt; randomrandom.choice从一个序列中随机的抽取一个元素123456&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values)2&gt;&gt;&gt; random.choice(values)3 random.sample提取出N个不同元素的样本123456&gt;&gt;&gt; random.sample(values,4)[4, 5, 2, 1]&gt;&gt;&gt; random.sample(values,3)[4, 1, 2]&gt;&gt;&gt; random.sample(values,3)[5, 4, 2] random.shuffle123456&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 3, 5, 1, 2]&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[4, 5, 2, 3, 1] random.randint生成随机整数123456&gt;&gt;&gt; random.randint(0,10)6&gt;&gt;&gt; random.randint(0,10)0&gt;&gt;&gt; random.randint(0,10)10 random.random()生成0到1范围内均匀分布的浮点数 random.getrandbits()获取N位随机位(二进制)的整数12&gt;&gt;&gt; random.getrandbits(22)343509 random.seed()random 模块使用 Mersenne Twister 算法来计算生成随机数。这是一个确定性算法， 但是你可以通过random.seed() 函数修改初始化种子123random.seed() # Seed based on system time or os.urandom()random.seed(12345) # Seed based on integer givenrandom.seed(b'bytedata') # Seed based on byte data]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>learning python 5th</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C24_CURSOR]]></title>
    <url>%2F2018%2F04%2F14%2FC24-CURSOR%2F</url>
    <content type="text"><![CDATA[需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在MySQL服务器上的数据库查询，它不是一条SELECT 语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。 MySQL游标只能用于存储过程（和函数）。使用游标涉及几个明确的步骤: 在能够使用游标前，必须声明（定义）它。这个过程实际上没有检索数据，它只是定义要使用的SELECT 语句。 一旦声明后，必须打开游标以供使用。这个过程用前面定义的SELECT 语句把数据实际检索出来。 对于填有数据的游标，根据需要取出（检索）各行。 在结束游标使用时，必须关闭游标。 DECLARE CURSOR12345678#这个存储过程处理完成后，游标就消失（因为它局限于存储过程）。CREATE PROCEDURE processorders()BEGIN DECLARE ordernumbers CURSOR # DECLARE 语句用来定义和命名游标 FOR SELECT ordernum FROM orders;END; 打开和关闭游标CLOSE 释放游标使用的所有内部内存和资源，因此在每个游标不再需要时都应该关闭。1234567891011121314CREATE PROCEDURE processorders()BEGIN -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Open the cursor OPEN ordernumbers; -- Close the cursor CLOSE ordernumbers;END; FETCH游标数据在一个游标被打开后，可以使用FETCH 语句分别访问它的每一行。FETCH 指定检索什么数据（所需的列），检索出来的数据存储在什么地方。它还向前移动游标中的内部行指针，使下一条FETCH 语句检索下一行（不重复读取同一行）。 123456789101112131415161718192021222324252627282930313233CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler -- 当SQLSTATE '02000' 出现时，SET done=1 。SQLSTATE '02000' 是一个未找到条件，当REPEAT 由于 -- 没有更多的行供循环而不能继续时，出现这个条件。 --- 句柄必须在游标之后定义 DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END 下面例子中，我们增加了另一个名为t 的变量（存储每个订单的合计）。此存储过程还在运行中创建了一个新表（如果它不存在的话），名为ordertotals 。这个表将保存存储过程生成的结果。FETCH 像以前一样取每个order_num ，然后用CALL 执行另一个存储过程（我们在前一章中创建）来计算每个订单的带税的合计（结果存储到t ）。最后，用INSERT 保存每个订单的订单号和合计。123456789101112131415161718192021222324252627282930313233343536373839404142CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL(8,2); -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Create a table to store the results CREATE TABLE IF NOT EXISTS ordertotals (order_num INT, total DECIMAL(8,2)); -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- Get the total for this order CALL ordertotal(o, 1, t); -- Insert order and total into ordertotals INSERT INTO ordertotals(order_num, total) VALUES(o, t); -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C23_存储过程]]></title>
    <url>%2F2018%2F04%2F14%2FC23-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MySQL称存储过程的执行为调用，因此MySQL执行存储过程的语句为CALL 。CALL 接受存储过程的名字以及需要传递给它的任意参数。1234CALL productpricing ( @pricelow, @pricehigh, @priceaverage);#执行名为productpricing 的存储过程，它计算并返回产品的最低、最高和平均价格 CREATR PROCEDURE 创建存储过程1234567CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END;#此存储过程名为productpricing ，用CREATE PROCEDURE productpricing() 语句定义。如果存储过程接受#参数，它们将在() 中列举出来。 更改命令行分隔符DELIMITER// 告诉命令行实用程序使用// 作为新的语句结束分隔符1234567DELIMITER // CREATE PROCEDURE productpricing() BEGIN SELECT Avg(prod_price) AS priceaverage FROM products; END // 删除存储过程1234DROP PROCEDURE productpricing;DROP PROCEDURE productpricing IF EXISTS;# 使用参数关键字OUT 指出相应的参数用来从存储过程传出一个值（返回给调用者）。MySQL支持IN （传递给存储过程）、OUT （从存储过程传出，如这里所用）和INOUT （对存储过程传入和传出）类型的参数。存储过程的代码位于BEGIN 和END 语句内，一系列SELECT 语句，用来检索值，然后保存到相应的变量（通过指定INTO 关键字）。 所有MySQL变量都必须以@ 开始。 OUT 参数123456789101112131415161718192021222324252627CREATE PROCEDURE productpricing( OUT pl DECIMAL(8,2), OUT ph DECIMAL(8,2), OUT pa DECIMAL(8,2))BEGIN SELECT Min(prod_price) INTO pl FROM products; SELECT Max(prod_price) INTO ph FROM products; SELECT Avg(prod_price) INTO pa FROM products;END;#调用productpricing， 指定三个变量,#在调用时，这条语句并不显示任何数据。它返回以后可以显示（或在其他处理中使用）的变量。CALL productpricing(@pricelow,@pricehigh,@priceaverage);#显示检索出的产品平均价格，可如下进行：SELECT @pricehigh, @pricelow, @priceaverage; IN, OUT 参数12345678910111213CREATE PROCEDURE ordertotal(IN onumber INT,OUT ototal DECIMAL(8,2))BEGINSELECT Sum(item_price * quantity)FROM orderitemsWHERE order_num = onumberINTO ototal;END;#调用ordertotal, 第一个参数为订单号，第二个参数为包含计算出来的合计的变量名CALL ordertotal(20005, @total); 建立智能存储过程`sql #注释 – – Name: ordertotal– Parameters: onumber = order number– taxable = 0 if not taxable, 1 if taxable– ototal = order total variable CREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2)) COMMENT ‘Obtain order total, optionally adding tax’ #将在SHOW PROCEDURE STATUS 的结果中显示 BEGIN # DECLARE 语句定义了两个局部变量 DECLARE total DECIMAL(8,2); DECLARE taxrate INT DEFAULT 6; SELECT Sum(item_price * quantity) FROM orderitems WHERE order_num = onumber INTO total; IF taxable THEN SELECT total + (total/100*taxrate) INTO total; END IF; SELECT total INTO ototal; END; CALL ordertotal(20005, 1, @total); 获得包括何时、由谁创建等详细信息的存储过程列表，使用SHOW PROCEDURE STATUS#使用LIKE 指定一个过滤模式SHOW PROCEDURE STATUS LIKE ‘ordertotal’;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C22_View]]></title>
    <url>%2F2018%2F04%2F14%2FC22-View%2F</url>
    <content type="text"><![CDATA[视图是虚拟的表。与包含数据的表不一样，视图只包含使用时动态检索数据的查询 在视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行SELECT 操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据。 重要的是知道视图仅仅是用来查看存储在别处的数据的一种设施。视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。视图的一些常见应用: 重用SQL语句。 简化复杂的SQL操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节。 使用表的组成部分而不是整个表。 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 视图的规则和限制: 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字）。 对于可以创建的视图数目没有限制。 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图。 ORDER BY 可以用在视图中，但如果从该视图检索数据SELECT 中也含有ORDER BY ，那么该视图中的 ORDER BY 将被覆盖。 视图不能索引，也不能有关联的触发器或默认值。 视图可以和表一起使用。例如，编写一条联结表和视图的SELECT 语句。 CREATE VIEW1234567891011121314151617181920212223242526CREATE VIEW productcustomers ASSELECT cust_name, cust_contact, prod_idFROM customers, orders, orderitemsWHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num;CREATE VIEW vendorlocations ASSELECT Concat(RTrim(vend_name), ' (', RTrim(vend_country), ')') AS vend_titleFROM vendorsORDER BY vend_name;#过滤CREATE VIEW customeremaillist ASSELECT cust_id, cust_name, cust_emailFROM customersWHERE cust_email IS NOT NULL;# 计算字段CREATE VIEW orderitemsexpanded ASSELECT order_num, prod_id, quantity, item_price, quantity*item_price AS expanded_priceFROM orderitems; 更新视图视图是可更新的（即，可以对它们使用INSERT 、UPDATE 和DELETE ）。更新一个视图将更新其基表（视图本身没有数据）。如果你对视图增加或删除行，实际上是对其基表增加或删除行。 并非所有视图都是可更新的。基本上可以说，如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。这实际上意味着，如果视图定义中有以下操作，则不能进行视图的更新： 分组（使用GROUP BY 和HAVING ）； 联结； 子查询； 并； 聚集函数（Min() 、Count() 、Sum() 等）； DISTINCT ； 导出（计算）列 一般，应该将视图用于检索（SELECT 语句）而不用于更新（INSERT 、UPDATE 和DELETE ）。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_ALTER TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-ALTER-TABLE%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223#给表添加一个列ALTER TABLE vendorsADD vend_phone CHAR(20);#删除一个列ALTER TABLE VendorsDROP COLUMN vend_phone;#定义外键ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_ordersFOREIGN KEY (order_num) REFERENCES orders (order_num);ALTER TABLE orderitemsADD CONSTRAINT fk_orderitems_products FOREIGN KEY (prod_id)REFERENCES products (prod_id);ALTER TABLE ordersADD CONSTRAINT fk_orders_customers FOREIGN KEY (cust_id)REFERENCES customers (cust_id);ALTER TABLE productsADD CONSTRAINT fk_products_vendorsFOREIGN KEY (vend_id) REFERENCES vendors (vend_id); 复杂的表结构更改一般需要手动删除过程，它涉及以下步骤： 用新的列布局创建一个新表； 使用INSERT SELECT 语句（关于这条语句的详细介绍，请参阅第19章）从旧表复制数据到新表。如果有必要，可使用转换函数和计算字段； 检验包含所需数据的新表； 重命名旧表（如果确定，可以删除它）； 用旧表原来的名字重命名新表； 根据需要，重新创建触发器、存储过程、索引和外键。 DROP TABLE12#删除表没有确认，也不能撤销DROP TABLE customers2; RENAME TABLE123RENAME TABLE backup_customers TO customers, backup_vendors TO vendors, backup_products TO products;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C21_CREATE TABLE]]></title>
    <url>%2F2018%2F04%2F14%2FC21-CREATE-TABLE%2F</url>
    <content type="text"><![CDATA[PRIMARY KEY如果主键使用单个列，则它的值必须唯一。如果使用多个列，则这些列的组合值必须唯一。 123456789101112PRIMARY KEY (vend_id) #用单个列作为主键#创建由多个列组成的主键,应该以逗号分隔的列表给出各列名CREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL , item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB; AUTO_INCREMENT每个表只允许一个AUTO_INCREMENT 列，而且它必须被索引（如，通过使它成为主键）。1234567cust_id int NOT NULL AUTO_INCREMENT,#AUTO_INCREMENT 告诉MySQL，本列每当增加一行时自动增量。每次执行一个INSERT 操作时，MySQL#自动对该列增量，给该列赋予下一个可用的值。这样给每个行分配一个唯一的cust_id ，从而可以用作主#键值。SELECT last_insert_id();#返回最后一个AUTO_INCREMENT 值 DEFAULT如果在插入行时没有给出值，MySQL允许指定此时使用的默认值。默认值用CREATE TABLE 语句的列定义中的DEFAULT关键字指定。 1quantity int NOT NULL DEFAULT 1, ENGINE引擎类型可以混用。混用引擎类型有一个大缺陷。外键（用于强制实施引用完整性）不能跨引擎，即使用一个引擎的表不能引用具有使用不同引擎的表的外键。 InnoDB 是一个可靠的事务处理引擎，它不支持全文本搜索； MEMORY 在功能等同于MyISAM ，但由于数据存储在内存中，速度很快（特别适合于临时 表 MyISAM 是一个性能极高的引擎，它支持全文本搜索，但不支持事务处理。 sqlCREATE TABLE orderitems( order_num int NOT NULL , order_item int NOT NULL , prod_id char(10) NOT NULL , quantity int NOT NULL DEFAULT 1, item_price decimal(8,2) NOT NULL , PRIMARY KEY (order_num, order_item)) ENGINE=InnoDB;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C20_UPDATE_DELETE]]></title>
    <url>%2F2018%2F04%2F14%2FC20-UPDATE-DELETE%2F</url>
    <content type="text"><![CDATA[12345678UPDATE customersSET cust_email = 'elmer@fudd.com', cust_name = 'The Fudds',WHERE cust_id = 10005;UPDATE customersSET cust_email = NULL #NULL 用来去除cust_email 列中的值。WHERE cust_id = 10005; IGNORE如果用UPDATE 语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个UPDATE 操作被取消（错误发生前更新的所有行被恢复到它们原来的值）。为即使是发生错误，也继续进行更新，可使用IGNORE 关键字 1UPDATE IGNORE customers ... DELETE12DELETE FROM customersWHERE cust_id = 10006; Attention 除非确实打算更新和删除每一行，否则绝对不要使用不带WHERE 子句的UPDATE 或DELETE 语句。 保证每个表都有主键（如果忘记这个内容，请参阅第15章），尽可能像WHERE 子句那样使用它（可以指 定各主键、多个值或值的范围）。 在对UPDATE 或DELETE 语句使用WHERE 子句前，应该先用SELECT 进行测试，保证它过滤的是正确的记 录，以防编写的WHERE 子句不正确。 使用强制实施引用完整性的数据库（关于这个内容，请参阅第15章），这样MySQL将不允许删除具有与 其他表相关联的数据的行。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C19_INSERT]]></title>
    <url>%2F2018%2F04%2F14%2FC19-INSERT%2F</url>
    <content type="text"><![CDATA[INSERT 是用来插入（或添加）行到数据库表的。插入可以用几种方式使用： 插入完整的行； 插入行的一部分； 插入多行； 插入某些查询的结果。 12345678910111213141516INSERT INTO CustomersVALUES(NULL, 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#插入一个新客户到customers 表。存储到每个表列中的数据在VALUES 子句中给出，对每个列必须提供一#个值。如果某个列没有值（如上面的cust_contact 和cust_email 列），应该使用NULL 值（假定表允许对#该列指定空值）。各个列必须以它们在表定义中出现的次序填充。第一列cust_id 也为NULL 。这是因为#每次插入一个新行时，该列由MySQL自动增量。你不想给出一个值（这是MySQL的工作），又不能省略#此列（如前所述，必须给出每个列），所以指定一个NULL 值（它被MySQL忽略，MySQL在这里插入下#一个可用的cust_id 值）。 12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country, cust_contact, cust_email)VALUES('Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA', NULL, NULL);#在插入行时，MySQL将用VALUES 列表中的相应值填入列表中的对应项。VALUES 中的第一个值对应于第#一个指定的列名。第二个值对应于第二个列名，如此等等。#因为提供了列名，VALUES 必须以其指定的次序匹配指定的列名，不一定按各个列出现在实际表中的次#序。其优点是，即使表的结构改变，此INSERT 语句仍然能正确工作。你会发现cust_id 的NULL 值是不必#要的，cust_id 列并没有出现在列表中，所以不需要任何值。 如果数据检索是最重要的（通常是这样），则你可以通过在INSERT 和INTO 之间添加关键字LOW_PRIORITY ，指示MySQL降低INSERT 语句的优先级, 也适用于UPDATE 和DELETE 语句1INSERT LOW_PRIORITY INTO insert 多条12345678910111213141516171819202122INSERT INTO customers(cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)VALUES( 'Pep E. LaPew', '100 Main Street', 'Los Angeles', 'CA', '90046', 'USA' ), ( 'M. Martian', '42 Galaxy Way', 'New York', 'NY', '11213', 'USA' ); INSERT SELECT12345678910111213141516171819INSERT INTO customers(cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country)SELECT cust_id, cust_contact, cust_email, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_countryFROM custnew; 这个例子使用INSERT SELECT 从custnew 中将所有数据导入customers 。SELECT 语句从custnew 检索出要插入的值，而不是列出它们。SELECT 中列出的每个列对应于customers 表名后所跟的列表中的每个列。这条语句将插入多少行有赖于custnew 表中有多少行。 为简单起见，这个例子在INSERT 和SELECT 语句中使用了相同的列名。但是，不一定要求列名匹配。事实上，MySQL甚至不关心SELECT 返回的列名。它使用的是列的位置，因此SELECT 中的第一列（不管其列名）将用来填充表列中指定的第一个列，第二列将用来填充表列中指定的第二个列，如此等等。这对于从使用不同列名的表中导入数据是非常有用的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_布尔方式 搜索_boolean mode]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%B8%83%E5%B0%94%E6%96%B9%E5%BC%8F-%E6%90%9C%E7%B4%A2-boolean-mode%2F</url>
    <content type="text"><![CDATA[###MySQL支持全文本搜索的另外一种形式，称为布尔方式 （boolean mode）。以布尔方式，可以提供关于如下内容的细节： 要匹配的词； 要排斥的词（如果某行包含这个词，则不返回该行，即使它包含其他指定的词也是如此）； 排列提示（指定某些词比其他词更重要，更重要的词等级 表达式分组； 另外一些内容。 即使没有定义FULLTEXT 索引，也可以使用它。但这是一种非常缓慢的操作 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('heavy-rope*' IN BOOLEAN MODE);#匹配包含heavy 但不包含任意以rope 开始的词的行, -rope* 明确地指示MySQL排除包含rope* （任何以#rope 开始的词，包括ropes ）的行 + 包含，词必须存在 - 排除，词必须不出现 > 包含，而且增加等级值 &lt; 包含，且减少等级值 () 把词组成子表达式（允许这些子表达式作为一个组被包含、排除、排列等） ~ 取消一个词的排序值 * 词尾的通配符 ‘ ‘ 定义一个短语（与单个词的列表不一样，它匹配整个短语以便包含或排除这个短语） 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('+rabbit +bait'' IN BOOLEAN MODE);# 搜索匹配包含词rabbit 和bait 的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit bait' IN BOOLEAN MODE);#没有指定操作符，这个搜索匹配包含rabbit 和bait 中的至少一个词的行 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against(''rabbit bait'' IN BOOLEAN MODE);#搜索匹配短语rabbit bait 而不是匹配两个词rabbit 和bait 1234SELECT note_textFROM productnotesWHERE Match(note_text) Against('&gt;rabbit &lt;carrot' IN BOOLEAN MODE);#匹配rabbit 和carrot ，增加前者的等级，降低后者的等级 12345SELECT note_textFROM productnotesWHERE Match(note_text) Against('+safe +(&lt;combination)' IN BOOLEANMODE);#搜索匹配词safe 和combination ，降低后者的等级]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C18_全文本搜索]]></title>
    <url>%2F2018%2F04%2F14%2FC18-%E5%85%A8%E6%96%87%E6%9C%AC%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[启用一般在创建表时启用全文本搜索。CREATE TABLE 语句接受FULLTEXT 子句，它给出被索引列的一个逗号分隔的列表。123456789CREATE TABLE productnotes( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL, note_text text NULL , PRIMARY KEY(note_id), FULLTEXT(note_text)) ENGINE=MyISAM; 为了进行全文本搜索，MySQL根据子句FULLTEXT(note_text) 的指示对它进行索引。这里的FULLTEXT 索引单个列，如果需要也可以指定多个列。在定义之后，MySQL自动维护该索引。在增加、更新或删除行时，索引随之自动更新。 Match(), Against()在索引之后，使用两个函数Match() 和Against() 执行全文本搜索，其中Match() 指定被搜索的列，Against() 指定要使用的搜索表达式 Match(note_text) 指示MySQL针对指定的列进行搜索，Against(‘rabbit’) 指定词rabbit 作为搜索文本。由于有两行包含词rabbit ，这两个行被返回。123ELECT note_textFROM productnotesWHERE Match(note_text) Against('rabbit');]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_组合查询]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[多数SQL查询都只包含从一个或多个表中返回数据的单条SELECT 语句。MySQL也允许执行多个查询（多条SELECT 语句），并将结果作为单个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compoundquery） UNION可用UNION 操作符来组合数条SQL查询。利用UNION ，可给出多条SELECT 语句，将它们的结果组合成单个结果集。 UNION 必须由两条或两条以上的SELECT 语句组成，语句之间用关键字UNION 分隔（因此，如果组合4条SELECT 语句，将要使用3个UNION 关键字）。 UNION 中的每个查询必须包含相同的列、表达式或聚集函数（不过各个列不需要以相同的次序列出）。 列数据类型必须兼容：类型不必完全相同，但必须是DBMS可以隐含地转换的类型（例如，不同的数值类型或不同的日期类型）。 123456789101112SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5UNIONSELECT vend_id, prod_id, prod_priceFROM productsWHERE vend_id IN (1001,1002);SELECT vend_id, prod_id, prod_priceFROM productsWHERE prod_price &lt;= 5 OR vend_id IN (1001,1002); 包含或取消重复的行UNION 从查询结果集中自动去除了重复的行, 这是UNION 的默认行为，但是如果需要，可以改变它。事实上，如果想返回所有匹配行，可使用UNION ALL 而不是UNION 。如果确实需要每个条件的匹配行全部出现（包括重复行），则必须使用UNION ALL 而不是WHERE 。 排序SELECT 语句的输出用ORDER BY 子句排序。在用UNION 组合查询时，只能使用一条ORDER BY 子句，它必须出现在最后一条SELECT 语句之后。对于结果集，不存在用一种方式排序一部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条ORDER BY 子句。虽然ORDER BY 子句似乎只是最后一条SELECT 语句的组成部分，但实际上MySQL将用它来排序所有SELECT 语句返回的所有结果。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_带聚集函数的联结]]></title>
    <url>%2F2018%2F04%2F14%2FC16-%E5%B8%A6%E8%81%9A%E9%9B%86%E5%87%BD%E6%95%B0%E7%9A%84%E8%81%94%E7%BB%93%2F</url>
    <content type="text"><![CDATA[聚集函数用来汇总数据。虽然至今为止聚集函数的所有例子只是从单个表汇总数据，但这些函数也可以与联结一起使用。123456789#检索所有客户及每个客户所下的订单数，下面使用了COUNT() 函数的代码可完成此工作。此SELECT 语句#使用INNER JOIN 将customers 和orders 表互相关联。GROUP BY 子句按客户分组数据，因此，函数调用#COUNT(orders.order_num) 对每个客户的订单计数，将它作为num_ord 返回SELECT customers.cust_name, customers.cust_id, COUNT(orders.order_num) AS num_ordFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_idGROUP BY customers.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C16_INNER JOIN, LEFT OUTER JOIN]]></title>
    <url>%2F2018%2F04%2F14%2FC16-INNER-JOIN-LEFT-OUTER-JOIN%2F</url>
    <content type="text"><![CDATA[自联结某物品（其ID为DTNTR ）存在问题，因此想知道生产该物品的供应商生产的其他物品是否也存在这些问题。此查询要求首先找到生产ID为DTNTR 的物品的供应商，然后找出这个供应商生产的其他物品1234567891011121314SELECT prod_id, prod_nameFROM productsWHERE vend_id = (SELECT vend_id FROM products WHERE prod_id = 'DTNTR');#内部的SELECT语句做了一个简单的检索，返回生产ID为DTNTR 的物品供应商的vend_id 。该ID用于外部#查询的WHERE 子句中，以便检索出这个供应商生产的所有物品#使用联结的相同查询SELECT p1.prod_id, p1.prod_nameFROM products AS p1, products AS p2WHERE p1.vend_id = p2.vend_id AND p2.prod_id = 'DTNTR'; 自然联结123456SELECT c.*, o.order_num, o.order_date, oi.prod_id, oi.quantity, OI.item_priceFROM customers AS c, orders AS o, orderitems AS oiWHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'FB'; 外部联结联结包含了那些在相关表中没有关联行的行。这种类型的联结称为外部联结。 在使用OUTER JOIN 语法时，必须使用RIGHT 或LEFT 关键字指定包括其所有行的表（RIGHT 指出的是OUTER JOIN 右边的表，而LEFT 指出的是OUTER JOIN 左边的表）。 存在两种基本的外部联结形式：左外部联结和右外部联结。它们之间的唯一差别是所关联的表的顺序不同。换句话说，左外部联结可通过颠倒FROM 或WHERE 子句中表的顺序转换为右外部联结。因此，两种类型的外部联结可互换使用，而究竟使用哪一种纯粹是根据方便而定。 12345678910#一个简单的内部联结。它检索所有客户及其订单SELECT customers.cust_id, orders.order_numFROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id;#检索所有客户，包括那些没有订单的客户， 使用LEFT OUTER JOIN 从FROM 子句的左边表#（customers 表）中选择所有行SELECT customers.cust_id, orders.order_numFROM customers LEFT OUTER JOIN orders ON customers.cust_id = orders.cust_id;]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql crashcourse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_ocr.py_PIL-convert, point用法]]></title>
    <url>%2F2018%2F04%2F14%2FC07-ocr-py-PIL-convert-point%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对于彩色图像，不管其图像格式是PNG，还是BMP，或者JPG，在PIL中，使用Image模块的open()函数打开后，返回的图像对象的模式都是“RGB”。而对于灰度图像，不管其图像格式是PNG，还是BMP，或者JPG，打开后，其模式为“L”。对于PNG、BMP和JPG彩色图像格式之间的互相转换都可以通过Image模块的open()和save()函数来完成。具体说就是，在打开这些图像时，PIL会将它们解码为三通道的“RGB”图像。用户可以基于这个“RGB”图像，对其进行处理。处理完毕，使用函数save()，可以将处理结果保存成PNG、BMP和JPG中任何格式。这样也就完成了几种格式之间的转换。同理，其他格式的彩色图像也可以通过这种方式完成转换。当然，对于不同格式的灰度图像，也可通过类似途径完成，只是PIL解码后是模式为“L”的图像。 Image模块的convert()函数，用于不同模式图像之间的转换。PIL中有九种不同模式。分别为1，L，P，RGB，RGBA，CMYK，YCbCr，I，F。convert()函数有三种形式的定义，它们定义形式如下： im.convert(mode) ⇒ image im.convert(“P”, **options) ⇒ image im.convert(mode, matrix) ⇒ image使用不同的参数，将当前的图像转换为新的模式，并产生新的图像作为返回值。 模式“1”12345678910111213141516171819&gt;&gt;&gt;from PIL import Image &gt;&gt;&gt; lena =Image.open("D:\\Code\\Python\\test\\img\\lena.jpg") &gt;&gt;&gt; lena.mode 'RGB' &gt;&gt;&gt; lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt; lena_1 = lena.convert("1") &gt;&gt;&gt; lena_1.mode '1' &gt;&gt;&gt; lena_1.size (512, 512) &gt;&gt;&gt;lena_1.getpixel((0,0)) 255 &gt;&gt;&gt; lena_1.getpixel((10,10)) 255 &gt;&gt;&gt;lena_1.getpixel((10,120)) 0&gt;&gt;&gt;lena_1.getpixel((130,120)) 255 模式“L”模式“L”为灰色图像，它的每个像素用8个bit表示，0表示黑，255表示白，其他数字表示不同的灰度。在PIL中，从模式“RGB”转换为“L”模式是按照下面的公式转换的：L = R 299/1000 + G 587/1000+ B * 114/1000 123456789lena_L =lena.convert("L") &gt;&gt;&gt; lena_L.mode 'L' &gt;&gt;&gt; lena_L.size (512, 512) &gt;&gt;&gt;lena.getpixel((0,0)) (197, 111, 78) &gt;&gt;&gt;lena_L.getpixel((0,0)) 132 Pointpoint()方法通过一个函数或者查询表对图像中的像素点进行处理 定义1im.point(table)⇒ imageim.point(function) ⇒ image返回给定查找表对应的图像像素值的拷贝。变量table为图像的每个通道设置256个值。如果使用变量function，其对应函数应该有一个参数。这个函数将对每个像素值使用一次，结果表格将应用于图像的所有通道。 12345678910&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 = Image.open("D:\\Code\\Python\\test\\img\\test01.jpg") &gt;&gt;&gt;im_point_fun = im01.point(lambda i:i*1.2+10)&gt;&gt;&gt;im_point_fun.show()#图像im_point_fun比原图im01亮度增加了很多；因为lambda表达式中对原图的每个像素点的值都做了增#加操作。 定义2im.point(table,mode) ⇒ imageim.point(function, mode) ⇒ image与定义1一样，但是它会为输出图像指定一个新的模式。这个方法可以一步将模式为“L”和“P”的图像转换为模式为“1”的图像 1234567891011121314151617181920212223&gt;&gt;&gt;from PIL import Image&gt;&gt;&gt; im01 =Image.open("D:\\Code\\Python\\test\\img\\test01.jpg")&gt;&gt;&gt;r,g,b = im01.split()&gt;&gt;&gt;r.mode'L'&gt;&gt;&gt; im= r.point(lambda x:x*1.3+5, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))19#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:1, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt;im.getpixel((0,0))1#图像im为全白图；&gt;&gt;&gt; im= r.point(lambda x:x*0, "1")&gt;&gt;&gt;im.show()&gt;&gt;&gt; im.getpixel((0,0))0#图像im为全黑图；]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C07_form.py_PIL]]></title>
    <url>%2F2018%2F04%2F14%2FC07-form-py-PIL%2F</url>
    <content type="text"><![CDATA[PIL (Python Image Library) 是 Python 平台处理图片的事实标准，兼具强大的功能和简洁的 API。PIL 的更新速度很慢，而且存在一些难以配置的问题，不推荐使用；而 Pillow 库则是 PIL 的一个分支，维护和开发活跃，Pillow 兼容 PIL 的绝大多数语法，推荐使用。 新建一个 Image 类的实例PIL 的主要功能定义在 Image 类当中，而 Image 类定义在同名的 Image 模块当中。使用 PIL 的功能，一般都是从新建一个 Image 类的实例开始。新建 Image 类的实例有多种方法。你可以用 Image 模块的 open() 函数打开已有的图片档案，也可以处理其它的实例，或者从零开始构建一个实例。 12345678from PIL import ImagesourceFileName = "source.png"avatar = Image.open(sourceFileName)# open() 方法打开了 source.png 这个图像，构建了名为 avatar 的实例。如果打开失败，则会抛出IOError 异常。#使用 show() 方法来查看实例。注意，PIL 会将实例暂存为一个临时文件，而后打开它。avatar.show() 查看实例的属性Image 类的实例有 5 个属性，分别是： format: 以 string 返回图片档案的格式（JPG, PNG, BMP, None, etc.）；如果不是从打开文件得到的实例，则返回 None。 mode: 以 string 返回图片的模式（RGB, CMYK, etc.）；完整的列表参见 官方说明·图片模式列表 size: 以二元 tuple 返回图片档案的尺寸 (width, height) palette: 仅当 mode 为 P 时有效，返回 ImagePalette 示例 info: 以字典形式返回示例的信息 12print avatar.format, avatar.size, avatar.mode#图片的格式 PNG、图片的大小 (400, 400) 和图片的模式 RGB 图片 IO - 转换图片格式Image 模块提供了 open() 函数打开图片档案，Image 类则提供了 save() 方法将图片实例保存为图片档案。 save() 函数可以以特定的图片格式保存图片档案。比如 save(‘target.jpg’, ‘JPG’) 将会以 JPG 格式将图片示例保存为 target.jpg。不过，大多数时候也可以省略图片格式。此时，save() 方法会根据文件扩展名来选择相应的图片格式。 1234567891011import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: f, e = os.path.splitext(infile) outfile = f + ".jpg" if infile != outfile: try: Image.open(infile).save(outfile) except IOError: print "cannot convert", infile 制作缩略图Image 类的 thumbnail() 方法可以用来制作缩略图。它接受一个二元数组作为缩略图的尺寸，然后将示例缩小到指定尺寸。 12345678910111213import os, sysfrom PIL import Imagefor infile in sys.argv[1:]: outfile = os.path.splitext(infile)[0] + ".thumbnail" if infile != outfile: try: im = Image.open(infile) x, y = im.size #用 im.size 获取原图档的尺寸 im.thumbnail((x//2, y//2)) im.save(outfile, "JPEG") except IOError: print "cannot create thumbnail for", infile 剪裁图档按照 horizon 和 vertic 两个变量切割当前目录下所有图片（包括子目录）。 123456789101112131415161718192021222324import Image as imgimport osimgTypes = ['.png','.jpg','.bmp']horizon = 8vertic = 1for root, dirs, files in os.walk('.'): for currentFile in files: crtFile = root + '\\' + currentFile if crtFile[crtFile.rindex('.'):].lower() in imgTypes: crtIm = img.open(crtFile) crtW, crtH = crtIm.size hStep = crtW // horizon vStep = crtH // vertic for i in range(vertic): for j in range(horizon): crtOutFileName = crtFile[:crtFile.rindex('.')] + \ '_' + str(i) + '_' + str(j)\ + crtFile[crtFile.rindex('.'):].lower() box = (j * hStep, i * vStep, (j + 1) * hStep, (i + 1) * vStep) cropped = crtIm.crop(box) cropped.save(crtOutFileName) 变形与粘贴transpose() 方法可以将图片左右颠倒、上下颠倒、旋转 90°、旋转 180° 或旋转 270°。paste() 方法则可以将一个 Image 示例粘贴到另一个 Image 示例上。 尝试将一张图片的左半部分截取下来，左右颠倒之后旋转 180°；将图片的右半边不作更改粘贴到左半部分；最后将修改过的左半部分粘贴到右半部分。 123456789101112131415161718192021222324252627282930313233from PIL import ImageimageFName = 'source.png'def iamge_transpose(image): ''' Input: a Image instance Output: a transposed Image instance Function: * switches the left and the right part of a Image instance * for the left part of the original instance, flips left and right\ and then make it upside down. ''' xsize, ysize = image.size xsizeLeft = xsize // 2 # while xsizeRight = xsize - xsizeLeft boxLeft = (0, 0, xsizeLeft, ysize) boxRight = (xsizeLeft, 0, xsize, ysize) boxLeftNew = (0, 0, xsize - xsizeLeft, ysize) boxRightNew = (xsize - xsizeLeft, 0, xsize, ysize) partLeft = image.crop(boxLeft).transpose(Image.FLIP_LEFT_RIGHT).\ transpose(Image.ROTATE_180) partRight = image.crop(boxRight) image.paste(partRight, boxLeftNew) image.paste(partLeft, boxRightNew) return imageavatar = Image.open(imageFName)avatar = iamge_transpose(avatar)avatar.show() 以 xsize 和 ysize 接收图片的宽和高，然后以 xsizeLeft 计算得到左半边图片的大小。需要注意的是，我们构建了四个元组，并命名为盒子。这个盒子用直角坐标的值在 image 的画布上框定了一个区域。注意，Image 模块以图片的左上角为直角坐标原点，向右为 x 轴正方向，向下为 y 轴正方向。元组中的前两个数，代表区域左上角的坐标值；后两个数代表区域右下角的坐标值。 接下来的代码相当易懂。我们先用 crop() 方法将原图 boxLeft 的区域（也就是原图的左半边）切下来，然后用 transpose() 方法先后进行左右颠倒和旋转 180° 的工作，并最周公将它保存在 partLeft 这个实例中。而 partRight 的操作更为简单。 函数的最后，我们用 paste() 方法，将前两步得到的 partLeft 和 partRight 分别粘贴到指定的区域；并最终返回 image 示例。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C06_login.py_glob]]></title>
    <url>%2F2018%2F04%2F14%2FC06-login-py-glob%2F</url>
    <content type="text"><![CDATA[基本用法 glob.glob（pathname), 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。 glob.iglob(pathname), 获取一个可编历对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而glob.iglob一次只获取一个匹配路径。 列出子目录中的文件，必须把子目录包含在模式中。 12345678import glob# get all py filesfiles = glob.glob('*.py')print files# Output# ['arg.py', 'g.py', 'shut.py', 'test.py'] 123456789101112131415import itertools as it, globdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements print filename# output#=========## test.txt# arg.py# g.py# shut.py# test.py 12345678910111213141516import itertools as it, glob, osdef multiple_file_types(*patterns): return it.chain.from_iterable(glob.glob(pattern) for pattern in patterns)for filename in multiple_file_types("*.txt", "*.py"): # add as many filetype arguements realpath = os.path.realpath(filename) print realpath# output#=========## C:\xxx\pyfunc\test.txt# C:\xxx\pyfunc\arg.py# C:\xxx\pyfunc\g.py# C:\xxx\pyfunc\shut.py# C:\xxx\pyfunc\test.py]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C05_search2.py_csv]]></title>
    <url>%2F2018%2F04%2F14%2FC05-search2-py-csv%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样：12345678Symbol,Price,Date,Time,Change,Volume"AA",39.48,"6/11/2007","9:36am",-0.18,181800"AIG",71.38,"6/11/2007","9:36am",-0.15,195500"AXP",62.58,"6/11/2007","9:36am",-0.46,935000"BA",98.31,"6/11/2007","9:36am",+0.12,104800"C",53.08,"6/11/2007","9:36am",-0.25,360900"CAT",78.29,"6/11/2007","9:36am",-0.23,225400 读取csv数据row[0] 访问Symbol， row[4] 访问Change1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f,delimiter='\t') headers = next(f_csv) for row in f_csv: # Process row ... row.Symbol 和 row.Change 代替下标访问123456789101112131415161718from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) #可能有一个包含非法标识符的列头行 Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ...#或者 row['Symbol']， row['Change']import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... csv.DictReader, csv.DictWriter用法12345678910111213141516171819202122232425FIELDS = ['Name', 'Sex', 'E-mail', 'Blog'] # DictWriter csv_file = open('test.csv', 'wb') writer = csv.DictWriter(csv_file, fieldnames=FIELDS) # write header writer.writerow(dict(zip(FIELDS, FIELDS))) d = &#123;&#125; d['Name'] = 'Qi' d['Sex'] = 'Male' d['E-mail'] = 'redice@163.com' d['Blog'] = 'http://www.redicecn.com' writer.writerow(d) csv_file.close() # DictReader # A easier way for skipping the header # Usually we need a extra flag variables for d in csv.DictReader(open('test.csv', 'rb')): print d # Output: # &#123;'Blog': 'http://www.redicecn.com', 'E-mail': 'redice@163.com', 'Name': 'Qi', 'Sex': 'Male'&#125; 写入csv数据123456789101112131415161718192021222324252627#创建writer对象headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)#创建DictWriter对象headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) csv数据进行类型转换123456789field_types = [ ('Price', float), ('Change', float), ('Volume', int) ]with open('stocks.csv') as f: for row in csv.DictReader(f): row.update((key, conversion(row[key])) for key, conversion in field_types) print(row)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_process_test.py_multiprocessing]]></title>
    <url>%2F2018%2F04%2F14%2FC04-process-test-py-multiprocessing%2F</url>
    <content type="text"><![CDATA[multiprocessing是Python的标准模块，它既可以用来编写多进程，也可以用来编写多线程。如果是多线程的话，用multiprocessing.dummy即可。 如果每个子进程执行需要消耗的时间非常短（执行+1操作等），这不必使用多进程，因为进程的启动关闭也会耗费资源。 当然使用多进程往往是用来处理CPU密集型（科学计算）的需求，如果是IO密集型（文件读取，爬虫等）则可以使用多线程去处理。 创建管理进程模块： Process（用于创建进程模块） Pool（用于创建管理进程池） Queue（用于进程通信，资源共享） Value，Array（用于进程通信，资源共享） Pipe（用于管道通信） Manager（用于资源共享） 同步子进程模块： Condition Event Lock RLock Semaphore]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C04_threaded_crawler.py_threading]]></title>
    <url>%2F2018%2F04%2F14%2FC04-threaded-crawler-py-threading%2F</url>
    <content type="text"><![CDATA[创建threading.Thread的子类来包装一个线程对象 threading.Thread类的使用： 1，在自己的线程类的init里调用threading.Thread.init(self, name = threadname) Threadname为线程的名字2， run()，通常需要重写，编写代码实现做需要的功能。 3，getName()，获得线程对象名称 4，setName()，设置线程对象名称 5，start()，启动线程 6，join([timeout])，等待另一线程结束后再运行。 7，setDaemon(bool)，设置子线程是否随主线程一起结束，必须在start()之前调用。默认为False。 8，isDaemon()，判断线程是否随主线程一起结束。 9，isAlive()，检查线程是否在运行中。 12345678910111213141516171819202122232425262728293031323334353637import threadingimport timeclass timer(threading.Thread): #The timer class is derived from the class threading.Thread def __init__(self, num, interval): threading.Thread.__init__(self) self.thread_num = num self.interval = interval self.thread_stop = False def run(self): #Overwrite run() method, put what you want the thread do here while not self.thread_stop: print 'Thread Object(%d), Time:%s\n' %(self.thread_num, time.ctime()) time.sleep(self.interval) def stop(self): self.thread_stop = True def test(): thread1 = timer(1, 1) thread2 = timer(2, 2) thread1.start() thread2.start() time.sleep(10) thread1.stop() thread2.stop() return if __name__ == '__main__': test()假设两个线程对象t1和t2都要对num=0进行增1运算，t1和t2都各对num修改10次，num的最终的结果应该为20。但是由于是多线程访问，有可能出现下面情况：在num=0时，t1取得num=0。系统此时把t1调度为”sleeping”状态，把t2转换为”running”状态，t2页获得num=0。然后t2对得到的值进行加1并赋给num，使得num=1。然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给num。这样，明明t1和t2都完成了1次加1工作，但结果仍然是num=1。上面的case描述了多线程情况下最常见的问题之一：数据共享。当多个线程都要去修改某一个共享数据的时候，我们需要对数据访问进行同步。 简单的同步 最简单的同步机制就是“锁”。锁对象由threading.RLock类创建。线程可以使用锁的acquire()方法获得锁，这样锁就进入“locked”状态。每次只有一个线程可以获得锁。如果当另一个线程试图获得这个锁的时候，就会被系统变为“blocked”状态，直到那个拥有锁的线程调用锁的release()方法来释放锁，这样锁就会进入“unlocked”状态。“blocked”状态的线程就会收到一个通知，并有权利获得锁。如果多个线程处于“blocked”状态，所有线程都会先解除“blocked”状态，然后系统选择一个线程来获得锁，其他的线程继续沉默（“blocked”）。 Python的threading module是在建立在thread module基础之上的一个module，在threading module中，暴露了许多thread module中的属性。在thread module中，python提供了用户级的线程同步工具“Lock”对象。而在threading module中，python又提供了Lock对象的变种: RLock对象。RLock对象内部维护着一个Lock对象，它是一种可重入的对象。对于Lock对象而言，如果一个线程连续两次进行acquire操作，那么由于第一次acquire之后没有release，第二次acquire将挂起线程。这会导致Lock对象永远不会release，使得线程死锁。RLock对象允许一个线程多次对其进行acquire操作，因为在其内部通过一个counter变量维护着线程acquire的次数。而且每一次的acquire操作必须有一个release操作与之对应，在所有的release操作完成之后，别的线程才能申请该RLock对象。 12345678910111213141516171819202122import threadingimport timemylock=threading.RLock()num=0f=file('test_result.txt','w')class dog(theading.Thread): def __init__(self,name): theading.Thread,__init__(self) self.name=name def run(self): global num while num&lt;=5: time.sleep(0.5) mylock.acquire() print "Th(%s) locked, number: %d\n" %(self.name, num) f.write(self.name+" "+str(num)+'\n') print "Th(%s) released, number: %d\n" %(self.name, num) mylock.release() num += 1 条件同步 锁只能提供最基本的同步。假如只在发生某些事件时才访问一个“临界区”，这时需要使用条件变量 Condition。Condition对象是对Lock对象的包装，在创建Condition对象时，其构造函数需要一个Lock对象作为参数，如果没有这个Lock对象参数，Condition将在内部自行创建一个Rlock对象。在Condition对象上，当然也可以调用acquire和release操作，因为内部的Lock对象本身就支持这些操作。但是Condition的价值在于其提供的wait和notify的语义。 条件变量是如何工作的呢？首先一个线程成功获得一个条件变量后，调用此条件变量的wait()方法会 导致这个线程释放这个锁，并进入“blocked”状态，直到另一个线程调用同一个条件变量的notify()方法来唤醒那个进入“blocked”状态的线程。如果调用这个条件变量的notifyAll()方法的话就会唤醒所有的在等待的线程。 如果程序或者线程永远处于“blocked”状态的话，就会发生死锁。所以如果使用了锁、条件变量等同步机制的话，一定要注意仔细检查，防止死锁情况的发生。对于可能产生异常的临界区要使用异常处理机制中的finally子句来保证释放锁。等待一个条件变量的线程必须用notify()方法显式的唤醒，否则就永远沉默。保证每一个wait()方法调用都有一个相对应的notify()调用，当然也可以调用notifyAll()方法以防万一。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import threadingimport time con = threading.Condition()x=0 class Producer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x&gt;0: con.wait() else: for i in range(5): x += 1 print "producing... "+str(x) con.notify() print x con.release() class Consumer(threading.Thread): def __init__(self, name): threading.Thread.__init__(self) self.name = name def run(self): global x con.acquire() if x==0: print "consumer wait" con.wait() else: for i in range(5): x -= 1 print "consuming... "+str(x) con.notify() print x con.release() def test(): print "start consumer\n" th1 = Consumer("consumer") print "start producer\n" th2 = Producer("producer") th1.start() th2.start() th1.join() th2.join() if __name__ == '__main__': test() 同步队列 Python中的Queue对象也提供了对线程同步的支持。使用Queue对象可以实现多个生产者和多个消费 者形成的FIFO的队列。 生产者将数据依次存入队列，消费者依次从队列中取出数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import threadingimport timeimport Queueimport random #producerclass Producer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): print "%s is producing %d to the queue!\n" %(self.getName(), i) self.data.put(i) time.sleep(random.randrange(10)/5) print "%s finished!\n" %(self.getName()) #consumerclass Consumer(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.data = queue def run(self): for i in range(5): something = self.data.get() print "%s is consuming. %d in the queue is consumed!\n" %(self.getName(), something) time.sleep(random.randrange(10)) print "%s finished!\n" %(self.getName()) def test(): queue = Queue.Queue() th1 = Producer(queue) th2 = Consumer(queue) th1.start() th2.start() th1.join() th2.join() print "all threads terminate!\n" if __name__ == '__main__': test() join的用法123456789101112131415161718192021222324252627282930313233343536import threadingimport timedef context(tJoin): print 'in threadContext.' tJoin.start() # 将阻塞tContext直到threadJoin终止。 tJoin.join() # tJoin终止后继续执行。 print 'out threadContext.'def join(): print 'in threadJoin.' time.sleep(1) print 'out threadJoin.'tJoin = threading.Thread(target=join)tContext = threading.Thread(target=context, args=(tJoin,))tContext.start()#结果：in threadContext.in threadJoin.out threadJoin.out threadContext.&gt; tContext = threading.Thread(target=context, args=(tJoin,))&gt; tContext.start()# tJoin = threading.Thread(target=join)执行后，只是创建了一个线程对象tJoin，但并未启动该线程,这两# 句执行后，创建了另一个线程对象tContext并启动该线程（打印in threadContext.），同时将tJoin线程# 对象作为参数传给context函数，在context函数中，启动了tJoin这个线程，同时该线程又调用了join()函# 数（tJoin.join()），那tContext线程将等待tJoin这线程执行完成后，才能继续tContext线程后面的，所以# 先执行join()函数,tJoin线程执行结束后，继续执行tContext线程，于是打印输出了out threadContext.，# 于是就看到我们上面看到的输出结果，并且无论执行多少次，结果都是这个顺序。但如果将context()函# 数中tJoin.join()这句注释掉，再执行该程序，打印输出的结果顺序就不定了，因为此时这两线程就是并# 发执行的。]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_disk_cached.py_shutil]]></title>
    <url>%2F2018%2F04%2F14%2FC03-disk-cached-py-shutil%2F</url>
    <content type="text"><![CDATA[os模块提供了对目录或者文件的新建/删除/查看文件属性，还提供了对文件以及目录的路径操作。比如说：绝对路径，父目录…… 但是，os文件的操作还应该包含移动 复制 打包 压缩 解压等操作，这些os模块都没有提供，shutil则就是对os中文件操作的补充， shutil是shell utility的缩写。 shutil 模块shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 os 模块os.sep 可以取代操作系统特定的路径分隔符。windows下为 ‘\‘os.name 字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是 ‘posix’os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径os.getenv() 获取一个环境变量，如果没有返回noneos.putenv(key, value) 设置一个环境变量值os.listdir(path) 返回指定目录下的所有文件和目录名os.remove(path) 函数用来删除一个文件os.system(command) 函数用来运行shell命令os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 ‘\r\n’，Linux使用 ‘\n’ 而Mac使用 ‘\r’os.path.split(path) 函数返回一个路径的目录名和文件名os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录os.path.exists() 函数用来检验给出的路径是否真地存在os.curdir 返回当前目录 (‘.’)os.mkdir(path) 创建一个目录os.makedirs(path) 递归的创建目录os.chdir(dirname) 改变工作目录到dirnameos.path.getsize(name) 获得文件大小，如果name是目录返回0Los.path.abspath(name) 获得绝对路径os.path.normpath(path) 规范path字符串形式os.path.splitext() 分离文件名与扩展名os.path.join(path,name) 连接目录与文件名或目录os.path.basename(path) 返回文件名os.path.dirname(path) 返回文件路径os.walk(top,topdown=True,onerror=None) 遍历迭代目录os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。os.renames(old, new) 递归重命名文件夹或者文件。像rename()]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_mongo_cache.py_pymongo]]></title>
    <url>%2F2018%2F04%2F14%2FC03-mongo-cache-py-pymongo%2F</url>
    <content type="text"><![CDATA[基本用法 123456789101112131415161718192021222324252627282930313233import pymongoclient = pymongo.MongoClient(host='localhost', port=27017)db = client.cache# 指定为test 数据库collection = db.webpage#MongoDB 的每个数据库又包含了许多集合 Collection，也就类似与关系型数据库中的表condition = &#123;'name': 'Kevin'&#125;student = collection.find_one(condition)student['age'] = 25result = collection.update(condition, student)或者result = collection.update(condition, &#123;'$set': student&#125;)#只更新 student 字典内存在的字段，如果其原先还有其他字段则不会更新，也不会删除。而如果不用 $set 的话则会把之前的数据全部用 student 字典替换，如果原本存在其他的字段则会被删除。condition = &#123;'age': &#123;'$gt': 20&#125;&#125;result = collection.update_one(condition, &#123;'$inc': &#123;'age': 1&#125;&#125;)print(result)print(result.matched_count, result.modified_count)#指定查询条件为年龄大于 20，然后更新条件为 &#123;'$inc': &#123;'age': 1&#125;&#125;，也就是年龄加 1，执行之后会将第一条符合条件的数据年龄加 1# find_and_modify用法class MongoQueue: def pop(self): """Get an outstanding URL from the queue and set its status to processing. If the queue is empty a KeyError exception is raised. """ record = self.db.crawl_queue.find_and_modify( query=&#123;'status': self.OUTSTANDING&#125;, update=&#123;'$set': &#123;'status': self.PROCESSING, 'timestamp': datetime.now()&#125;&#125; ) if record: return record['_id'] else: self.repair() raise KeyError() mongodb存储二进制数据 BSON是一种类json的一种二进制形式的存储格式，简称Binary JSON，它和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。{“hello”:”world”} 这是一个BSON的例子，其中”hello”是key name，它一般是cstring类型，字节表示是cstring::= (byte) “/x00” ,其中表示零个或多个byte字节，/x00表示结束符;后面的”world”是value值，它的类型一般是、string,double,array,binarydata等类型。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pymongoimport bson.binaryfrom pymongo import MongoClientfrom cStringIO import StringIOdef insertFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image filename = 'F:/测试数据/hehe.jpg'.decode('utf-8') with open (filename,'rb') as myimage: content = StringIO(myimage.read()) coll.save(dict( content= bson.binary.Binary(content.getvalue()), filename = 'hehe.jpg' )) def getFile(): client = MongoClient('localhost', 27017) #获得一个database db = client.MongoFile #获得一个collection coll = db.image data = coll.find_one(&#123;'filename':'hehe.jpg'&#125;) out = open('F:/测试数据/test4.jpg'.decode('utf-8'),'wb') out.write(data['content']) out.close()getFile()#Here is an example of how to save some data to MongoDB and then load it:&gt;&gt;&gt; url = 'http://example.webscraping.com/view/United-Kingdom-239'&gt;&gt;&gt; html = '...'&gt;&gt;&gt; db = client.cache &gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)ObjectId('5518c0644e0c87444c12a577')&gt;&gt;&gt; db.webpage.find_one(url=url)&#123;u'_id': ObjectId('5518c0644e0c87444c12a577'), u'html': u'...', u'url': u'http://example.webscraping.com/view/United-Kingdom-239'&#125;#A problem with the preceding example is that if we now insert another document #with the same URL, MongoDB will happily insert it for us, as follows:&gt;&gt;&gt; db.webpage.insert(&#123;'url': url, 'html': html&#125;)&gt;&gt;&gt; db.webpage.find(url=url).count()2#Now we have multiple records for the same URL when we are only interested in #storing the latest data. To prevent duplicates, we can set the ID to the URL and #perform upsert, which means updating the existing record if it exists; otherwise, #insert a new one, as shown here:&gt;&gt;&gt; self.db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': html&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.update(&#123;'_id': url&#125;, &#123;'$set': &#123;'html': ''&#125;&#125;, upsert=True)&gt;&gt;&gt; db.webpage.find_one(&#123;'_id': url&#125;)&#123;u'_id': u'http://example.webscraping.com/view/United-Kingdom-239', u'html': u'...'&#125;#A timestamp index was created in the constructor. This is a handy MongoDB feature that will #automatically delete records in a specified number of seconds after the given timestamp. This means#that we do not need to manually check whether a record is still valid, as in the DiskCache class.&gt;&gt;&gt;expires=timedelta(days=30)&gt;&gt;&gt;self.db.webpage.create_index('timestamp',expireAfterSeconds=expires.total_seconds()) zlib压缩与解压缩1234567891011121314151617181920212223242526272829303132import zlibdef compress(infile, dst, level=9): infile = open(infile, 'rb') dst = open(dst, 'wb') compress = zlib.compressobj(level) data = infile.read(1024) while data: dst.write(compress.compress(data)) data = infile.read(1024) dst.write(compress.flush())def decompress(infile, dst): infile = open(infile, 'rb') dst = open(dst, 'wb') decompress = zlib.decompressobj() data = infile.read(1024) while data: dst.write(decompress.decompress(data)) data = infile.read(1024) dst.write(decompress.flush()) if __name__ == "__main__": infile = "1.txt" dst = "1.zlib.txt" compress(infile, dst) infile = "1.zlib.txt" dst = "2.txt" decompress(infile, dst) print "done~"compressobj返回一个压缩对象，用来压缩不能一下子读入内存的数据流。 level 从9到-1表示压缩等级，其中1最快但压缩度最小，9最慢但压缩度最大，0不压缩，默认是-1大约相当于与等级6，是一个压缩速度和压缩度适中的level。 picklepickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle模块只能在python中使用，python中几乎所有的数据类型（列表，字典，集合，类等）都可以用pickle来序列化 pickle.dump(obj, file[, protocol]) 序列化对象，并将结果数据流写入到文件对象中。参数protocol是序列化模式，默认值为0，表示以文本的形式序列化。protocol的值还可以是1或2，表示以二进制的形式序列化。 dumps()函数执行和dump() 函数相同的序列化，但是与dump不同的dumps并不将转换后的字符串写入文件，而是将所得到的转换后的数据以字符串的形式返回。 pickle.load(file) 反序列化对象。将文件中的数据解析为一个Python对象。 loads()函数执行和load()函数一样的反序列化。 loads接受一个字符串参数，将字符串解码成为python的数据类型，函数loads和dumps进行的是互逆的操作。12345678910111213141516171819202122232425import cPickle #序列化到文件obj = 123,"abcdedf",["ac",123],&#123;"key":"value","key1":"value1"&#125;print obj#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)#r 读写权限 r b 读写到二进制文件f = open(r"d:\a.txt","r ")cPickle.dump(obj,f)f.close()f = open(r"d:\a.txt")print cPickle.load(f)#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;) #序列化到内存（字符串格式保存），然后对象可以以任何方式处理如通过网络传输obj1 = cPickle.dumps(obj)print type(obj1)#输出：&lt;type 'str'&gt;print obj1#输出：python专用的存储格式obj2 = cPickle.loads(obj1)print type(obj2)#输出：&lt;type 'tuple'&gt;print obj2#输出：(123, 'abcdedf', ['ac', 123], &#123;'key1': 'value1', 'key': 'value'&#125;)]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C03_downloader.py_datetime]]></title>
    <url>%2F2018%2F04%2F14%2FC03-downloader-py-datetime%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718from datetime import datetime&gt;&gt;&gt; today=datetime.today()&gt;&gt;&gt; today.date()datetime.date(2018, 4, 2)&lt;!-- more --&gt;&gt;&gt;&gt; today.time()datetime.time(4, 52, 20, 254000)&gt;&gt;&gt; next_month=today.replace(month=today.month+1)&gt;&gt;&gt; next_monthdatetime.datetime(2018, 5, 2, 4, 52, 20, 254000)&gt;&gt;&gt; next_month.strftime('%Y-%m-%d %H:%M:%S')'2018-05-02 04:52:20'&gt;&gt;&gt; import time&gt;&gt;&gt; t=time.mktime(today.timetuple()) #将一个datetime对象转成时间戳，很遗憾的是python并没直接提供这个方法，但是提供了一个timetuple()方法，它返回一个time.struct_time对象，通过它我们可以构造出时间戳&gt;&gt;&gt; t1522669940.0]]></content>
      <categories>
        <category>book notes</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web scraper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask 博客配置]]></title>
    <url>%2F2018%2F04%2F14%2Fflask-%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[服务器，域名，DNSvps: vultr debian7 domain: Freenom ，具体操作可见此处(需科学上网) DNS: cloudfare, 具体操作可见此处(需科学上网)，这篇文章介绍的是cloudXNS, 也适用于cloudfare 服务器环境设置 工具 git bash 或者 putty，git bash是windows上安装git时顺带安装的bash模拟器，后续还用到git,所以直接安装git 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#此处ip是vps的ip地址， 22是ssh连接的默认端口号， 如果没有修改过默认端口，这个-p可以省略不写了ssh -p 22 root@ip#输入vps的密码后就可以登录了，此时建议换一个密码[root@vultr:~]passwd#新建一个普通用户,名为user, ‘adduser’也可以新建用户， 但是创建的用户是三无用户，无家目录/无bash/无密码，所以此处用的是'useradd',记性不好，这两个命令常常搞错。。[root@vultr:~]# user#创建密码[root@vultr:~]passwd user#此时新用户创建完成， 还需把它加入sudoers列表中，让它可以使用root的命令[root@vultr:~]apt-get install sudo #debian7 上没有默认安装sudo命令[root@vultr:~]visudo...root ALL=(ALL) ALLuser ALL=(ALL) ALL #新增加这一行就Ok了#切换到user用户[root@vultr:~]su - user[user@vultr:~]$#这个博客是基于flask,python3.5写的，所以还得安装python3.5, debian7默认已安装的是python2.7[user@vultr:~]$sudo apt-get update[user@vultr:~]$sudo dpkg -l python* #可以看到最新的python是到3.1#安装依赖[user@vultr:~]$sudo apt-get install -y build-essential libncurses5-dev libncursesw5-dev libreadline6-dev libdb5.1-dev libgdbm-dev libsqlite3-dev libssl-dev libbz2-dev libexpat1-dev liblzma-dev zlib1g-dev#下载安装包[user@vultr:~]$wget --no-check-certificate https://www.python.org/ftp/python/3.5.1/Python-3.5.1.tgz#编译安装[user@vultr:~]$tar xzvf Python-3.5.1.tgz[user@vultr:~]$cd Python-3.5.1.tgz[user@vultr:~]$./configure --prefix=/usr/local/python3 #创建 Makefile 这个文件，prefix是安装路径[user@vultr:~]$make all #all会编译所有子模块，如sqlite3等[user@vultr:~]$sudo make install#成功安装后应该可以看到一下信息Installing collected packages: setuptools, pipSuccessfully installed pip-7.1.2 setuptools-18.2#按照提示升级pip3[user@vultr:~]$pip3 install --upgrade pip#为当前用户user添加路径[user@vultr:~]$vim ~/.bashrc添加 export PATH=$PATH:/usr/local/python3/bin[user@vultr:~]$source ~/.bashrc #将当前bashrc设置读入目前的bash环境中[user@vultr:~]$python3 --verison #当前python3的版本#为python3, pip3, virtualenv设置软链接, 这里没有把原来的python命令删除[user@vultr:~]$sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3[user@vultr:~]$sudo ln -s /usr/local/python3/bin/virtualenv /usr/bin/virtualenv #/usr/bin 所以一般用户用的指令放的地方 安装数据库redis123456789101112131415161718#redis安装，方法一$wget http://download.redis.io/releases/redis-stable.tar.gz$ xzf redis-stable.tar.gz$cd redis-stable$make$make test #run the test after build$sudo make install #redis安装， 方法二#其实直接可以用apt-get安装啦$sudo apt-get install redis-server#安装完成后， redis会自动启动$ps -aux | grep redis #检查redis进程$netstat -nlt | grep 6379 #检查redis网络监听端口$sudo /etc/init.d/redis-server status # /etc/init.d/ #系统服务启动的接口放在这个目录下 安装postgresql123456789101112131415#添加postgrelsql apt repository$sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ wheezy-pgdg main" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'#导入密匙$wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -#安装$sudo apt-get update$sudo apt-get install postgresql postgresql-contrib#默认情况下安装PostgreSQL数据库服务器后，它将创建一个用户'postgres'，角色为'postgres'。 它还创建一个名称为“postgres”的系统帐户#登陆$sudo su - postgres$psql 上传文件到vps一般来说有两种方式， 一种是scp, 一种git clone 123456789101112#从本地复制目录到远程服务器， -r 递归复制整个目录$scp -r local_folder remote_username@remote_ip:remote_folder #此处remote_folder为dir=/home/&#123;user&#125;/blog，local_folder为your_blogname#如果是git clone的话，先注册个git hub 账号，然后创建一个repository,把本地文件push到这个库里$git init $git add .$git commit -m 'upload'$git configure --global$git remote add origin https://github...$git push -u origin master#感觉用git clone比较快点，而且可以随时随地上传，比较方便 安装虚拟环境把文件上传到vps后，就需要安装python3的虚拟环境了 12345678#在当前目录下，比如dir=/home/&#123;user&#125;/blog$ virtualenv venv #此时dir下生成一个venv文件夹$ source ./venv/bin/activate #激活虚拟环境（关闭虚拟环境 deactivate）#安装库$ pip install -r requirements.txt redis配置根据config.py设置的redis 密码，将/etc/redis/redis.conf 中设置成相应的密码 123$ sudo cp /etc/redis/redis.conf /usr/local/etc/redis.conf$ sudo vim /usr/local/etc/redis.conf #找到requirepass这一行，取消注释，并加上自己的password$ sudo redis-server /usr/local/etc/redis.conf #按照配置路径启动 postgresql配置根据config.py设置 ，SQLALCHEMY_DATABASE_URI = ‘postgresql://user:password@localhost/blog’，一个数据库名为’blog’, 用户是’user’，密码是’password’ 12345678$sudo -u postgres psqlpostgres= CREATE DATABASE blog;postgres= CREATE USER user WITH PASSWORD 'password';postgres= ALTER ROLE user SET client_encoding TO 'utf8';postgres= ALTER ROLE user SET default_transaction_isolation TO 'read committed';postgres= ALTER ROLE user SET timezone TO 'UTC';postgres= GRANT ALL PRIVILEGES ON DATABASE blog TO user;postgres= \q nginx安装和配置123456789101112131415161718192021222324252627#安装最新稳定版$echo deb http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list #更新库$echo deb-src http://nginx.org/packages/debian/ wheezy nginx &gt;&gt; /etc/apt/sources.list$wget http://nginx.org/keys/nginx_signing.key &amp;&amp; apt-key add nginx_signing.key #升级key$apt-get install nginx#配置文件$ sudo vim /etc/nginx/site-avaliable/blog添加以下内容server &#123; listen 80; server_name whistlestop.ml; # 域名， 用ip也行 location / &#123; proxy_pass http://127.0.0.1:8000; # gunicorn.py里的端口号 proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; $ cp /etc/nginx/sites-avaliable/blog /etc/nginx/sites-enabled/blog$ rm /etc/nginx/sites-enable/default #把原来的default文件删除#重启nginx$ sudo service nginx restart 启动博客1gunicorn --config gunicorn.py manager:app 后续要做的事 数据库备份 markdown预览功能 整个vps如何维护]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
</search>
